{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6793,"status":"ok","timestamp":1669295438842,"user":{"displayName":"Souw Chuan Neo","userId":"08556196228836469917"},"user_tz":-480},"id":"hLWmLD6fa1vR","outputId":"77a6ae4b-417d-4885-ba9d-6f152dbe02e0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torchcam in /usr/local/lib/python3.7/dist-packages (0.3.2)\n","Requirement already satisfied: Pillow>=8.3.2 in /usr/local/lib/python3.7/dist-packages (from torchcam) (9.3.0)\n","Requirement already satisfied: matplotlib<4.0.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from torchcam) (3.2.2)\n","Requirement already satisfied: torch<2.0.0,>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from torchcam) (1.12.1+cu113)\n","Requirement already satisfied: numpy<2.0.0,>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from torchcam) (1.21.6)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.0.0->torchcam) (2.8.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.0.0->torchcam) (0.11.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.0.0->torchcam) (1.4.4)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.0.0->torchcam) (3.0.9)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib<4.0.0,>=3.0.0->torchcam) (4.1.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib<4.0.0,>=3.0.0->torchcam) (1.15.0)\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["!pip install torchcam\n","import torch\n","import torchvision\n","from torch import nn\n","import numpy as np\n","from tqdm import tqdm\n","from torchvision.datasets import CIFAR10\n","from torchvision.models import resnet\n","from torch.utils.data import DataLoader\n","from torchvision import transforms\n","from datetime import datetime\n","from google.colab import drive\n","import random\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4257,"status":"ok","timestamp":1669295635717,"user":{"displayName":"Souw Chuan Neo","userId":"08556196228836469917"},"user_tz":-480},"id":"RY6-77T0pUEd","outputId":"e43ebb47-1294-477a-d6fe-4e697599b8fc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n","Files already downloaded and verified\n","Files already downloaded and verified\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]}],"source":["import os.path\n","import pickle\n","from typing import Any, Callable, Optional, Tuple\n","\n","import numpy as np\n","from PIL import Image\n","imagetotensor= transforms.ToTensor()\n","tensortopil = transforms.ToPILImage()\n","def sample(tenss,number = 1):\n","    #Supposed to return index based on weighted random choice\n","    #Issue here is although code is likely to work gonna be slower than prebuilt options\n","    output = []\n","    value = np.random.uniform(size = number)\n","    for i in range(4):\n","        for j in range(4):\n","            value -= int(tenss[i,j])\n","            for k in range(len(value)):\n","              if value[k]<=0:\n","                output.append((i,j))\n","\n","              np.delete(value,np.where(value<=0))\n","    if len(output)>1:\n","      return output\n","\n","    return [(3,3)]\n","finaldata = torch.ones(50000,4,4)\n","finaldata=torch.load('/content/drive/My Drive/Models/Supervised/cutdata.pt')\n","comparator = torch.ones(4,4)\n","for i in range(50000):\n","  if torch.all(torch.eq(comparator,finaldata[i])):\n","    print(i)\n","    k+=1\n","  finaldata[i] = finaldata[i]/torch.sum(finaldata[i])\n","\n","class indicesCIFAR10(CIFAR10):\n","  #Not only changes so indices are added, also now cutouts pre transform the image wait actually since pretransformation cutout, no longer need return indices or edit in post\n","    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n","        \"\"\"\n","        Args:\n","            index (int): Index\n","        Returns:\n","            tuple: (image, target) where target is index of the target class.\n","        \"\"\"\n","        img, target = self.data[index], self.targets[index]\n","        img = Image.fromarray(img)\n","        if self.train==True:\n","          \n","          img, target = self.data[index], self.targets[index]\n","          img = imagetotensor(img)\n","          xsys = sample(finaldata[index],3)\n","          for x,y in xsys:\n","            img[:,x*8:x*8+8,y*8:8*y+8] = 0\n","          img = tensortopil(img)\n","        # doing this so that it is consistent with all other datasets\n","        # to return a PIL Image\n","      \n","        if self.transform is not None:\n","            img = self.transform(img)\n","\n","        if self.target_transform is not None:\n","            target = self.target_transform(target)\n","\n","        return img, target,index\n","\n","batch_size = 128\n","train_transform = transforms.Compose([\n","    \n","    transforms.RandomResizedCrop(32),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])])\n","\n","test_transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])])\n","\n","\n","\n","train_data = CIFAR10(root='data', train=True, transform=train_transform, download=True)\n","\n","\n","def get_indices(dataset, num_samples):\n","  indices = []\n","  for Class in range(10):\n","    for j in range(num_samples):\n","      x = np.random.randint(0, 50000)\n","      while dataset.targets[x] != Class:\n","        x = np.random.randint(0, 50000)\n","      indices.append(x)\n","  return indices\n","\n","\n","train_data = indicesCIFAR10(root='data', train=True, transform=train_transform, download=True)\n","test_data = indicesCIFAR10(root='data', train=False, transform=test_transform, download=True)\n","test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=16, pin_memory=True)"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1669295592579,"user":{"displayName":"Souw Chuan Neo","userId":"08556196228836469917"},"user_tz":-480},"id":"rV0e4e6Mctpz"},"outputs":[],"source":["\n","class ModelBase(nn.Module):\n","    \"\"\"\n","    Common CIFAR ResNet recipe.\n","    Comparing with ImageNet ResNet recipe, it:\n","    (i) replaces conv1 with kernel=3, str=1\n","    (ii) removes pool1\n","    \"\"\"\n","    def __init__(self, feature_dim=10, arch='resnet18', bn_splits=16):\n","        super(ModelBase, self).__init__()\n","\n","        # use split batchnorm\n","        norm_layer = nn.BatchNorm2d\n","        # get specified resnet model\n","        resnet_arch = getattr(resnet, arch)\n","        net = resnet_arch(num_classes=feature_dim, norm_layer=norm_layer)\n","\n","        # make changes to original resnet\n","        self.net = []\n","        for name, module in net.named_children():\n","            if name == 'conv1':\n","                module = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n","            if isinstance(module, nn.MaxPool2d):\n","                continue\n","            if isinstance(module, nn.Linear):\n","                self.net.append(nn.Flatten(1))\n","            self.net.append(module)\n","\n","        # build net\n","        self.net = nn.Sequential(*self.net)\n","\n","    def forward(self, x):\n","        x = self.net(x)\n","        # note: not normalized here\n","        return x\n","\n","def train_val(net, data_loader, train_optimizer):\n","    global lr\n","    schedule = [60, 80]\n","    criterion = nn.CrossEntropyLoss().cuda()\n","    is_train = train_optimizer is not None\n","    net.train() if is_train else net.eval()\n","\n","    total_loss, total_correct_1, total_correct_3, total_num, data_bar = 0.0, 0.0, 0.0, 0, tqdm(data_loader, position=0, leave=True)\n","    with (torch.enable_grad() if is_train else torch.no_grad()):\n","        for data, target in data_bar:\n","            #print(target.size())\n","            #insert cropping here\n","            data, target = data.cuda(non_blocking=True), target.cuda(non_blocking=True)\n","            out = net(data)\n","            loss = criterion(out, target)\n","\n","            if is_train:\n","                train_optimizer.zero_grad()\n","                loss.backward()\n","                train_optimizer.step()\n","\n","\n","            total_num += data.size(0)\n","            total_loss += loss.item() * data.size(0)\n","            prediction = torch.argsort(out, dim=-1, descending=True)\n","            total_correct_1 += torch.sum((prediction[:, 0:1] == target.unsqueeze(dim=-1)).any(dim=-1).float()).item()\n","            total_correct_3 += torch.sum((prediction[:, 0:3] == target.unsqueeze(dim=-1)).any(dim=-1).float()).item()\n","            \n","            data_bar.set_description('{} Epoch: [{}/{}] lr: {:.4f} Loss: {:.4f} ACC@1: {:.2f}% ACC@5: {:.2f}%'\n","                                     .format('Train' if is_train else 'Test', epoch, epochs, lr, total_loss / total_num,\n","                                             total_correct_1 / total_num * 100, total_correct_3 / total_num * 100))\n","        if is_train:  \n","          if schedule is not None:\n","            for milestone in schedule:\n","                lr *= 0.1 if epoch == milestone else 1.\n","            for param_group in train_optimizer.param_groups:\n","                param_group['lr'] = lr\n","\n","        return total_correct_1/total_num * 100\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S61SwmbLIOaK"},"outputs":[],"source":["# torch.save(finaldata, '/content/drive/My Drive/Models/Supervised/cutdata.pt')\n","# finaldata = torch.ones(50000,4,4)\n","# finaldata=torch.load('/content/drive/My Drive/Models/Supervised/cutdata.pt')\n","# for i in range(50000):\n","#     finaldata[i] = finaldata[i]/torch.sum(finaldata[i])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":422,"status":"ok","timestamp":1669260921491,"user":{"displayName":"NEO SOUW CHUAN HCI","userId":"01093665843765466289"},"user_tz":-480},"id":"Wd6pvAhEni9C","outputId":"a09718ce-a5bd-47c7-cebc-069ca55c1b5e"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[5.0653e-05, 1.7633e-02, 2.7128e-02, 5.9558e-02],\n","        [0.0000e+00, 4.2108e-02, 9.9771e-02, 6.2544e-02],\n","        [9.6054e-03, 6.9857e-02, 1.9324e-01, 9.7627e-02],\n","        [1.4498e-02, 9.5292e-02, 1.0997e-01, 1.0112e-01]])\n"]}],"source":["def sample(tenss):\n","    #Supposed to return index based on weighted random choice\n","    #Issue here is although code is likely to work gonna be slower than prebuilt options\n","    value = np.random.uniform()\n","    for i in range(4):\n","        for j in range(4):\n","            value -= tenss[i,j]\n","            if value<=0:\n","                return i,j\n","    return 3,3\n","sample(finaldata[6893])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A84sJVJnnH1J"},"outputs":[],"source":["indices = get_indices(train_data, 1000)\n","sampler = torch.utils.data.SubsetRandomSampler(indices)\n","supervised = ModelBase()\n","\n","supervised.load_state_dict(torch.load(\"/content/drive/My Drive/Models/Supervised/supervisedresnet18v1.1.pth\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":169},"executionInfo":{"elapsed":7,"status":"error","timestamp":1669168647341,"user":{"displayName":"Souw Chuan Neo","userId":"08556196228836469917"},"user_tz":-480},"id":"4TAYHlDNNC5W","outputId":"4a73a6e4-9709-43dc-c77c-1d3b3e914067"},"outputs":[{"ename":"AttributeError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-9271a816bfea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"]}],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iPFIMciWiGbA"},"outputs":[],"source":["indices = get_indices(train_data, 1000)\n","sampler = torch.utils.data.SubsetRandomSampler(indices)\n","train_loader = DataLoader(train_data, batch_size=batch_size, sampler = sampler , num_workers=16, pin_memory=True)\n","for batch in train_loader:\n","  data,target = batch\n","  special = data[0]\n","  break"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k6xHaYgTeLEm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669280772383,"user_tz":-480,"elapsed":2874,"user":{"displayName":"Souw Chuan Neo","userId":"08556196228836469917"}},"outputId":"e54842bd-2100-4047-d8cd-14f16236bd5a"},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:root:no value was provided for `target_layer`, thus set to 'net.6'.\n"]},{"output_type":"stream","name":"stdout","text":["tensor([[ -6.4669,  18.5649,  -1.9640,  -6.8271, -12.8749,  -8.3182,  -1.3064,\n","         -10.8043,  -7.3363,  -0.1034]], grad_fn=<AddmmBackward0>)\n","[tensor([[[0.2195, 0.4906, 0.5149, 0.2337],\n","         [0.2724, 0.7078, 1.0000, 0.6716],\n","         [0.0936, 0.5853, 0.9919, 0.8324],\n","         [0.0000, 0.1363, 0.5818, 0.4546]]])]\n"]}],"source":["from torchcam.methods import SmoothGradCAMpp\n","supervised = ModelBase()\n","\n","supervised.load_state_dict(torch.load(\"/content/drive/My Drive/Models/Supervised/supervisedresnet18v1.1.pth\"))\n","cam_extractor = SmoothGradCAMpp(supervised)\n","data = special\n","out = supervised(data.unsqueeze(0))\n","print(out)\n","activation_map = cam_extractor(out.squeeze(0).argmax().item(), out)\n","print(activation_map)\n","#result = overlay_mask(to_pil_image(data), to_pil_image(activation_map[0].squeeze(0), mode='F'), alpha=0.5)\n","# Display it\n","#plt.imshow(result); plt.axis('off'); plt.tight_layout(); plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1006,"status":"ok","timestamp":1669126551220,"user":{"displayName":"Souw Chuan Neo","userId":"08556196228836469917"},"user_tz":-480},"id":"IxCRayLzsWcT","outputId":"3e47ef78-7793-40a0-bf38-3d5362f5e0ae"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([1, 3, 32, 32])\n","[tensor([[[0.0451, 0.0551, 0.0421, 0.0000],\n","         [0.1042, 0.3766, 0.4040, 0.3928],\n","         [0.3212, 0.5861, 0.7993, 1.0000],\n","         [0.1716, 0.3910, 0.6594, 0.8727]]])]\n"]}],"source":["print(cam_extractor._input.shape)\n","print(activation_map)"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2462552,"status":"ok","timestamp":1669298127806,"user":{"displayName":"Souw Chuan Neo","userId":"08556196228836469917"},"user_tz":-480},"id":"yXBgycP60A4-","outputId":"67559305-182a-45ee-a0a2-08d74e2c5b71"},"outputs":[{"output_type":"stream","name":"stdout","text":["Random Seed: 0\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch: [1/100] lr: 0.0010 Loss: 1.9332 ACC@1: 28.84% ACC@5: 63.01%: 100%|██████████| 79/79 [00:19<00:00,  4.03it/s]\n","Test Epoch: [1/100] lr: 0.0010 Loss: 1.7366 ACC@1: 32.94% ACC@5: 72.98%: 100%|██████████| 79/79 [00:05<00:00, 14.93it/s]\n","Train Epoch: [2/100] lr: 0.0010 Loss: 1.7484 ACC@1: 35.63% ACC@5: 70.41%: 100%|██████████| 79/79 [00:18<00:00,  4.16it/s]\n","Test Epoch: [2/100] lr: 0.0010 Loss: 1.5850 ACC@1: 40.49% ACC@5: 77.06%: 100%|██████████| 79/79 [00:04<00:00, 16.00it/s]\n","Train Epoch: [3/100] lr: 0.0010 Loss: 1.6305 ACC@1: 40.50% ACC@5: 74.23%: 100%|██████████| 79/79 [00:20<00:00,  3.82it/s]\n","Test Epoch: [3/100] lr: 0.0010 Loss: 1.5579 ACC@1: 44.61% ACC@5: 77.08%: 100%|██████████| 79/79 [00:04<00:00, 16.18it/s]\n","Train Epoch: [4/100] lr: 0.0010 Loss: 1.5606 ACC@1: 43.51% ACC@5: 76.60%: 100%|██████████| 79/79 [00:18<00:00,  4.20it/s]\n","Test Epoch: [4/100] lr: 0.0010 Loss: 1.4754 ACC@1: 47.02% ACC@5: 79.83%: 100%|██████████| 79/79 [00:04<00:00, 16.00it/s]\n","Train Epoch: [5/100] lr: 0.0010 Loss: 1.4937 ACC@1: 46.35% ACC@5: 78.78%: 100%|██████████| 79/79 [00:19<00:00,  3.98it/s]\n","Test Epoch: [5/100] lr: 0.0010 Loss: 1.5434 ACC@1: 46.37% ACC@5: 81.39%: 100%|██████████| 79/79 [00:05<00:00, 15.06it/s]\n","Train Epoch: [6/100] lr: 0.0010 Loss: 1.4118 ACC@1: 49.06% ACC@5: 80.44%: 100%|██████████| 79/79 [00:19<00:00,  4.08it/s]\n","Test Epoch: [6/100] lr: 0.0010 Loss: 1.5938 ACC@1: 48.40% ACC@5: 77.72%: 100%|██████████| 79/79 [00:05<00:00, 15.61it/s]\n","Train Epoch: [7/100] lr: 0.0010 Loss: 1.3501 ACC@1: 51.35% ACC@5: 81.95%: 100%|██████████| 79/79 [00:18<00:00,  4.17it/s]\n","Test Epoch: [7/100] lr: 0.0010 Loss: 1.5063 ACC@1: 49.76% ACC@5: 81.04%: 100%|██████████| 79/79 [00:05<00:00, 15.76it/s]\n","Train Epoch: [8/100] lr: 0.0010 Loss: 1.3336 ACC@1: 52.60% ACC@5: 82.70%: 100%|██████████| 79/79 [00:19<00:00,  4.03it/s]\n","Test Epoch: [8/100] lr: 0.0010 Loss: 1.4593 ACC@1: 49.30% ACC@5: 82.05%: 100%|██████████| 79/79 [00:05<00:00, 15.30it/s]\n","Train Epoch: [9/100] lr: 0.0010 Loss: 1.2636 ACC@1: 55.19% ACC@5: 84.00%: 100%|██████████| 79/79 [00:19<00:00,  4.02it/s]\n","Test Epoch: [9/100] lr: 0.0010 Loss: 1.2088 ACC@1: 57.72% ACC@5: 86.04%: 100%|██████████| 79/79 [00:05<00:00, 15.69it/s]\n","Train Epoch: [10/100] lr: 0.0010 Loss: 1.2351 ACC@1: 56.51% ACC@5: 84.32%: 100%|██████████| 79/79 [00:19<00:00,  4.08it/s]\n","Test Epoch: [10/100] lr: 0.0010 Loss: 1.1691 ACC@1: 57.55% ACC@5: 87.48%: 100%|██████████| 79/79 [00:05<00:00, 15.24it/s]\n","Train Epoch: [11/100] lr: 0.0010 Loss: 1.1954 ACC@1: 57.37% ACC@5: 85.47%: 100%|██████████| 79/79 [00:19<00:00,  4.00it/s]\n","Test Epoch: [11/100] lr: 0.0010 Loss: 1.2585 ACC@1: 57.73% ACC@5: 85.69%: 100%|██████████| 79/79 [00:05<00:00, 15.53it/s]\n","Train Epoch: [12/100] lr: 0.0010 Loss: 1.1704 ACC@1: 58.09% ACC@5: 86.07%: 100%|██████████| 79/79 [00:21<00:00,  3.74it/s]\n","Test Epoch: [12/100] lr: 0.0010 Loss: 1.1355 ACC@1: 60.80% ACC@5: 87.36%: 100%|██████████| 79/79 [00:05<00:00, 15.48it/s]\n","Train Epoch: [13/100] lr: 0.0010 Loss: 1.1363 ACC@1: 59.35% ACC@5: 86.20%: 100%|██████████| 79/79 [00:19<00:00,  4.03it/s]\n","Test Epoch: [13/100] lr: 0.0010 Loss: 1.1193 ACC@1: 62.16% ACC@5: 88.23%: 100%|██████████| 79/79 [00:05<00:00, 15.56it/s]\n","Train Epoch: [14/100] lr: 0.0010 Loss: 1.1154 ACC@1: 60.83% ACC@5: 86.85%: 100%|██████████| 79/79 [00:19<00:00,  4.12it/s]\n","Test Epoch: [14/100] lr: 0.0010 Loss: 1.1152 ACC@1: 62.68% ACC@5: 88.76%: 100%|██████████| 79/79 [00:05<00:00, 15.52it/s]\n","Train Epoch: [15/100] lr: 0.0010 Loss: 1.0713 ACC@1: 62.19% ACC@5: 87.89%: 100%|██████████| 79/79 [00:19<00:00,  4.07it/s]\n","Test Epoch: [15/100] lr: 0.0010 Loss: 1.0859 ACC@1: 63.24% ACC@5: 88.80%: 100%|██████████| 79/79 [00:05<00:00, 15.19it/s]\n","Train Epoch: [16/100] lr: 0.0010 Loss: 1.0570 ACC@1: 63.09% ACC@5: 87.86%: 100%|██████████| 79/79 [00:19<00:00,  4.13it/s]\n","Test Epoch: [16/100] lr: 0.0010 Loss: 1.0280 ACC@1: 64.39% ACC@5: 90.70%: 100%|██████████| 79/79 [00:05<00:00, 15.41it/s]\n","Train Epoch: [17/100] lr: 0.0010 Loss: 1.0307 ACC@1: 63.74% ACC@5: 88.59%: 100%|██████████| 79/79 [00:19<00:00,  4.14it/s]\n","Test Epoch: [17/100] lr: 0.0010 Loss: 1.0591 ACC@1: 65.35% ACC@5: 90.25%: 100%|██████████| 79/79 [00:05<00:00, 15.43it/s]\n","Train Epoch: [18/100] lr: 0.0010 Loss: 1.0029 ACC@1: 64.44% ACC@5: 88.60%: 100%|██████████| 79/79 [00:20<00:00,  3.94it/s]\n","Test Epoch: [18/100] lr: 0.0010 Loss: 0.9977 ACC@1: 66.16% ACC@5: 90.41%: 100%|██████████| 79/79 [00:05<00:00, 15.07it/s]\n","Train Epoch: [19/100] lr: 0.0010 Loss: 0.9788 ACC@1: 65.41% ACC@5: 89.19%: 100%|██████████| 79/79 [00:19<00:00,  4.07it/s]\n","Test Epoch: [19/100] lr: 0.0010 Loss: 1.1542 ACC@1: 61.97% ACC@5: 87.77%: 100%|██████████| 79/79 [00:05<00:00, 15.16it/s]\n","Train Epoch: [20/100] lr: 0.0010 Loss: 0.9670 ACC@1: 65.99% ACC@5: 89.56%: 100%|██████████| 79/79 [00:19<00:00,  3.99it/s]\n","Test Epoch: [20/100] lr: 0.0010 Loss: 0.9674 ACC@1: 68.19% ACC@5: 91.19%: 100%|██████████| 79/79 [00:05<00:00, 15.56it/s]\n","Train Epoch: [21/100] lr: 0.0010 Loss: 0.9513 ACC@1: 66.32% ACC@5: 89.99%: 100%|██████████| 79/79 [00:21<00:00,  3.68it/s]\n","Test Epoch: [21/100] lr: 0.0010 Loss: 0.9358 ACC@1: 68.34% ACC@5: 91.68%: 100%|██████████| 79/79 [00:05<00:00, 15.11it/s]\n","Train Epoch: [22/100] lr: 0.0010 Loss: 0.9334 ACC@1: 67.00% ACC@5: 90.12%: 100%|██████████| 79/79 [00:19<00:00,  4.12it/s]\n","Test Epoch: [22/100] lr: 0.0010 Loss: 1.0018 ACC@1: 65.59% ACC@5: 90.11%: 100%|██████████| 79/79 [00:05<00:00, 15.55it/s]\n","Train Epoch: [23/100] lr: 0.0010 Loss: 0.8953 ACC@1: 68.63% ACC@5: 90.73%: 100%|██████████| 79/79 [00:19<00:00,  4.13it/s]\n","Test Epoch: [23/100] lr: 0.0010 Loss: 0.8421 ACC@1: 72.39% ACC@5: 93.02%: 100%|██████████| 79/79 [00:05<00:00, 15.44it/s]\n","Train Epoch: [24/100] lr: 0.0010 Loss: 0.8932 ACC@1: 68.57% ACC@5: 90.59%: 100%|██████████| 79/79 [00:19<00:00,  4.08it/s]\n","Test Epoch: [24/100] lr: 0.0010 Loss: 1.2019 ACC@1: 65.43% ACC@5: 88.10%: 100%|██████████| 79/79 [00:04<00:00, 15.84it/s]\n","Train Epoch: [25/100] lr: 0.0010 Loss: 0.8890 ACC@1: 68.42% ACC@5: 90.60%: 100%|██████████| 79/79 [00:19<00:00,  4.15it/s]\n","Test Epoch: [25/100] lr: 0.0010 Loss: 1.0339 ACC@1: 67.53% ACC@5: 90.29%: 100%|██████████| 79/79 [00:05<00:00, 15.61it/s]\n","Train Epoch: [26/100] lr: 0.0010 Loss: 0.8495 ACC@1: 69.66% ACC@5: 91.20%: 100%|██████████| 79/79 [00:19<00:00,  4.14it/s]\n","Test Epoch: [26/100] lr: 0.0010 Loss: 1.0554 ACC@1: 68.34% ACC@5: 90.92%: 100%|██████████| 79/79 [00:05<00:00, 15.52it/s]\n","Train Epoch: [27/100] lr: 0.0010 Loss: 0.8668 ACC@1: 69.84% ACC@5: 90.84%: 100%|██████████| 79/79 [00:18<00:00,  4.18it/s]\n","Test Epoch: [27/100] lr: 0.0010 Loss: 0.8706 ACC@1: 71.54% ACC@5: 93.14%: 100%|██████████| 79/79 [00:05<00:00, 15.62it/s]\n","Train Epoch: [28/100] lr: 0.0010 Loss: 0.8192 ACC@1: 71.08% ACC@5: 91.90%: 100%|██████████| 79/79 [00:19<00:00,  4.00it/s]\n","Test Epoch: [28/100] lr: 0.0010 Loss: 0.9993 ACC@1: 69.35% ACC@5: 92.17%: 100%|██████████| 79/79 [00:05<00:00, 15.55it/s]\n","Train Epoch: [29/100] lr: 0.0010 Loss: 0.8087 ACC@1: 71.57% ACC@5: 91.71%: 100%|██████████| 79/79 [00:19<00:00,  4.12it/s]\n","Test Epoch: [29/100] lr: 0.0010 Loss: 0.8723 ACC@1: 71.92% ACC@5: 92.71%: 100%|██████████| 79/79 [00:05<00:00, 15.64it/s]\n","Train Epoch: [30/100] lr: 0.0010 Loss: 0.8129 ACC@1: 71.06% ACC@5: 91.54%: 100%|██████████| 79/79 [00:21<00:00,  3.73it/s]\n","Test Epoch: [30/100] lr: 0.0010 Loss: 1.1540 ACC@1: 67.19% ACC@5: 90.17%: 100%|██████████| 79/79 [00:05<00:00, 15.54it/s]\n","Train Epoch: [31/100] lr: 0.0010 Loss: 0.7933 ACC@1: 72.08% ACC@5: 91.87%: 100%|██████████| 79/79 [00:19<00:00,  4.08it/s]\n","Test Epoch: [31/100] lr: 0.0010 Loss: 0.9203 ACC@1: 70.57% ACC@5: 92.58%: 100%|██████████| 79/79 [00:05<00:00, 15.34it/s]\n","Train Epoch: [32/100] lr: 0.0010 Loss: 0.7810 ACC@1: 72.48% ACC@5: 91.69%: 100%|██████████| 79/79 [00:18<00:00,  4.21it/s]\n","Test Epoch: [32/100] lr: 0.0010 Loss: 0.9136 ACC@1: 72.42% ACC@5: 92.16%: 100%|██████████| 79/79 [00:05<00:00, 15.32it/s]\n","Train Epoch: [33/100] lr: 0.0010 Loss: 0.7846 ACC@1: 72.87% ACC@5: 92.20%: 100%|██████████| 79/79 [00:19<00:00,  4.15it/s]\n","Test Epoch: [33/100] lr: 0.0010 Loss: 0.8063 ACC@1: 74.86% ACC@5: 93.76%: 100%|██████████| 79/79 [00:05<00:00, 15.78it/s]\n","Train Epoch: [34/100] lr: 0.0010 Loss: 0.7807 ACC@1: 72.83% ACC@5: 92.35%: 100%|██████████| 79/79 [00:19<00:00,  4.06it/s]\n","Test Epoch: [34/100] lr: 0.0010 Loss: 0.9847 ACC@1: 72.09% ACC@5: 91.14%: 100%|██████████| 79/79 [00:05<00:00, 15.51it/s]\n","Train Epoch: [35/100] lr: 0.0010 Loss: 0.7642 ACC@1: 72.93% ACC@5: 92.31%: 100%|██████████| 79/79 [00:18<00:00,  4.18it/s]\n","Test Epoch: [35/100] lr: 0.0010 Loss: 1.1518 ACC@1: 68.75% ACC@5: 90.03%: 100%|██████████| 79/79 [00:05<00:00, 15.30it/s]\n","Train Epoch: [36/100] lr: 0.0010 Loss: 0.7431 ACC@1: 74.07% ACC@5: 92.84%: 100%|██████████| 79/79 [00:19<00:00,  4.10it/s]\n","Test Epoch: [36/100] lr: 0.0010 Loss: 0.9799 ACC@1: 71.09% ACC@5: 91.66%: 100%|██████████| 79/79 [00:05<00:00, 15.55it/s]\n","Train Epoch: [37/100] lr: 0.0010 Loss: 0.7485 ACC@1: 73.95% ACC@5: 92.82%: 100%|██████████| 79/79 [00:18<00:00,  4.20it/s]\n","Test Epoch: [37/100] lr: 0.0010 Loss: 0.7404 ACC@1: 75.83% ACC@5: 94.24%: 100%|██████████| 79/79 [00:05<00:00, 15.65it/s]\n","Train Epoch: [38/100] lr: 0.0010 Loss: 0.7269 ACC@1: 74.27% ACC@5: 93.08%: 100%|██████████| 79/79 [00:19<00:00,  4.15it/s]\n","Test Epoch: [38/100] lr: 0.0010 Loss: 0.9558 ACC@1: 71.10% ACC@5: 91.30%: 100%|██████████| 79/79 [00:05<00:00, 15.58it/s]\n","Train Epoch: [39/100] lr: 0.0010 Loss: 0.7490 ACC@1: 73.98% ACC@5: 92.95%: 100%|██████████| 79/79 [00:20<00:00,  3.82it/s]\n","Test Epoch: [39/100] lr: 0.0010 Loss: 0.7583 ACC@1: 75.78% ACC@5: 93.77%: 100%|██████████| 79/79 [00:05<00:00, 15.76it/s]\n","Train Epoch: [40/100] lr: 0.0010 Loss: 0.7064 ACC@1: 75.29% ACC@5: 93.13%: 100%|██████████| 79/79 [00:19<00:00,  4.11it/s]\n","Test Epoch: [40/100] lr: 0.0010 Loss: 0.9177 ACC@1: 74.38% ACC@5: 91.76%: 100%|██████████| 79/79 [00:05<00:00, 15.33it/s]\n","Train Epoch: [41/100] lr: 0.0010 Loss: 0.7065 ACC@1: 75.24% ACC@5: 93.19%: 100%|██████████| 79/79 [00:19<00:00,  4.11it/s]\n","Test Epoch: [41/100] lr: 0.0010 Loss: 0.7879 ACC@1: 75.77% ACC@5: 93.60%: 100%|██████████| 79/79 [00:05<00:00, 15.41it/s]\n","Train Epoch: [42/100] lr: 0.0010 Loss: 0.7312 ACC@1: 74.45% ACC@5: 92.92%: 100%|██████████| 79/79 [00:19<00:00,  4.13it/s]\n","Test Epoch: [42/100] lr: 0.0010 Loss: 0.7649 ACC@1: 76.39% ACC@5: 93.97%: 100%|██████████| 79/79 [00:05<00:00, 15.35it/s]\n","Train Epoch: [43/100] lr: 0.0010 Loss: 0.6664 ACC@1: 76.86% ACC@5: 93.11%: 100%|██████████| 79/79 [00:19<00:00,  4.11it/s]\n","Test Epoch: [43/100] lr: 0.0010 Loss: 0.9816 ACC@1: 73.76% ACC@5: 92.49%: 100%|██████████| 79/79 [00:05<00:00, 15.55it/s]\n","Train Epoch: [44/100] lr: 0.0010 Loss: 0.6387 ACC@1: 77.41% ACC@5: 94.07%: 100%|██████████| 79/79 [00:18<00:00,  4.18it/s]\n","Test Epoch: [44/100] lr: 0.0010 Loss: 0.8926 ACC@1: 74.74% ACC@5: 93.97%: 100%|██████████| 79/79 [00:05<00:00, 15.55it/s]\n","Train Epoch: [45/100] lr: 0.0010 Loss: 0.6497 ACC@1: 76.81% ACC@5: 94.01%: 100%|██████████| 79/79 [00:19<00:00,  4.13it/s]\n","Test Epoch: [45/100] lr: 0.0010 Loss: 0.8164 ACC@1: 76.70% ACC@5: 93.93%: 100%|██████████| 79/79 [00:05<00:00, 15.62it/s]\n","Train Epoch: [46/100] lr: 0.0010 Loss: 0.6365 ACC@1: 77.78% ACC@5: 94.30%: 100%|██████████| 79/79 [00:18<00:00,  4.19it/s]\n","Test Epoch: [46/100] lr: 0.0010 Loss: 0.8838 ACC@1: 74.25% ACC@5: 92.65%: 100%|██████████| 79/79 [00:05<00:00, 15.58it/s]\n","Train Epoch: [47/100] lr: 0.0010 Loss: 0.6394 ACC@1: 77.44% ACC@5: 94.30%: 100%|██████████| 79/79 [00:19<00:00,  4.14it/s]\n","Test Epoch: [47/100] lr: 0.0010 Loss: 0.7899 ACC@1: 76.75% ACC@5: 94.77%: 100%|██████████| 79/79 [00:05<00:00, 15.54it/s]\n","Train Epoch: [48/100] lr: 0.0010 Loss: 0.6188 ACC@1: 78.50% ACC@5: 94.34%: 100%|██████████| 79/79 [00:19<00:00,  4.06it/s]\n","Test Epoch: [48/100] lr: 0.0010 Loss: 0.8425 ACC@1: 74.87% ACC@5: 93.72%: 100%|██████████| 79/79 [00:05<00:00, 13.22it/s]\n","Train Epoch: [49/100] lr: 0.0010 Loss: 0.6468 ACC@1: 77.13% ACC@5: 93.90%: 100%|██████████| 79/79 [00:19<00:00,  4.15it/s]\n","Test Epoch: [49/100] lr: 0.0010 Loss: 0.7958 ACC@1: 77.00% ACC@5: 94.30%: 100%|██████████| 79/79 [00:05<00:00, 15.33it/s]\n","Train Epoch: [50/100] lr: 0.0010 Loss: 0.6198 ACC@1: 78.07% ACC@5: 94.47%: 100%|██████████| 79/79 [00:19<00:00,  4.11it/s]\n","Test Epoch: [50/100] lr: 0.0010 Loss: 0.8349 ACC@1: 75.38% ACC@5: 94.17%: 100%|██████████| 79/79 [00:05<00:00, 15.01it/s]\n","Train Epoch: [51/100] lr: 0.0010 Loss: 0.6109 ACC@1: 78.75% ACC@5: 94.19%: 100%|██████████| 79/79 [00:19<00:00,  4.07it/s]\n","Test Epoch: [51/100] lr: 0.0010 Loss: 0.9629 ACC@1: 74.97% ACC@5: 92.83%: 100%|██████████| 79/79 [00:05<00:00, 15.25it/s]\n","Train Epoch: [52/100] lr: 0.0010 Loss: 0.5946 ACC@1: 79.27% ACC@5: 94.76%: 100%|██████████| 79/79 [00:18<00:00,  4.17it/s]\n","Test Epoch: [52/100] lr: 0.0010 Loss: 0.9042 ACC@1: 75.57% ACC@5: 93.22%: 100%|██████████| 79/79 [00:05<00:00, 15.39it/s]\n","Train Epoch: [53/100] lr: 0.0010 Loss: 0.6060 ACC@1: 79.01% ACC@5: 94.61%: 100%|██████████| 79/79 [00:19<00:00,  4.09it/s]\n","Test Epoch: [53/100] lr: 0.0010 Loss: 0.6957 ACC@1: 79.36% ACC@5: 95.52%: 100%|██████████| 79/79 [00:05<00:00, 15.13it/s]\n","Train Epoch: [54/100] lr: 0.0010 Loss: 0.6130 ACC@1: 78.48% ACC@5: 94.23%: 100%|██████████| 79/79 [00:19<00:00,  4.11it/s]\n","Test Epoch: [54/100] lr: 0.0010 Loss: 0.7388 ACC@1: 78.74% ACC@5: 94.88%: 100%|██████████| 79/79 [00:05<00:00, 15.24it/s]\n","Train Epoch: [55/100] lr: 0.0010 Loss: 0.5920 ACC@1: 79.20% ACC@5: 94.68%: 100%|██████████| 79/79 [00:19<00:00,  4.09it/s]\n","Test Epoch: [55/100] lr: 0.0010 Loss: 0.7871 ACC@1: 78.19% ACC@5: 95.21%: 100%|██████████| 79/79 [00:05<00:00, 15.09it/s]\n","Train Epoch: [56/100] lr: 0.0010 Loss: 0.5767 ACC@1: 79.41% ACC@5: 94.69%: 100%|██████████| 79/79 [00:19<00:00,  4.05it/s]\n","Test Epoch: [56/100] lr: 0.0010 Loss: 0.7435 ACC@1: 78.96% ACC@5: 94.75%: 100%|██████████| 79/79 [00:05<00:00, 15.53it/s]\n","Train Epoch: [57/100] lr: 0.0010 Loss: 0.5778 ACC@1: 79.30% ACC@5: 94.68%: 100%|██████████| 79/79 [00:19<00:00,  4.07it/s]\n","Test Epoch: [57/100] lr: 0.0010 Loss: 0.8750 ACC@1: 76.72% ACC@5: 93.13%: 100%|██████████| 79/79 [00:05<00:00, 14.97it/s]\n","Train Epoch: [58/100] lr: 0.0010 Loss: 0.5554 ACC@1: 80.52% ACC@5: 95.05%: 100%|██████████| 79/79 [00:21<00:00,  3.64it/s]\n","Test Epoch: [58/100] lr: 0.0010 Loss: 0.7428 ACC@1: 78.18% ACC@5: 94.79%: 100%|██████████| 79/79 [00:05<00:00, 15.41it/s]\n","Train Epoch: [59/100] lr: 0.0010 Loss: 0.5622 ACC@1: 80.49% ACC@5: 94.69%: 100%|██████████| 79/79 [00:18<00:00,  4.19it/s]\n","Test Epoch: [59/100] lr: 0.0010 Loss: 0.8602 ACC@1: 75.39% ACC@5: 93.93%: 100%|██████████| 79/79 [00:05<00:00, 15.14it/s]\n","Train Epoch: [60/100] lr: 0.0010 Loss: 0.5442 ACC@1: 81.24% ACC@5: 94.93%: 100%|██████████| 79/79 [00:19<00:00,  4.08it/s]\n","Test Epoch: [60/100] lr: 0.0001 Loss: 0.8009 ACC@1: 78.01% ACC@5: 94.72%: 100%|██████████| 79/79 [00:05<00:00, 15.45it/s]\n","Train Epoch: [61/100] lr: 0.0001 Loss: 0.4731 ACC@1: 83.59% ACC@5: 95.68%: 100%|██████████| 79/79 [00:19<00:00,  4.14it/s]\n","Test Epoch: [61/100] lr: 0.0001 Loss: 0.6448 ACC@1: 81.50% ACC@5: 95.55%: 100%|██████████| 79/79 [00:05<00:00, 15.14it/s]\n","Train Epoch: [62/100] lr: 0.0001 Loss: 0.4372 ACC@1: 84.93% ACC@5: 96.34%: 100%|██████████| 79/79 [00:18<00:00,  4.16it/s]\n","Test Epoch: [62/100] lr: 0.0001 Loss: 0.6324 ACC@1: 81.86% ACC@5: 95.85%: 100%|██████████| 79/79 [00:05<00:00, 15.39it/s]\n","Train Epoch: [63/100] lr: 0.0001 Loss: 0.4215 ACC@1: 85.35% ACC@5: 96.57%: 100%|██████████| 79/79 [00:19<00:00,  4.09it/s]\n","Test Epoch: [63/100] lr: 0.0001 Loss: 0.6220 ACC@1: 82.00% ACC@5: 96.08%: 100%|██████████| 79/79 [00:05<00:00, 15.31it/s]\n","Train Epoch: [64/100] lr: 0.0001 Loss: 0.4115 ACC@1: 85.55% ACC@5: 96.34%: 100%|██████████| 79/79 [00:19<00:00,  4.16it/s]\n","Test Epoch: [64/100] lr: 0.0001 Loss: 0.6274 ACC@1: 82.02% ACC@5: 96.06%: 100%|██████████| 79/79 [00:05<00:00, 14.94it/s]\n","Train Epoch: [65/100] lr: 0.0001 Loss: 0.3999 ACC@1: 86.53% ACC@5: 96.30%: 100%|██████████| 79/79 [00:19<00:00,  4.01it/s]\n","Test Epoch: [65/100] lr: 0.0001 Loss: 0.6345 ACC@1: 82.26% ACC@5: 95.89%: 100%|██████████| 79/79 [00:05<00:00, 14.86it/s]\n","Train Epoch: [66/100] lr: 0.0001 Loss: 0.3977 ACC@1: 86.34% ACC@5: 96.60%: 100%|██████████| 79/79 [00:19<00:00,  4.10it/s]\n","Test Epoch: [66/100] lr: 0.0001 Loss: 0.6437 ACC@1: 82.30% ACC@5: 96.00%: 100%|██████████| 79/79 [00:05<00:00, 15.30it/s]\n","Train Epoch: [67/100] lr: 0.0001 Loss: 0.3927 ACC@1: 86.53% ACC@5: 96.65%: 100%|██████████| 79/79 [00:20<00:00,  3.78it/s]\n","Test Epoch: [67/100] lr: 0.0001 Loss: 0.6740 ACC@1: 82.01% ACC@5: 95.89%: 100%|██████████| 79/79 [00:05<00:00, 15.23it/s]\n","Train Epoch: [68/100] lr: 0.0001 Loss: 0.3885 ACC@1: 86.17% ACC@5: 96.66%: 100%|██████████| 79/79 [00:19<00:00,  4.12it/s]\n","Test Epoch: [68/100] lr: 0.0001 Loss: 0.7086 ACC@1: 81.68% ACC@5: 95.48%: 100%|██████████| 79/79 [00:05<00:00, 14.99it/s]\n","Train Epoch: [69/100] lr: 0.0001 Loss: 0.3884 ACC@1: 86.21% ACC@5: 96.87%: 100%|██████████| 79/79 [00:19<00:00,  4.11it/s]\n","Test Epoch: [69/100] lr: 0.0001 Loss: 0.6497 ACC@1: 82.38% ACC@5: 96.07%: 100%|██████████| 79/79 [00:05<00:00, 15.13it/s]\n","Train Epoch: [70/100] lr: 0.0001 Loss: 0.3800 ACC@1: 86.70% ACC@5: 96.94%: 100%|██████████| 79/79 [00:19<00:00,  4.03it/s]\n","Test Epoch: [70/100] lr: 0.0001 Loss: 0.6927 ACC@1: 81.70% ACC@5: 95.77%: 100%|██████████| 79/79 [00:05<00:00, 14.94it/s]\n","Train Epoch: [71/100] lr: 0.0001 Loss: 0.3655 ACC@1: 87.39% ACC@5: 97.01%: 100%|██████████| 79/79 [00:19<00:00,  4.13it/s]\n","Test Epoch: [71/100] lr: 0.0001 Loss: 0.6934 ACC@1: 82.25% ACC@5: 95.90%: 100%|██████████| 79/79 [00:05<00:00, 15.02it/s]\n","Train Epoch: [72/100] lr: 0.0001 Loss: 0.3712 ACC@1: 87.08% ACC@5: 96.87%: 100%|██████████| 79/79 [00:19<00:00,  4.10it/s]\n","Test Epoch: [72/100] lr: 0.0001 Loss: 0.6927 ACC@1: 82.40% ACC@5: 95.92%: 100%|██████████| 79/79 [00:05<00:00, 15.11it/s]\n","Train Epoch: [73/100] lr: 0.0001 Loss: 0.3647 ACC@1: 87.15% ACC@5: 96.99%: 100%|██████████| 79/79 [00:19<00:00,  4.15it/s]\n","Test Epoch: [73/100] lr: 0.0001 Loss: 0.7042 ACC@1: 82.34% ACC@5: 95.85%: 100%|██████████| 79/79 [00:05<00:00, 15.15it/s]\n","Train Epoch: [74/100] lr: 0.0001 Loss: 0.3632 ACC@1: 87.65% ACC@5: 96.97%: 100%|██████████| 79/79 [00:19<00:00,  4.06it/s]\n","Test Epoch: [74/100] lr: 0.0001 Loss: 0.7220 ACC@1: 82.36% ACC@5: 95.88%: 100%|██████████| 79/79 [00:05<00:00, 15.42it/s]\n","Train Epoch: [75/100] lr: 0.0001 Loss: 0.3644 ACC@1: 87.59% ACC@5: 96.69%: 100%|██████████| 79/79 [00:19<00:00,  4.13it/s]\n","Test Epoch: [75/100] lr: 0.0001 Loss: 0.7201 ACC@1: 82.14% ACC@5: 96.01%: 100%|██████████| 79/79 [00:05<00:00, 15.13it/s]\n","Train Epoch: [76/100] lr: 0.0001 Loss: 0.3717 ACC@1: 87.31% ACC@5: 96.80%: 100%|██████████| 79/79 [00:19<00:00,  4.03it/s]\n","Test Epoch: [76/100] lr: 0.0001 Loss: 0.7221 ACC@1: 82.42% ACC@5: 95.73%: 100%|██████████| 79/79 [00:06<00:00, 12.17it/s]\n","Train Epoch: [77/100] lr: 0.0001 Loss: 0.3836 ACC@1: 86.79% ACC@5: 96.37%: 100%|██████████| 79/79 [00:19<00:00,  4.07it/s]\n","Test Epoch: [77/100] lr: 0.0001 Loss: 0.7297 ACC@1: 82.09% ACC@5: 95.74%: 100%|██████████| 79/79 [00:05<00:00, 14.89it/s]\n","Train Epoch: [78/100] lr: 0.0001 Loss: 0.3518 ACC@1: 87.60% ACC@5: 96.98%: 100%|██████████| 79/79 [00:18<00:00,  4.17it/s]\n","Test Epoch: [78/100] lr: 0.0001 Loss: 0.7188 ACC@1: 82.20% ACC@5: 95.81%: 100%|██████████| 79/79 [00:05<00:00, 15.32it/s]\n","Train Epoch: [79/100] lr: 0.0001 Loss: 0.3476 ACC@1: 88.00% ACC@5: 97.01%: 100%|██████████| 79/79 [00:19<00:00,  4.11it/s]\n","Test Epoch: [79/100] lr: 0.0001 Loss: 0.7187 ACC@1: 82.41% ACC@5: 95.91%: 100%|██████████| 79/79 [00:05<00:00, 14.93it/s]\n","Train Epoch: [80/100] lr: 0.0001 Loss: 0.3522 ACC@1: 88.01% ACC@5: 97.08%: 100%|██████████| 79/79 [00:19<00:00,  4.09it/s]\n","Test Epoch: [80/100] lr: 0.0000 Loss: 0.7204 ACC@1: 82.72% ACC@5: 96.15%: 100%|██████████| 79/79 [00:05<00:00, 14.95it/s]\n","Train Epoch: [81/100] lr: 0.0000 Loss: 0.3554 ACC@1: 87.64% ACC@5: 96.81%: 100%|██████████| 79/79 [00:19<00:00,  4.13it/s]\n","Test Epoch: [81/100] lr: 0.0000 Loss: 0.7211 ACC@1: 82.65% ACC@5: 96.14%: 100%|██████████| 79/79 [00:05<00:00, 15.35it/s]\n","Train Epoch: [82/100] lr: 0.0000 Loss: 0.3439 ACC@1: 88.00% ACC@5: 97.09%: 100%|██████████| 79/79 [00:19<00:00,  4.15it/s]\n","Test Epoch: [82/100] lr: 0.0000 Loss: 0.7159 ACC@1: 82.65% ACC@5: 96.11%: 100%|██████████| 79/79 [00:05<00:00, 14.81it/s]\n","Train Epoch: [83/100] lr: 0.0000 Loss: 0.3525 ACC@1: 87.87% ACC@5: 96.86%: 100%|██████████| 79/79 [00:19<00:00,  4.12it/s]\n","Test Epoch: [83/100] lr: 0.0000 Loss: 0.7191 ACC@1: 82.57% ACC@5: 95.98%: 100%|██████████| 79/79 [00:05<00:00, 15.05it/s]\n","Train Epoch: [84/100] lr: 0.0000 Loss: 0.3406 ACC@1: 88.06% ACC@5: 97.43%: 100%|██████████| 79/79 [00:19<00:00,  4.08it/s]\n","Test Epoch: [84/100] lr: 0.0000 Loss: 0.7358 ACC@1: 82.27% ACC@5: 95.90%: 100%|██████████| 79/79 [00:05<00:00, 15.05it/s]\n","Train Epoch: [85/100] lr: 0.0000 Loss: 0.3457 ACC@1: 87.96% ACC@5: 97.15%: 100%|██████████| 79/79 [00:19<00:00,  4.03it/s]\n","Test Epoch: [85/100] lr: 0.0000 Loss: 0.7288 ACC@1: 82.37% ACC@5: 96.00%: 100%|██████████| 79/79 [00:05<00:00, 14.86it/s]\n","Train Epoch: [86/100] lr: 0.0000 Loss: 0.3428 ACC@1: 88.33% ACC@5: 96.95%: 100%|██████████| 79/79 [00:20<00:00,  3.86it/s]\n","Test Epoch: [86/100] lr: 0.0000 Loss: 0.7237 ACC@1: 82.34% ACC@5: 96.06%: 100%|██████████| 79/79 [00:05<00:00, 14.66it/s]\n","Train Epoch: [87/100] lr: 0.0000 Loss: 0.3321 ACC@1: 88.37% ACC@5: 97.17%: 100%|██████████| 79/79 [00:19<00:00,  4.08it/s]\n","Test Epoch: [87/100] lr: 0.0000 Loss: 0.7273 ACC@1: 82.30% ACC@5: 96.06%: 100%|██████████| 79/79 [00:05<00:00, 15.34it/s]\n","Train Epoch: [88/100] lr: 0.0000 Loss: 0.3485 ACC@1: 87.79% ACC@5: 97.10%: 100%|██████████| 79/79 [00:19<00:00,  4.13it/s]\n","Test Epoch: [88/100] lr: 0.0000 Loss: 0.7255 ACC@1: 82.37% ACC@5: 96.04%: 100%|██████████| 79/79 [00:05<00:00, 15.03it/s]\n","Train Epoch: [89/100] lr: 0.0000 Loss: 0.3453 ACC@1: 88.04% ACC@5: 97.20%: 100%|██████████| 79/79 [00:19<00:00,  4.11it/s]\n","Test Epoch: [89/100] lr: 0.0000 Loss: 0.7240 ACC@1: 82.31% ACC@5: 96.00%: 100%|██████████| 79/79 [00:05<00:00, 15.28it/s]\n","Train Epoch: [90/100] lr: 0.0000 Loss: 0.3273 ACC@1: 88.78% ACC@5: 97.35%: 100%|██████████| 79/79 [00:19<00:00,  4.07it/s]\n","Test Epoch: [90/100] lr: 0.0000 Loss: 0.7278 ACC@1: 82.58% ACC@5: 95.95%: 100%|██████████| 79/79 [00:05<00:00, 14.95it/s]\n","Train Epoch: [91/100] lr: 0.0000 Loss: 0.3327 ACC@1: 88.44% ACC@5: 97.02%: 100%|██████████| 79/79 [00:19<00:00,  4.15it/s]\n","Test Epoch: [91/100] lr: 0.0000 Loss: 0.7137 ACC@1: 82.74% ACC@5: 95.98%: 100%|██████████| 79/79 [00:05<00:00, 14.95it/s]\n","Train Epoch: [92/100] lr: 0.0000 Loss: 0.3315 ACC@1: 88.53% ACC@5: 97.14%: 100%|██████████| 79/79 [00:19<00:00,  4.16it/s]\n","Test Epoch: [92/100] lr: 0.0000 Loss: 0.7322 ACC@1: 82.35% ACC@5: 95.94%: 100%|██████████| 79/79 [00:05<00:00, 15.46it/s]\n","Train Epoch: [93/100] lr: 0.0000 Loss: 0.3335 ACC@1: 88.45% ACC@5: 97.30%: 100%|██████████| 79/79 [00:18<00:00,  4.20it/s]\n","Test Epoch: [93/100] lr: 0.0000 Loss: 0.7399 ACC@1: 82.34% ACC@5: 95.92%: 100%|██████████| 79/79 [00:05<00:00, 15.13it/s]\n","Train Epoch: [94/100] lr: 0.0000 Loss: 0.3348 ACC@1: 88.56% ACC@5: 97.35%: 100%|██████████| 79/79 [00:19<00:00,  4.05it/s]\n","Test Epoch: [94/100] lr: 0.0000 Loss: 0.7484 ACC@1: 82.24% ACC@5: 95.93%: 100%|██████████| 79/79 [00:05<00:00, 14.86it/s]\n","Train Epoch: [95/100] lr: 0.0000 Loss: 0.3421 ACC@1: 88.27% ACC@5: 97.03%: 100%|██████████| 79/79 [00:19<00:00,  4.09it/s]\n","Test Epoch: [95/100] lr: 0.0000 Loss: 0.7423 ACC@1: 82.35% ACC@5: 95.92%: 100%|██████████| 79/79 [00:05<00:00, 15.53it/s]\n","Train Epoch: [96/100] lr: 0.0000 Loss: 0.3332 ACC@1: 88.40% ACC@5: 97.00%: 100%|██████████| 79/79 [00:21<00:00,  3.74it/s]\n","Test Epoch: [96/100] lr: 0.0000 Loss: 0.7239 ACC@1: 82.50% ACC@5: 96.02%: 100%|██████████| 79/79 [00:05<00:00, 14.84it/s]\n","Train Epoch: [97/100] lr: 0.0000 Loss: 0.3415 ACC@1: 88.31% ACC@5: 97.04%: 100%|██████████| 79/79 [00:19<00:00,  4.07it/s]\n","Test Epoch: [97/100] lr: 0.0000 Loss: 0.7363 ACC@1: 82.18% ACC@5: 96.04%: 100%|██████████| 79/79 [00:05<00:00, 15.01it/s]\n","Train Epoch: [98/100] lr: 0.0000 Loss: 0.3248 ACC@1: 89.07% ACC@5: 97.64%: 100%|██████████| 79/79 [00:19<00:00,  4.13it/s]\n","Test Epoch: [98/100] lr: 0.0000 Loss: 0.7148 ACC@1: 82.49% ACC@5: 96.17%: 100%|██████████| 79/79 [00:05<00:00, 14.70it/s]\n","Train Epoch: [99/100] lr: 0.0000 Loss: 0.3377 ACC@1: 88.29% ACC@5: 97.20%: 100%|██████████| 79/79 [00:19<00:00,  4.10it/s]\n","Test Epoch: [99/100] lr: 0.0000 Loss: 0.7399 ACC@1: 82.35% ACC@5: 95.99%: 100%|██████████| 79/79 [00:05<00:00, 15.39it/s]\n","Train Epoch: [100/100] lr: 0.0000 Loss: 0.3326 ACC@1: 88.46% ACC@5: 97.31%: 100%|██████████| 79/79 [00:19<00:00,  4.14it/s]\n","Test Epoch: [100/100] lr: 0.0000 Loss: 0.7018 ACC@1: 82.85% ACC@5: 96.17%: 100%|██████████| 79/79 [00:05<00:00, 15.15it/s]"]},{"output_type":"stream","name":"stdout","text":["[88.46000000000001]\n","[82.85]\n","88.46000000000001\n","82.85\n","0.0\n","0.0\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["import random\n","from torchvision.io.image import read_image\n","from torchvision.transforms.functional import normalize, resize, to_pil_image\n","from torchvision.models import resnet18\n","from torchcam.methods import GradCAMpp\n","\n","from time import time \n","import matplotlib.pyplot as plt\n","from torchcam.utils import overlay_mask\n","from typing import Any, Callable, Optional, Tuple\n","\n","import numpy as np\n","from PIL import Image\n","\n","def train_val(net, data_loader, train_optimizer):\n","    global lr\n","    schedule = [60, 80]\n","    criterion = nn.CrossEntropyLoss().cuda()\n","    is_train = train_optimizer is not None\n","    net.train() if is_train else net.eval()\n","    sa = 0\n","    total_loss, total_correct_1, total_correct_3, total_num, data_bar = 0.0, 0.0, 0.0, 0, tqdm(data_loader, position=0, leave=True)\n","    with (torch.enable_grad() if is_train else torch.no_grad()):\n","        for data, target,indexes in data_bar:\n","            data, target = data.cuda(non_blocking=True), target.cuda(non_blocking=True)\n","            out = net(data)\n","            loss = criterion(out, target)\n","\n","            if is_train:\n","                train_optimizer.zero_grad()\n","                loss.backward()\n","                train_optimizer.step()\n","\n","\n","            total_num += data.size(0)\n","            total_loss += loss.item() * data.size(0)\n","            prediction = torch.argsort(out, dim=-1, descending=True)\n","            total_correct_1 += torch.sum((prediction[:, 0:1] == target.unsqueeze(dim=-1)).any(dim=-1).float()).item()\n","            total_correct_3 += torch.sum((prediction[:, 0:3] == target.unsqueeze(dim=-1)).any(dim=-1).float()).item()\n","            \n","            data_bar.set_description('{} Epoch: [{}/{}] lr: {:.4f} Loss: {:.4f} ACC@1: {:.2f}% ACC@5: {:.2f}%'\n","                                     .format('Train' if is_train else 'Test', epoch, epochs, lr, total_loss / total_num,\n","                                             total_correct_1 / total_num * 100, total_correct_3 / total_num * 100))\n","        if is_train:  \n","          if schedule is not None:\n","            for milestone in schedule:\n","                lr *= 0.1 if epoch == milestone else 1.\n","            for param_group in train_optimizer.param_groups:\n","                param_group['lr'] = lr\n","\n","        return total_correct_1/total_num * 100\n","\n","train_acc = []\n","test_acc = []\n","for i in range(1):\n","  print(f\"Random Seed: {i}\")\n","  best_train = 0\n","  best_test = 0\n","  np.random.seed(i)\n","\n","  indices = get_indices(train_data, 1000)\n","  sampler = torch.utils.data.SubsetRandomSampler(indices)\n","\n","  train_loader = DataLoader(train_data, batch_size=batch_size, sampler = sampler , num_workers=16, pin_memory=True)\n","  resnet18 = ModelBase()\n","  resnet18.cuda()\n","  lr = 1e-3\n","  optimizer = torch.optim.Adam(resnet18.parameters(), lr=lr, weight_decay = 1e-5)\n","  epoch_start = 1\n","  epochs = 100\n","  for epoch in range(epoch_start, epochs+1):\n","    acc1 = train_val(resnet18, train_loader, optimizer)\n","    acc2 = train_val(resnet18, test_loader, None)\n","    if acc1 > best_train:\n","      best_train = acc1\n","    if acc2 > best_test:\n","      best_test = acc2\n","  train_acc.append(acc1)\n","  test_acc.append(acc2)\n","  \n","  torch.save(resnet18.state_dict(), \"/content/drive/My Drive/Models/Supervised/CAMsupervisedsample3.pth\")\n","print(train_acc)\n","print(test_acc)\n","import numpy as np\n","train_acc = np.array(train_acc)\n","test_acc = np.array(test_acc)\n","print(np.mean(train_acc))\n","print(np.mean(test_acc))\n","print(np.std(train_acc))\n","print(np.std(test_acc))\n"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":962,"status":"ok","timestamp":1669298128762,"user":{"displayName":"Souw Chuan Neo","userId":"08556196228836469917"},"user_tz":-480},"id":"xM_M25-DdS-w"},"outputs":[],"source":["class ModelBase(nn.Module):\n","    \"\"\"\n","    Common CIFAR ResNet recipe.\n","    Comparing with ImageNet ResNet recipe, it:\n","    (i) replaces conv1 with kernel=3, str=1\n","    (ii) removes pool1\n","    \"\"\"\n","    def __init__(self, feature_dim=10, arch='resnet18', bn_splits=16):\n","        super(ModelBase, self).__init__()\n","\n","        # use split batchnorm\n","        norm_layer = nn.BatchNorm2d\n","        # get specified resnet model\n","        resnet_arch = getattr(resnet, arch)\n","        net = resnet_arch(num_classes=feature_dim, norm_layer=norm_layer)\n","\n","        # make changes to original resnet\n","        self.net = []\n","        for name, module in net.named_children():\n","            if name == 'conv1':\n","                module = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n","            if isinstance(module, nn.MaxPool2d):\n","                continue\n","            if isinstance(module, nn.Linear):\n","                self.net.append(nn.Flatten(1))\n","            self.net.append(module)\n","\n","        # build net\n","        self.net = nn.Sequential(*self.net)\n","\n","    def forward(self, x):\n","        x = self.net(x)\n","        x = torch.flatten(x,start_dim=1)\n","        # note: not normalized here\n","        return x\n","class classifier(nn.Module):\n","\n","    def __init__(self, feature_dim=10, arch='resnet18', bn_splits=16):\n","        super(classifier, self).__init__()\n","\n","        self.net = nn.Linear(1024,10)\n","\n","    def forward(self, x):\n","        x = self.net(x)\n","        # note: not normalized here\n","        return x\n","supervised = ModelBase()\n","supervised.load_state_dict(torch.load(\"/content/drive/My Drive/Models/Supervised/supervisedresnet18v1.1.pth\"))\n","cutout = ModelBase()\n","cutout.load_state_dict(torch.load(\"/content/drive/My Drive/Models/Supervised/CAMsupervisedsample3.pth\"))\n","supervised.net =supervised.net[:-1]\n","cutout.net =cutout.net[:-1]\n","for i in supervised.parameters():\n","  i.requires_grad = False\n","for i in cutout.parameters():\n","  i.requires_grad = False\n","supervised=supervised.cuda()\n","cutout = cutout.cuda()\n","finallayer = classifier()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vbkq9JtH757B","outputId":"b6da8f0e-263d-421e-b8df-f818e4862754"},"outputs":[{"output_type":"stream","name":"stdout","text":["Random Seed: 0\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch: [1/100] lr: 0.0001 Loss: 1.5209 ACC@1: 63.60% ACC@5: 84.75%: 100%|██████████| 79/79 [00:17<00:00,  4.56it/s]\n","Test Epoch: [1/100] lr: 0.0001 Loss: 0.9562 ACC@1: 80.75% ACC@5: 95.78%: 100%|██████████| 79/79 [00:07<00:00, 10.40it/s]\n","Train Epoch: [2/100] lr: 0.0001 Loss: 0.7774 ACC@1: 86.75% ACC@5: 96.88%: 100%|██████████| 79/79 [00:17<00:00,  4.63it/s]\n","Test Epoch: [2/100] lr: 0.0001 Loss: 0.6829 ACC@1: 82.51% ACC@5: 96.44%: 100%|██████████| 79/79 [00:07<00:00, 10.36it/s]\n","Train Epoch: [3/100] lr: 0.0001 Loss: 0.5874 ACC@1: 87.96% ACC@5: 97.07%: 100%|██████████| 79/79 [00:16<00:00,  4.75it/s]\n","Test Epoch: [3/100] lr: 0.0001 Loss: 0.5905 ACC@1: 82.85% ACC@5: 96.49%: 100%|██████████| 79/79 [00:07<00:00, 10.53it/s]\n","Train Epoch: [4/100] lr: 0.0001 Loss: 0.4975 ACC@1: 88.47% ACC@5: 97.03%: 100%|██████████| 79/79 [00:16<00:00,  4.76it/s]\n","Test Epoch: [4/100] lr: 0.0001 Loss: 0.5487 ACC@1: 82.89% ACC@5: 96.55%: 100%|██████████| 79/79 [00:07<00:00, 10.54it/s]\n","Train Epoch: [5/100] lr: 0.0001 Loss: 0.4658 ACC@1: 88.13% ACC@5: 96.95%: 100%|██████████| 79/79 [00:16<00:00,  4.81it/s]\n","Test Epoch: [5/100] lr: 0.0001 Loss: 0.5293 ACC@1: 83.00% ACC@5: 96.43%: 100%|██████████| 79/79 [00:07<00:00, 10.52it/s]\n","Train Epoch: [6/100] lr: 0.0001 Loss: 0.4362 ACC@1: 87.88% ACC@5: 97.16%: 100%|██████████| 79/79 [00:18<00:00,  4.22it/s]\n","Test Epoch: [6/100] lr: 0.0001 Loss: 0.5174 ACC@1: 82.92% ACC@5: 96.51%: 100%|██████████| 79/79 [00:07<00:00, 10.18it/s]\n","Train Epoch: [7/100] lr: 0.0001 Loss: 0.4007 ACC@1: 89.16% ACC@5: 97.12%: 100%|██████████| 79/79 [00:16<00:00,  4.71it/s]\n","Test Epoch: [7/100] lr: 0.0001 Loss: 0.5124 ACC@1: 83.04% ACC@5: 96.55%: 100%|██████████| 79/79 [00:07<00:00, 10.31it/s]\n","Train Epoch: [8/100] lr: 0.0001 Loss: 0.3820 ACC@1: 89.03% ACC@5: 97.45%: 100%|██████████| 79/79 [00:16<00:00,  4.66it/s]\n","Test Epoch: [8/100] lr: 0.0001 Loss: 0.5088 ACC@1: 83.21% ACC@5: 96.47%: 100%|██████████| 79/79 [00:07<00:00, 10.28it/s]\n","Train Epoch: [9/100] lr: 0.0001 Loss: 0.3817 ACC@1: 88.78% ACC@5: 97.11%: 100%|██████████| 79/79 [00:16<00:00,  4.68it/s]\n","Test Epoch: [9/100] lr: 0.0001 Loss: 0.5081 ACC@1: 83.18% ACC@5: 96.43%: 100%|██████████| 79/79 [00:07<00:00, 10.31it/s]\n","Train Epoch: [10/100] lr: 0.0001 Loss: 0.3663 ACC@1: 88.65% ACC@5: 97.29%: 100%|██████████| 79/79 [00:17<00:00,  4.60it/s]\n","Test Epoch: [10/100] lr: 0.0001 Loss: 0.5089 ACC@1: 83.16% ACC@5: 96.42%: 100%|██████████| 79/79 [00:07<00:00, 10.38it/s]\n","Train Epoch: [11/100] lr: 0.0001 Loss: 0.3567 ACC@1: 89.14% ACC@5: 97.06%: 100%|██████████| 79/79 [00:16<00:00,  4.71it/s]\n","Test Epoch: [11/100] lr: 0.0001 Loss: 0.5094 ACC@1: 83.23% ACC@5: 96.54%: 100%|██████████| 79/79 [00:07<00:00, 10.44it/s]\n","Train Epoch: [12/100] lr: 0.0001 Loss: 0.3507 ACC@1: 88.78% ACC@5: 97.17%: 100%|██████████| 79/79 [00:17<00:00,  4.64it/s]\n","Test Epoch: [12/100] lr: 0.0001 Loss: 0.5111 ACC@1: 83.31% ACC@5: 96.57%: 100%|██████████| 79/79 [00:07<00:00, 10.26it/s]\n","Train Epoch: [13/100] lr: 0.0001 Loss: 0.3423 ACC@1: 89.01% ACC@5: 97.37%: 100%|██████████| 79/79 [00:16<00:00,  4.68it/s]\n","Test Epoch: [13/100] lr: 0.0001 Loss: 0.5148 ACC@1: 83.27% ACC@5: 96.51%: 100%|██████████| 79/79 [00:07<00:00, 10.26it/s]\n","Train Epoch: [14/100] lr: 0.0001 Loss: 0.3459 ACC@1: 89.02% ACC@5: 96.94%: 100%|██████████| 79/79 [00:16<00:00,  4.66it/s]\n","Test Epoch: [14/100] lr: 0.0001 Loss: 0.5175 ACC@1: 83.23% ACC@5: 96.53%: 100%|██████████| 79/79 [00:07<00:00, 10.29it/s]\n","Train Epoch: [15/100] lr: 0.0001 Loss: 0.3317 ACC@1: 89.16% ACC@5: 97.49%: 100%|██████████| 79/79 [00:16<00:00,  4.74it/s]\n","Test Epoch: [15/100] lr: 0.0001 Loss: 0.5207 ACC@1: 83.43% ACC@5: 96.41%: 100%|██████████| 79/79 [00:07<00:00,  9.90it/s]\n","Train Epoch: [16/100] lr: 0.0001 Loss: 0.3348 ACC@1: 89.17% ACC@5: 97.21%: 100%|██████████| 79/79 [00:17<00:00,  4.48it/s]\n","Test Epoch: [16/100] lr: 0.0001 Loss: 0.5250 ACC@1: 83.32% ACC@5: 96.47%: 100%|██████████| 79/79 [00:07<00:00, 10.24it/s]\n","Train Epoch: [17/100] lr: 0.0001 Loss: 0.3355 ACC@1: 88.87% ACC@5: 97.12%: 100%|██████████| 79/79 [00:17<00:00,  4.63it/s]\n","Test Epoch: [17/100] lr: 0.0001 Loss: 0.5277 ACC@1: 83.27% ACC@5: 96.46%: 100%|██████████| 79/79 [00:07<00:00, 10.37it/s]\n","Train Epoch: [18/100] lr: 0.0001 Loss: 0.3344 ACC@1: 88.99% ACC@5: 97.44%: 100%|██████████| 79/79 [00:16<00:00,  4.69it/s]\n","Test Epoch: [18/100] lr: 0.0001 Loss: 0.5292 ACC@1: 83.30% ACC@5: 96.55%: 100%|██████████| 79/79 [00:07<00:00, 10.38it/s]\n","Train Epoch: [19/100] lr: 0.0001 Loss: 0.3477 ACC@1: 88.16% ACC@5: 96.93%: 100%|██████████| 79/79 [00:17<00:00,  4.62it/s]\n","Test Epoch: [19/100] lr: 0.0001 Loss: 0.5325 ACC@1: 83.33% ACC@5: 96.52%: 100%|██████████| 79/79 [00:07<00:00, 10.23it/s]\n","Train Epoch: [20/100] lr: 0.0001 Loss: 0.3190 ACC@1: 89.50% ACC@5: 97.48%: 100%|██████████| 79/79 [00:16<00:00,  4.65it/s]\n","Test Epoch: [20/100] lr: 0.0001 Loss: 0.5366 ACC@1: 83.30% ACC@5: 96.45%: 100%|██████████| 79/79 [00:07<00:00, 10.31it/s]\n","Train Epoch: [21/100] lr: 0.0001 Loss: 0.3307 ACC@1: 88.86% ACC@5: 97.20%: 100%|██████████| 79/79 [00:16<00:00,  4.69it/s]\n","Test Epoch: [21/100] lr: 0.0001 Loss: 0.5386 ACC@1: 83.39% ACC@5: 96.52%: 100%|██████████| 79/79 [00:07<00:00, 10.30it/s]\n","Train Epoch: [22/100] lr: 0.0001 Loss: 0.3097 ACC@1: 89.47% ACC@5: 97.55%: 100%|██████████| 79/79 [00:16<00:00,  4.65it/s]\n","Test Epoch: [22/100] lr: 0.0001 Loss: 0.5404 ACC@1: 83.33% ACC@5: 96.62%: 100%|██████████| 79/79 [00:07<00:00, 10.24it/s]\n","Train Epoch: [23/100] lr: 0.0001 Loss: 0.3267 ACC@1: 88.87% ACC@5: 97.27%: 100%|██████████| 79/79 [00:17<00:00,  4.60it/s]\n","Test Epoch: [23/100] lr: 0.0001 Loss: 0.5435 ACC@1: 83.32% ACC@5: 96.54%: 100%|██████████| 79/79 [00:07<00:00, 10.28it/s]\n","Train Epoch: [24/100] lr: 0.0001 Loss: 0.3179 ACC@1: 89.26% ACC@5: 97.34%: 100%|██████████| 79/79 [00:17<00:00,  4.57it/s]\n","Test Epoch: [24/100] lr: 0.0001 Loss: 0.5462 ACC@1: 83.43% ACC@5: 96.51%: 100%|██████████| 79/79 [00:07<00:00, 10.19it/s]\n","Train Epoch: [25/100] lr: 0.0001 Loss: 0.3131 ACC@1: 89.31% ACC@5: 97.59%: 100%|██████████| 79/79 [00:17<00:00,  4.60it/s]\n","Test Epoch: [25/100] lr: 0.0001 Loss: 0.5488 ACC@1: 83.29% ACC@5: 96.61%: 100%|██████████| 79/79 [00:08<00:00,  9.49it/s]\n","Train Epoch: [26/100] lr: 0.0001 Loss: 0.3174 ACC@1: 88.98% ACC@5: 97.41%: 100%|██████████| 79/79 [00:17<00:00,  4.64it/s]\n","Test Epoch: [26/100] lr: 0.0001 Loss: 0.5512 ACC@1: 83.35% ACC@5: 96.52%: 100%|██████████| 79/79 [00:07<00:00, 10.29it/s]\n","Train Epoch: [27/100] lr: 0.0001 Loss: 0.3225 ACC@1: 89.04% ACC@5: 97.14%: 100%|██████████| 79/79 [00:17<00:00,  4.57it/s]\n","Test Epoch: [27/100] lr: 0.0001 Loss: 0.5545 ACC@1: 83.38% ACC@5: 96.55%: 100%|██████████| 79/79 [00:07<00:00, 10.21it/s]\n","Train Epoch: [28/100] lr: 0.0001 Loss: 0.3285 ACC@1: 88.72% ACC@5: 97.29%: 100%|██████████| 79/79 [00:17<00:00,  4.64it/s]\n","Test Epoch: [28/100] lr: 0.0001 Loss: 0.5569 ACC@1: 83.41% ACC@5: 96.55%: 100%|██████████| 79/79 [00:07<00:00, 10.28it/s]\n","Train Epoch: [29/100] lr: 0.0001 Loss: 0.3276 ACC@1: 88.90% ACC@5: 97.30%: 100%|██████████| 79/79 [00:17<00:00,  4.59it/s]\n","Test Epoch: [29/100] lr: 0.0001 Loss: 0.5567 ACC@1: 83.39% ACC@5: 96.58%: 100%|██████████| 79/79 [00:07<00:00, 10.35it/s]\n","Train Epoch: [30/100] lr: 0.0001 Loss: 0.3171 ACC@1: 89.30% ACC@5: 97.41%: 100%|██████████| 79/79 [00:16<00:00,  4.69it/s]\n","Test Epoch: [30/100] lr: 0.0001 Loss: 0.5593 ACC@1: 83.40% ACC@5: 96.50%: 100%|██████████| 79/79 [00:07<00:00, 10.20it/s]\n","Train Epoch: [31/100] lr: 0.0001 Loss: 0.3162 ACC@1: 88.77% ACC@5: 97.32%: 100%|██████████| 79/79 [00:19<00:00,  4.11it/s]\n","Test Epoch: [31/100] lr: 0.0001 Loss: 0.5586 ACC@1: 83.40% ACC@5: 96.54%: 100%|██████████| 79/79 [00:07<00:00, 10.22it/s]\n","Train Epoch: [32/100] lr: 0.0001 Loss: 0.3119 ACC@1: 89.27% ACC@5: 97.42%: 100%|██████████| 79/79 [00:17<00:00,  4.58it/s]\n","Test Epoch: [32/100] lr: 0.0001 Loss: 0.5622 ACC@1: 83.41% ACC@5: 96.62%: 100%|██████████| 79/79 [00:07<00:00, 10.23it/s]\n","Train Epoch: [33/100] lr: 0.0001 Loss: 0.3153 ACC@1: 88.95% ACC@5: 97.47%: 100%|██████████| 79/79 [00:17<00:00,  4.57it/s]\n","Test Epoch: [33/100] lr: 0.0001 Loss: 0.5643 ACC@1: 83.50% ACC@5: 96.51%: 100%|██████████| 79/79 [00:07<00:00, 10.20it/s]\n","Train Epoch: [34/100] lr: 0.0001 Loss: 0.3097 ACC@1: 89.20% ACC@5: 97.41%: 100%|██████████| 79/79 [00:16<00:00,  4.65it/s]\n","Test Epoch: [34/100] lr: 0.0001 Loss: 0.5671 ACC@1: 83.50% ACC@5: 96.58%: 100%|██████████| 79/79 [00:07<00:00, 10.30it/s]\n","Train Epoch: [35/100] lr: 0.0001 Loss: 0.3147 ACC@1: 89.34% ACC@5: 97.28%: 100%|██████████| 79/79 [00:19<00:00,  4.12it/s]\n","Test Epoch: [35/100] lr: 0.0001 Loss: 0.5676 ACC@1: 83.49% ACC@5: 96.53%: 100%|██████████| 79/79 [00:07<00:00, 10.20it/s]\n","Train Epoch: [36/100] lr: 0.0001 Loss: 0.3119 ACC@1: 89.12% ACC@5: 97.40%: 100%|██████████| 79/79 [00:17<00:00,  4.57it/s]\n","Test Epoch: [36/100] lr: 0.0001 Loss: 0.5702 ACC@1: 83.50% ACC@5: 96.61%: 100%|██████████| 79/79 [00:07<00:00, 10.23it/s]\n","Train Epoch: [37/100] lr: 0.0001 Loss: 0.3198 ACC@1: 88.96% ACC@5: 97.11%: 100%|██████████| 79/79 [00:16<00:00,  4.69it/s]\n","Test Epoch: [37/100] lr: 0.0001 Loss: 0.5713 ACC@1: 83.58% ACC@5: 96.52%: 100%|██████████| 79/79 [00:07<00:00, 10.12it/s]\n","Train Epoch: [38/100] lr: 0.0001 Loss: 0.3212 ACC@1: 88.84% ACC@5: 97.32%: 100%|██████████| 79/79 [00:17<00:00,  4.60it/s]\n","Test Epoch: [38/100] lr: 0.0001 Loss: 0.5736 ACC@1: 83.47% ACC@5: 96.63%: 100%|██████████| 79/79 [00:07<00:00, 10.31it/s]\n","Train Epoch: [39/100] lr: 0.0001 Loss: 0.3076 ACC@1: 89.46% ACC@5: 97.42%: 100%|██████████| 79/79 [00:17<00:00,  4.63it/s]\n","Test Epoch: [39/100] lr: 0.0001 Loss: 0.5732 ACC@1: 83.53% ACC@5: 96.49%: 100%|██████████| 79/79 [00:07<00:00, 10.26it/s]\n","Train Epoch: [40/100] lr: 0.0001 Loss: 0.3115 ACC@1: 89.26% ACC@5: 97.35%: 100%|██████████| 79/79 [00:19<00:00,  4.15it/s]\n","Test Epoch: [40/100] lr: 0.0001 Loss: 0.5765 ACC@1: 83.41% ACC@5: 96.53%: 100%|██████████| 79/79 [00:07<00:00, 10.16it/s]\n","Train Epoch: [41/100] lr: 0.0001 Loss: 0.3066 ACC@1: 89.30% ACC@5: 97.52%: 100%|██████████| 79/79 [00:17<00:00,  4.59it/s]\n","Test Epoch: [41/100] lr: 0.0001 Loss: 0.5772 ACC@1: 83.39% ACC@5: 96.49%: 100%|██████████| 79/79 [00:07<00:00, 10.31it/s]\n","Train Epoch: [42/100] lr: 0.0001 Loss: 0.3082 ACC@1: 88.99% ACC@5: 97.34%: 100%|██████████| 79/79 [00:17<00:00,  4.62it/s]\n","Test Epoch: [42/100] lr: 0.0001 Loss: 0.5769 ACC@1: 83.51% ACC@5: 96.54%: 100%|██████████| 79/79 [00:07<00:00, 10.09it/s]\n","Train Epoch: [43/100] lr: 0.0001 Loss: 0.3045 ACC@1: 89.30% ACC@5: 97.49%: 100%|██████████| 79/79 [00:16<00:00,  4.69it/s]\n","Test Epoch: [43/100] lr: 0.0001 Loss: 0.5798 ACC@1: 83.44% ACC@5: 96.57%: 100%|██████████| 79/79 [00:07<00:00, 10.31it/s]\n","Train Epoch: [44/100] lr: 0.0001 Loss: 0.3128 ACC@1: 89.11% ACC@5: 97.40%: 100%|██████████| 79/79 [00:16<00:00,  4.74it/s]\n","Test Epoch: [44/100] lr: 0.0001 Loss: 0.5791 ACC@1: 83.42% ACC@5: 96.53%: 100%|██████████| 79/79 [00:08<00:00,  9.85it/s]\n","Train Epoch: [45/100] lr: 0.0001 Loss: 0.3087 ACC@1: 89.43% ACC@5: 97.21%: 100%|██████████| 79/79 [00:17<00:00,  4.52it/s]\n","Test Epoch: [45/100] lr: 0.0001 Loss: 0.5824 ACC@1: 83.26% ACC@5: 96.52%: 100%|██████████| 79/79 [00:07<00:00, 10.13it/s]\n","Train Epoch: [46/100] lr: 0.0001 Loss: 0.3204 ACC@1: 89.20% ACC@5: 97.15%: 100%|██████████| 79/79 [00:16<00:00,  4.65it/s]\n","Test Epoch: [46/100] lr: 0.0001 Loss: 0.5798 ACC@1: 83.42% ACC@5: 96.53%: 100%|██████████| 79/79 [00:07<00:00, 10.24it/s]\n","Train Epoch: [47/100] lr: 0.0001 Loss: 0.3179 ACC@1: 88.97% ACC@5: 97.16%: 100%|██████████| 79/79 [00:16<00:00,  4.68it/s]\n","Test Epoch: [47/100] lr: 0.0001 Loss: 0.5821 ACC@1: 83.48% ACC@5: 96.57%: 100%|██████████| 79/79 [00:07<00:00, 10.25it/s]\n","Train Epoch: [48/100] lr: 0.0001 Loss: 0.3083 ACC@1: 89.37% ACC@5: 97.26%: 100%|██████████| 79/79 [00:16<00:00,  4.68it/s]\n","Test Epoch: [48/100] lr: 0.0001 Loss: 0.5864 ACC@1: 83.35% ACC@5: 96.62%: 100%|██████████| 79/79 [00:07<00:00, 10.15it/s]\n","Train Epoch: [49/100] lr: 0.0001 Loss: 0.3066 ACC@1: 89.55% ACC@5: 97.36%: 100%|██████████| 79/79 [00:17<00:00,  4.57it/s]\n","Test Epoch: [49/100] lr: 0.0001 Loss: 0.5822 ACC@1: 83.43% ACC@5: 96.57%: 100%|██████████| 79/79 [00:07<00:00, 10.15it/s]\n","Train Epoch: [50/100] lr: 0.0001 Loss: 0.3171 ACC@1: 89.07% ACC@5: 97.51%: 100%|██████████| 79/79 [00:18<00:00,  4.37it/s]\n","Test Epoch: [50/100] lr: 0.0001 Loss: 0.5820 ACC@1: 83.47% ACC@5: 96.44%: 100%|██████████| 79/79 [00:08<00:00,  9.75it/s]\n","Train Epoch: [51/100] lr: 0.0001 Loss: 0.3250 ACC@1: 88.51% ACC@5: 97.23%: 100%|██████████| 79/79 [00:16<00:00,  4.73it/s]\n","Test Epoch: [51/100] lr: 0.0001 Loss: 0.5812 ACC@1: 83.44% ACC@5: 96.56%: 100%|██████████| 79/79 [00:07<00:00, 10.12it/s]\n","Train Epoch: [52/100] lr: 0.0001 Loss: 0.3212 ACC@1: 88.92% ACC@5: 97.50%: 100%|██████████| 79/79 [00:17<00:00,  4.63it/s]\n","Test Epoch: [52/100] lr: 0.0001 Loss: 0.5815 ACC@1: 83.52% ACC@5: 96.57%: 100%|██████████| 79/79 [00:07<00:00, 10.18it/s]\n","Train Epoch: [53/100] lr: 0.0001 Loss: 0.3110 ACC@1: 89.33% ACC@5: 97.56%: 100%|██████████| 79/79 [00:16<00:00,  4.67it/s]\n","Test Epoch: [53/100] lr: 0.0001 Loss: 0.5818 ACC@1: 83.56% ACC@5: 96.59%: 100%|██████████| 79/79 [00:07<00:00, 10.15it/s]\n","Train Epoch: [54/100] lr: 0.0001 Loss: 0.3010 ACC@1: 89.45% ACC@5: 97.45%: 100%|██████████| 79/79 [00:17<00:00,  4.65it/s]\n","Test Epoch: [54/100] lr: 0.0001 Loss: 0.5826 ACC@1: 83.50% ACC@5: 96.59%: 100%|██████████| 79/79 [00:07<00:00, 10.25it/s]\n","Train Epoch: [55/100] lr: 0.0001 Loss: 0.3047 ACC@1: 89.24% ACC@5: 97.43%: 100%|██████████| 79/79 [00:18<00:00,  4.17it/s]\n","Test Epoch: [55/100] lr: 0.0001 Loss: 0.5837 ACC@1: 83.59% ACC@5: 96.47%: 100%|██████████| 79/79 [00:07<00:00, 10.22it/s]\n","Train Epoch: [56/100] lr: 0.0001 Loss: 0.2997 ACC@1: 89.37% ACC@5: 97.53%: 100%|██████████| 79/79 [00:16<00:00,  4.72it/s]\n","Test Epoch: [56/100] lr: 0.0001 Loss: 0.5857 ACC@1: 83.49% ACC@5: 96.58%: 100%|██████████| 79/79 [00:07<00:00, 10.18it/s]\n","Train Epoch: [57/100] lr: 0.0001 Loss: 0.3164 ACC@1: 89.00% ACC@5: 97.19%: 100%|██████████| 79/79 [00:16<00:00,  4.65it/s]\n","Test Epoch: [57/100] lr: 0.0001 Loss: 0.5854 ACC@1: 83.62% ACC@5: 96.44%: 100%|██████████| 79/79 [00:07<00:00, 10.34it/s]\n","Train Epoch: [58/100] lr: 0.0001 Loss: 0.3095 ACC@1: 89.30% ACC@5: 97.63%: 100%|██████████| 79/79 [00:16<00:00,  4.66it/s]\n","Test Epoch: [58/100] lr: 0.0001 Loss: 0.5844 ACC@1: 83.51% ACC@5: 96.56%: 100%|██████████| 79/79 [00:07<00:00, 10.12it/s]\n","Train Epoch: [59/100] lr: 0.0001 Loss: 0.3045 ACC@1: 89.45% ACC@5: 97.44%: 100%|██████████| 79/79 [00:16<00:00,  4.72it/s]\n","Test Epoch: [59/100] lr: 0.0001 Loss: 0.5861 ACC@1: 83.56% ACC@5: 96.60%: 100%|██████████| 79/79 [00:07<00:00, 10.25it/s]\n","Train Epoch: [60/100] lr: 0.0001 Loss: 0.3093 ACC@1: 89.07% ACC@5: 97.48%: 100%|██████████| 79/79 [00:17<00:00,  4.63it/s]\n","Test Epoch: [60/100] lr: 0.0000 Loss: 0.5880 ACC@1: 83.49% ACC@5: 96.64%: 100%|██████████| 79/79 [00:08<00:00,  9.83it/s]\n","Train Epoch: [61/100] lr: 0.0000 Loss: 0.3149 ACC@1: 89.28% ACC@5: 97.31%: 100%|██████████| 79/79 [00:16<00:00,  4.68it/s]\n","Test Epoch: [61/100] lr: 0.0000 Loss: 0.5867 ACC@1: 83.45% ACC@5: 96.64%: 100%|██████████| 79/79 [00:07<00:00, 10.08it/s]\n","Train Epoch: [62/100] lr: 0.0000 Loss: 0.3097 ACC@1: 89.35% ACC@5: 97.49%: 100%|██████████| 79/79 [00:16<00:00,  4.69it/s]\n","Test Epoch: [62/100] lr: 0.0000 Loss: 0.5865 ACC@1: 83.41% ACC@5: 96.63%: 100%|██████████| 79/79 [00:07<00:00, 10.09it/s]\n","Train Epoch: [63/100] lr: 0.0000 Loss: 0.3062 ACC@1: 89.38% ACC@5: 97.45%: 100%|██████████| 79/79 [00:16<00:00,  4.70it/s]\n","Test Epoch: [63/100] lr: 0.0000 Loss: 0.5861 ACC@1: 83.41% ACC@5: 96.63%: 100%|██████████| 79/79 [00:07<00:00, 10.23it/s]\n","Train Epoch: [64/100] lr: 0.0000 Loss: 0.2946 ACC@1: 89.63% ACC@5: 97.47%: 100%|██████████| 79/79 [00:17<00:00,  4.62it/s]\n","Test Epoch: [64/100] lr: 0.0000 Loss: 0.5861 ACC@1: 83.42% ACC@5: 96.65%: 100%|██████████| 79/79 [00:07<00:00, 10.11it/s]\n","Train Epoch: [65/100] lr: 0.0000 Loss: 0.3039 ACC@1: 89.92% ACC@5: 97.56%: 100%|██████████| 79/79 [00:19<00:00,  4.11it/s]\n","Test Epoch: [65/100] lr: 0.0000 Loss: 0.5861 ACC@1: 83.42% ACC@5: 96.63%: 100%|██████████| 79/79 [00:07<00:00, 10.16it/s]\n","Train Epoch: [66/100] lr: 0.0000 Loss: 0.3132 ACC@1: 89.05% ACC@5: 97.44%: 100%|██████████| 79/79 [00:16<00:00,  4.67it/s]\n","Test Epoch: [66/100] lr: 0.0000 Loss: 0.5863 ACC@1: 83.41% ACC@5: 96.61%: 100%|██████████| 79/79 [00:07<00:00, 10.18it/s]\n","Train Epoch: [67/100] lr: 0.0000 Loss: 0.3081 ACC@1: 89.21% ACC@5: 97.33%: 100%|██████████| 79/79 [00:17<00:00,  4.56it/s]\n","Test Epoch: [67/100] lr: 0.0000 Loss: 0.5859 ACC@1: 83.44% ACC@5: 96.63%: 100%|██████████| 79/79 [00:07<00:00, 10.04it/s]\n","Train Epoch: [68/100] lr: 0.0000 Loss: 0.3145 ACC@1: 89.31% ACC@5: 97.35%: 100%|██████████| 79/79 [00:17<00:00,  4.55it/s]\n","Test Epoch: [68/100] lr: 0.0000 Loss: 0.5862 ACC@1: 83.38% ACC@5: 96.65%: 100%|██████████| 79/79 [00:07<00:00, 10.06it/s]\n","Train Epoch: [69/100] lr: 0.0000 Loss: 0.3073 ACC@1: 89.50% ACC@5: 97.32%: 100%|██████████| 79/79 [00:17<00:00,  4.46it/s]\n","Test Epoch: [69/100] lr: 0.0000 Loss: 0.5866 ACC@1: 83.42% ACC@5: 96.61%: 100%|██████████| 79/79 [00:07<00:00, 10.10it/s]\n","Train Epoch: [70/100] lr: 0.0000 Loss: 0.2975 ACC@1: 89.85% ACC@5: 97.50%: 100%|██████████| 79/79 [00:17<00:00,  4.54it/s]\n","Test Epoch: [70/100] lr: 0.0000 Loss: 0.5867 ACC@1: 83.45% ACC@5: 96.62%: 100%|██████████| 79/79 [00:07<00:00, 10.14it/s]\n","Train Epoch: [71/100] lr: 0.0000 Loss: 0.3146 ACC@1: 88.91% ACC@5: 97.22%: 100%|██████████| 79/79 [00:19<00:00,  4.05it/s]\n","Test Epoch: [71/100] lr: 0.0000 Loss: 0.5869 ACC@1: 83.41% ACC@5: 96.59%: 100%|██████████| 79/79 [00:07<00:00, 10.07it/s]\n","Train Epoch: [72/100] lr: 0.0000 Loss: 0.3073 ACC@1: 89.46% ACC@5: 97.31%: 100%|██████████| 79/79 [00:17<00:00,  4.51it/s]\n","Test Epoch: [72/100] lr: 0.0000 Loss: 0.5869 ACC@1: 83.42% ACC@5: 96.62%: 100%|██████████| 79/79 [00:07<00:00, 10.02it/s]\n","Train Epoch: [73/100] lr: 0.0000 Loss: 0.3138 ACC@1: 89.27% ACC@5: 97.20%: 100%|██████████| 79/79 [00:16<00:00,  4.65it/s]\n","Test Epoch: [73/100] lr: 0.0000 Loss: 0.5870 ACC@1: 83.47% ACC@5: 96.60%: 100%|██████████| 79/79 [00:07<00:00,  9.96it/s]\n","Train Epoch: [74/100] lr: 0.0000 Loss: 0.2952 ACC@1: 90.04% ACC@5: 97.74%: 100%|██████████| 79/79 [00:17<00:00,  4.55it/s]\n","Test Epoch: [74/100] lr: 0.0000 Loss: 0.5873 ACC@1: 83.42% ACC@5: 96.56%: 100%|██████████| 79/79 [00:07<00:00, 10.11it/s]\n","Train Epoch: [75/100] lr: 0.0000 Loss: 0.3022 ACC@1: 89.85% ACC@5: 97.38%: 100%|██████████| 79/79 [00:17<00:00,  4.63it/s]\n","Test Epoch: [75/100] lr: 0.0000 Loss: 0.5871 ACC@1: 83.46% ACC@5: 96.56%: 100%|██████████| 79/79 [00:08<00:00,  9.14it/s]\n","Train Epoch: [76/100] lr: 0.0000 Loss: 0.3123 ACC@1: 89.24% ACC@5: 97.49%: 100%|██████████| 79/79 [00:18<00:00,  4.34it/s]\n","Test Epoch: [76/100] lr: 0.0000 Loss: 0.5875 ACC@1: 83.46% ACC@5: 96.60%: 100%|██████████| 79/79 [00:07<00:00, 10.03it/s]\n","Train Epoch: [77/100] lr: 0.0000 Loss: 0.3214 ACC@1: 89.02% ACC@5: 97.21%: 100%|██████████| 79/79 [00:17<00:00,  4.64it/s]\n","Test Epoch: [77/100] lr: 0.0000 Loss: 0.5876 ACC@1: 83.49% ACC@5: 96.60%: 100%|██████████| 79/79 [00:07<00:00, 10.13it/s]\n","Train Epoch: [78/100] lr: 0.0000 Loss: 0.3070 ACC@1: 89.53% ACC@5: 97.64%: 100%|██████████| 79/79 [00:17<00:00,  4.62it/s]\n","Test Epoch: [78/100] lr: 0.0000 Loss: 0.5874 ACC@1: 83.47% ACC@5: 96.56%: 100%|██████████| 79/79 [00:07<00:00, 10.04it/s]\n","Train Epoch: [79/100] lr: 0.0000 Loss: 0.3111 ACC@1: 89.43% ACC@5: 97.33%: 100%|██████████| 79/79 [00:17<00:00,  4.63it/s]\n","Test Epoch: [79/100] lr: 0.0000 Loss: 0.5872 ACC@1: 83.44% ACC@5: 96.57%: 100%|██████████| 79/79 [00:07<00:00, 10.10it/s]\n","Train Epoch: [80/100] lr: 0.0000 Loss: 0.3085 ACC@1: 89.02% ACC@5: 97.54%: 100%|██████████| 79/79 [00:17<00:00,  4.60it/s]\n","Test Epoch: [80/100] lr: 0.0000 Loss: 0.5872 ACC@1: 83.44% ACC@5: 96.58%: 100%|██████████| 79/79 [00:08<00:00,  9.85it/s]\n","Train Epoch: [81/100] lr: 0.0000 Loss: 0.3064 ACC@1: 89.25% ACC@5: 97.52%: 100%|██████████| 79/79 [00:17<00:00,  4.64it/s]\n","Test Epoch: [81/100] lr: 0.0000 Loss: 0.5872 ACC@1: 83.44% ACC@5: 96.57%: 100%|██████████| 79/79 [00:09<00:00,  8.59it/s]\n","Train Epoch: [82/100] lr: 0.0000 Loss: 0.3120 ACC@1: 89.03% ACC@5: 97.40%: 100%|██████████| 79/79 [00:16<00:00,  4.67it/s]\n","Test Epoch: [82/100] lr: 0.0000 Loss: 0.5871 ACC@1: 83.44% ACC@5: 96.57%: 100%|██████████| 79/79 [00:07<00:00,  9.96it/s]\n","Train Epoch: [83/100] lr: 0.0000 Loss: 0.3169 ACC@1: 88.82% ACC@5: 97.33%: 100%|██████████| 79/79 [00:17<00:00,  4.53it/s]\n","Test Epoch: [83/100] lr: 0.0000 Loss: 0.5871 ACC@1: 83.44% ACC@5: 96.57%: 100%|██████████| 79/79 [00:07<00:00, 10.07it/s]\n","Train Epoch: [84/100] lr: 0.0000 Loss: 0.3074 ACC@1: 89.20% ACC@5: 97.22%: 100%|██████████| 79/79 [00:17<00:00,  4.62it/s]\n","Test Epoch: [84/100] lr: 0.0000 Loss: 0.5871 ACC@1: 83.44% ACC@5: 96.57%: 100%|██████████| 79/79 [00:07<00:00,  9.99it/s]\n","Train Epoch: [85/100] lr: 0.0000 Loss: 0.3083 ACC@1: 89.81% ACC@5: 97.37%: 100%|██████████| 79/79 [00:17<00:00,  4.61it/s]\n","Test Epoch: [85/100] lr: 0.0000 Loss: 0.5871 ACC@1: 83.44% ACC@5: 96.58%: 100%|██████████| 79/79 [00:07<00:00,  9.91it/s]\n","Train Epoch: [86/100] lr: 0.0000 Loss: 0.2975 ACC@1: 89.67% ACC@5: 97.55%: 100%|██████████| 79/79 [00:19<00:00,  4.11it/s]\n","Test Epoch: [86/100] lr: 0.0000 Loss: 0.5871 ACC@1: 83.44% ACC@5: 96.58%: 100%|██████████| 79/79 [00:07<00:00, 10.04it/s]\n","Train Epoch: [87/100] lr: 0.0000 Loss: 0.3027 ACC@1: 89.32% ACC@5: 97.43%: 100%|██████████| 79/79 [00:16<00:00,  4.66it/s]\n","Test Epoch: [87/100] lr: 0.0000 Loss: 0.5870 ACC@1: 83.44% ACC@5: 96.58%: 100%|██████████| 79/79 [00:07<00:00, 10.07it/s]\n","Train Epoch: [88/100] lr: 0.0000 Loss: 0.3190 ACC@1: 88.82% ACC@5: 97.47%: 100%|██████████| 79/79 [00:16<00:00,  4.66it/s]\n","Test Epoch: [88/100] lr: 0.0000 Loss: 0.5870 ACC@1: 83.42% ACC@5: 96.58%: 100%|██████████| 79/79 [00:07<00:00, 10.04it/s]\n","Train Epoch: [89/100] lr: 0.0000 Loss: 0.2927 ACC@1: 89.78% ACC@5: 97.71%: 100%|██████████| 79/79 [00:17<00:00,  4.56it/s]\n","Test Epoch: [89/100] lr: 0.0000 Loss: 0.5870 ACC@1: 83.45% ACC@5: 96.58%: 100%|██████████| 79/79 [00:07<00:00, 10.04it/s]\n","Train Epoch: [90/100] lr: 0.0000 Loss: 0.3152 ACC@1: 88.98% ACC@5: 97.36%: 100%|██████████| 79/79 [00:17<00:00,  4.54it/s]\n","Test Epoch: [90/100] lr: 0.0000 Loss: 0.6027 ACC@1: 83.27% ACC@5: 96.16%:  28%|██▊       | 22/79 [00:03<00:04, 13.07it/s]"]}],"source":["\n","\n","\n","def train_val(base1,base2,final, data_loader, train_optimizer):\n","    global lr\n","    schedule = [60, 80]\n","    criterion = nn.CrossEntropyLoss().cuda()\n","    is_train = train_optimizer is not None\n","    final.train() if is_train else final.eval()\n","    sa = 0\n","    total_loss, total_correct_1, total_correct_3, total_num, data_bar = 0.0, 0.0, 0.0, 0, tqdm(data_loader, position=0, leave=True)\n","    with (torch.enable_grad() if is_train else torch.no_grad()):\n","        for data, target,indexes in data_bar:\n","            data = data.cuda()\n","            output1 = base1(data)\n","            output2 = base2(data)\n","            #print(output1.shape,output2.shape)\n","            data = torch.cat((output1,output2),1)\n","            #print(data.shape)\n","\n","            data, target = data.cuda(non_blocking=True), target.cuda(non_blocking=True)\n","            out = final(data)\n","            loss = criterion(out, target)\n","\n","            if is_train:\n","                train_optimizer.zero_grad()\n","                loss.backward()\n","                train_optimizer.step()\n","\n","\n","            total_num += data.size(0)\n","            total_loss += loss.item() * data.size(0)\n","            prediction = torch.argsort(out, dim=-1, descending=True)\n","            total_correct_1 += torch.sum((prediction[:, 0:1] == target.unsqueeze(dim=-1)).any(dim=-1).float()).item()\n","            total_correct_3 += torch.sum((prediction[:, 0:3] == target.unsqueeze(dim=-1)).any(dim=-1).float()).item()\n","            \n","            data_bar.set_description('{} Epoch: [{}/{}] lr: {:.4f} Loss: {:.4f} ACC@1: {:.2f}% ACC@5: {:.2f}%'\n","                                     .format('Train' if is_train else 'Test', epoch, epochs, lr, total_loss / total_num,\n","                                             total_correct_1 / total_num * 100, total_correct_3 / total_num * 100))\n","        if is_train:  \n","          if schedule is not None:\n","            for milestone in schedule:\n","                lr *= 0.1 if epoch == milestone else 1.\n","            for param_group in train_optimizer.param_groups:\n","                param_group['lr'] = lr\n","\n","        return total_correct_1/total_num * 100\n","\n","train_acc = []\n","test_acc = []\n","for i in range(1):\n","  print(f\"Random Seed: {i}\")\n","  best_train = 0\n","  best_test = 0\n","  np.random.seed(i)\n","\n","  indices = get_indices(train_data, 1000)\n","  sampler = torch.utils.data.SubsetRandomSampler(indices)\n","\n","  train_loader = DataLoader(train_data, batch_size=batch_size, sampler = sampler , num_workers=16, pin_memory=True)\n","  finallayer = classifier()\n","  finallayer=finallayer.cuda()\n","  lr = 1e-4\n","  optimizer = torch.optim.Adam(finallayer.parameters(), lr=lr, weight_decay = 1e-5)\n","  epoch_start = 1\n","  epochs = 100\n","  for epoch in range(epoch_start, epochs+1):\n","    acc1 = train_val(supervised,cutout,finallayer, train_loader, optimizer)\n","    acc2 = train_val(supervised,cutout,finallayer, test_loader, None)\n","    if acc1 > best_train:\n","      best_train = acc1\n","    if acc2 > best_test:\n","      best_test = acc2\n","  train_acc.append(acc1)\n","  test_acc.append(acc2)\n","  \n","  torch.save(finallayer.state_dict(), \"/content/drive/My Drive/Models/Supervised/finallayerconcat1024butCAM.pth\")\n","print(train_acc)\n","print(test_acc)\n","top10 = np.sort(train_acc)[::-1]\n","print(\"top10 trainacc\",np.mean(top10[:10]))\n","top10 = np.sort(test_acc)[::-1]\n","print(\"top10 testacc\",np.mean(top10[:10]))\n","print(\"best test\",np.sort(test_acc)[::-1][0])\n","train_acc = np.array(train_acc)\n","test_acc = np.array(test_acc)\n","print(np.mean(train_acc))\n","print(np.mean(test_acc))\n","print(np.std(train_acc))\n","print(np.std(test_acc))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gsIGXx3hTCzi"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lVvfdoc5dDyZ"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1621383,"status":"ok","timestamp":1669289376663,"user":{"displayName":"Souw Chuan Neo","userId":"08556196228836469917"},"user_tz":-480},"id":"7ZIPmJeAbzlm","outputId":"24c8ebdf-e00a-4131-fef5-b9938cbf6c4a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n","<class 'numpy.ndarray'>\n","50000\n","torch.Size([50000, 3, 32, 32])\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:no value was provided for `target_layer`, thus set to 'net.6'.\n"]},{"output_type":"stream","name":"stdout","text":["21000\n","22000\n","23000\n","24000\n","25000\n","26000\n","27000\n","28000\n","29000\n","30000\n","31000\n","32000\n","33000\n","34000\n","35000\n","36000\n","37000\n","38000\n","39000\n","40000\n","41000\n","42000\n","43000\n","44000\n","45000\n","46000\n","47000\n","48000\n","49000\n"]}],"source":["#This is originally to get the probabilities of places to cut out, however now no longer needed as results have been saved\n","from torchcam.methods import GradCAMpp\n","class ModelBase(nn.Module):\n","    \"\"\"\n","    Common CIFAR ResNet recipe.\n","    Comparing with ImageNet ResNet recipe, it:\n","    (i) replaces conv1 with kernel=3, str=1\n","    (ii) removes pool1\n","    \"\"\"\n","    def __init__(self, feature_dim=10, arch='resnet18', bn_splits=16):\n","        super(ModelBase, self).__init__()\n","\n","        # use split batchnorm\n","        norm_layer = nn.BatchNorm2d\n","        # get specified resnet model\n","        resnet_arch = getattr(resnet, arch)\n","        net = resnet_arch(num_classes=feature_dim, norm_layer=norm_layer)\n","\n","        # make changes to original resnet\n","        self.net = []\n","        for name, module in net.named_children():\n","            if name == 'conv1':\n","                module = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n","            if isinstance(module, nn.MaxPool2d):\n","                continue\n","            if isinstance(module, nn.Linear):\n","                self.net.append(nn.Flatten(1))\n","            self.net.append(module)\n","\n","        # build net\n","        self.net = nn.Sequential(*self.net)\n","\n","    def forward(self, x):\n","        x = self.net(x)\n","        # note: not normalized here\n","        return x\n","#Pretransform since getitem transform is very costly on time\n","tempdata = []\n","totensor = transforms.ToTensor()\n","train_data = CIFAR10(root='data', train=True, transform=test_transform, download=True)\n","print(type(train_data.data[0]))\n","for i in range(50000):\n","  tempdata.append(totensor(train_data.data[i]).unsqueeze(0))\n","print(len(tempdata))\n","tempdata = torch.cat(tempdata)\n","finaldata = torch.ones(50000,4,4)\n","finaldata =torch.load('/content/drive/My Drive/Models/Supervised/cutdatanotransform.pt')\n","print(tempdata.shape)\n","supervised = ModelBase()\n","supervised.load_state_dict(torch.load(\"/content/drive/My Drive/Models/Supervised/supervisedresnet18v1.1.pth\"))\n","cam_extractor = GradCAMpp(supervised)\n","for index in range(21000,50000,1000):\n","  data = tempdata[index:index+1000]\n","  target = train_data.targets[index:index+1000]\n","  pair = []\n","\n","  actualinput = data.clone()\n","  classes =[]\n","  classind = []\n","  for i in range(10):\n","    classes.append([])\n","    classind.append([])\n","  for i in range(len(data)):\n","    classes[target[i]].append(data[i].unsqueeze(0))\n","    classind[target[i]].append(i)\n","  for i in range(10):\n","    classes[i] = torch.cat(classes[i])\n","  #print(classind)\n","  for i in range(10):\n","    data = classes[i]\n","    out = supervised(data)\n","    activation_map = cam_extractor(scores=out,class_idx=i)\n","    activation_map = activation_map[0]\n","    for id in range(len(classind[i])):\n","      #argpos = (activation_map[id]==torch.max(activation_map[id])).nonzero()[0]\n","      finaldata[index+classind[i][id]] = activation_map[id]\n","      #actualinput[classind[i+index],:,argpos[0]*8:argpos[0]*8+8,argpos[1]*8:argpos[1]*8+8]=0\n","  torch.save(finaldata, '/content/drive/My Drive/Models/Supervised/cutdatanotransform.pt')\n","  print(index)\n"]},{"cell_type":"code","source":["temp = torch.ones(4,4)\n","print(temp,finaldata[0])\n","for i in range(50000):\n","  if torch.all(torch.eq(temp,finaldata[i])):\n","    print(i)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tROi5zYUZtW0","executionInfo":{"status":"ok","timestamp":1669289562055,"user_tz":-480,"elapsed":1183,"user":{"displayName":"Souw Chuan Neo","userId":"08556196228836469917"}},"outputId":"9169b266-c500-4b3c-c134-3d1b7e028d85"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1., 1., 1., 1.],\n","        [1., 1., 1., 1.],\n","        [1., 1., 1., 1.],\n","        [1., 1., 1., 1.]]) tensor([[0.0000, 0.3266, 0.0300, 0.2849],\n","        [0.0948, 0.3479, 0.7162, 0.5849],\n","        [0.3012, 0.6110, 0.9452, 0.5517],\n","        [0.5401, 0.6715, 0.9870, 1.0000]])\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12267,"status":"ok","timestamp":1669187908468,"user":{"displayName":"Souw Chuan Neo","userId":"08556196228836469917"},"user_tz":-480},"id":"3Fptzk8aFk6_","outputId":"d7763485-ee24-4760-8750-ecffaa497dac"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:root:no value was provided for `target_layer`, thus set to 'net.6'.\n"]},{"name":"stdout","output_type":"stream","text":["torch.Size([1, 3, 32, 32])\n","torch.Size([16, 3, 32, 32])\n","torch.Size([16, 10]) torch.Size([16, 3, 32, 32]) tensor(130)\n","torch.Size([15, 10]) torch.Size([15, 3, 32, 32]) tensor(31)\n","torch.Size([13, 10]) torch.Size([13, 3, 32, 32]) tensor(52)\n","torch.Size([20, 10]) torch.Size([20, 3, 32, 32]) tensor(3)\n","torch.Size([13, 10]) torch.Size([13, 3, 32, 32]) tensor(114)\n","torch.Size([12, 10]) torch.Size([12, 3, 32, 32]) tensor(45)\n","torch.Size([5, 10]) torch.Size([5, 3, 32, 32]) tensor(26)\n","torch.Size([10, 10]) torch.Size([10, 3, 32, 32]) tensor(47)\n","torch.Size([10, 10]) torch.Size([10, 3, 32, 32]) tensor(28)\n","torch.Size([14, 10]) torch.Size([14, 3, 32, 32]) tensor(129)\n","0 10.8184072971344\n"]}],"source":["from torchvision.io.image import read_image\n","from torchvision.transforms.functional import normalize, resize, to_pil_image\n","from torchvision.models import resnet18\n","from torchcam.methods import GradCAMpp\n","\n","from time import time \n","import matplotlib.pyplot as plt\n","from torchcam.utils import overlay_mask\n","\n","# for i in supervised.parameters():\n","#   i.requires_grad=False\n","indices = get_indices(train_data, 1000)\n","sampler = torch.utils.data.SubsetRandomSampler(indices)\n","supervised = ModelBase()\n","supervised.load_state_dict(torch.load(\"/content/drive/My Drive/Models/Supervised/supervisedresnet18v1.1.pth\"))\n","supervised.eval()\n","cam_extractor = GradCAMpp(supervised)\n","batchno = 0\n","st = time()\n","train_loader = DataLoader(train_data, batch_size=\n","                          batch_size, sampler = sampler , num_workers=16, pin_memory=True)\n","for batch in train_loader:\n","  pair = []\n","  data,target = batch\n","  actualinput = data.clone()\n","  classes =[]\n","  classind = []\n","  for i in range(10):\n","    classes.append([])\n","    classind.append([])\n","  for i in range(len(data)):\n","    classes[target[i]].append(data[i].unsqueeze(0))\n","    classind[target[i]].append(i)\n","  for i in range(10):\n","    classes[i] = torch.cat(classes[i])\n","  for i in range(10):\n","    data = classes[i]\n","    out = supervised(data)\n","    activation_map = cam_extractor(scores=out,class_idx=9)\n","    activation_map = activation_map[0]\n","    for id in range(len(classind[i])):\n","      argpos = (activation_map[id]==torch.max(activation_map[id])).nonzero()[0]\n","      actualinput[classind[i],:,argpos[0]*8:argpos[0]*8+8,argpos[1]*8:argpos[1]*8+8]=0\n","    data = actualinput\n","\n","et=time()\n","print(batchno,et-st)  \n"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1KD2lsC7zM0xiARdkegLNbM8IrxqGoAA5","timestamp":1669019254156},{"file_id":"1nxt7WYT-CjZnxV_mYTLdwB2puWuwvSWu","timestamp":1608087056099}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}