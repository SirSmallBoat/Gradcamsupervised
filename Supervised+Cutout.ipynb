{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1nxt7WYT-CjZnxV_mYTLdwB2puWuwvSWu","timestamp":1608087056099}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"6b564ffe43e0406d84785a0bab769786":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1f3b0909e0254438a94b03e86fd59218","IPY_MODEL_38d69b74613541b8964856870a24f366","IPY_MODEL_d7dfdf8dd9bc42c295ac7a5e9e3126d4"],"layout":"IPY_MODEL_4b15b6ec48064625ac55bc8c940a6e01"}},"1f3b0909e0254438a94b03e86fd59218":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f575a7cfc77a48a5891a847327533966","placeholder":"​","style":"IPY_MODEL_3e367f1f791f414694ea88c412a83371","value":"100%"}},"38d69b74613541b8964856870a24f366":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9be11d8921f64be6bd9a41fa4dc35fd3","max":170498071,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2dd48900694749df812c477607c89dca","value":170498071}},"d7dfdf8dd9bc42c295ac7a5e9e3126d4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_92b8b4a1cfa04e13bc6d8996276ffd0a","placeholder":"​","style":"IPY_MODEL_38e670a0b2994cfc81d1f90f0be4fd64","value":" 170498071/170498071 [00:13&lt;00:00, 14132106.38it/s]"}},"4b15b6ec48064625ac55bc8c940a6e01":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f575a7cfc77a48a5891a847327533966":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3e367f1f791f414694ea88c412a83371":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9be11d8921f64be6bd9a41fa4dc35fd3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2dd48900694749df812c477607c89dca":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"92b8b4a1cfa04e13bc6d8996276ffd0a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"38e670a0b2994cfc81d1f90f0be4fd64":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","metadata":{"id":"hLWmLD6fa1vR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669017976137,"user_tz":-480,"elapsed":35162,"user":{"displayName":"Souw Chuan Neo","userId":"08556196228836469917"}},"outputId":"323495e6-17b4-405d-cd05-d9631cf11ba4"},"source":["import torch\n","import torchvision\n","from torch import nn\n","import numpy as np\n","from tqdm import tqdm\n","from torchvision.datasets import CIFAR10\n","from torchvision.models import resnet\n","from torch.utils.data import DataLoader\n","from torchvision import transforms\n","from datetime import datetime\n","from google.colab import drive\n","import random\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":156,"referenced_widgets":["6b564ffe43e0406d84785a0bab769786","1f3b0909e0254438a94b03e86fd59218","38d69b74613541b8964856870a24f366","d7dfdf8dd9bc42c295ac7a5e9e3126d4","4b15b6ec48064625ac55bc8c940a6e01","f575a7cfc77a48a5891a847327533966","3e367f1f791f414694ea88c412a83371","9be11d8921f64be6bd9a41fa4dc35fd3","2dd48900694749df812c477607c89dca","92b8b4a1cfa04e13bc6d8996276ffd0a","38e670a0b2994cfc81d1f90f0be4fd64"]},"id":"RY6-77T0pUEd","executionInfo":{"status":"ok","timestamp":1669018164985,"user_tz":-480,"elapsed":18315,"user":{"displayName":"Souw Chuan Neo","userId":"08556196228836469917"}},"outputId":"8fb139c6-5ed9-4665-9396-72fc78564e7c"},"source":["batch_size = 128\n","train_transform = transforms.Compose([\n","    transforms.RandomResizedCrop(32),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])])\n","\n","test_transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])])\n","\n","\n","\n","train_data = CIFAR10(root='data', train=True, transform=train_transform, download=True)\n","\n","\n","def get_indices(dataset, num_samples):\n","  indices = []\n","  for Class in range(10):\n","    for j in range(num_samples):\n","      x = np.random.randint(0, 50000)\n","      while dataset.targets[x] != Class:\n","        x = np.random.randint(0, 50000)\n","      indices.append(x)\n","  return indices\n","\n","\n","\n","test_data = CIFAR10(root='data', train=False, transform=test_transform, download=True)\n","test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=16, pin_memory=True)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/170498071 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b564ffe43e0406d84785a0bab769786"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Extracting data/cifar-10-python.tar.gz to data\n","Files already downloaded and verified\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]}]},{"cell_type":"code","metadata":{"id":"rV0e4e6Mctpz"},"source":["class ModelBase(nn.Module):\n","    \"\"\"\n","    Common CIFAR ResNet recipe.\n","    Comparing with ImageNet ResNet recipe, it:\n","    (i) replaces conv1 with kernel=3, str=1\n","    (ii) removes pool1\n","    \"\"\"\n","    def __init__(self, feature_dim=10, arch='resnet18', bn_splits=16):\n","        super(ModelBase, self).__init__()\n","\n","        # use split batchnorm\n","        norm_layer = nn.BatchNorm2d\n","        # get specified resnet model\n","        resnet_arch = getattr(resnet, arch)\n","        net = resnet_arch(num_classes=feature_dim, norm_layer=norm_layer)\n","\n","        # make changes to original resnet\n","        self.net = []\n","        for name, module in net.named_children():\n","            if name == 'conv1':\n","                module = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n","            if isinstance(module, nn.MaxPool2d):\n","                continue\n","            if isinstance(module, nn.Linear):\n","                self.net.append(nn.Flatten(1))\n","            self.net.append(module)\n","\n","        # build net\n","        self.net = nn.Sequential(*self.net)\n","\n","    def forward(self, x):\n","        x = self.net(x)\n","        # note: not normalized here\n","        return x\n","\n","def train_val(net, data_loader, train_optimizer):\n","    global lr\n","    schedule = [60, 80]\n","    criterion = nn.CrossEntropyLoss().cuda()\n","    is_train = train_optimizer is not None\n","    net.train() if is_train else net.eval()\n","\n","    total_loss, total_correct_1, total_correct_3, total_num, data_bar = 0.0, 0.0, 0.0, 0, tqdm(data_loader, position=0, leave=True)\n","    with (torch.enable_grad() if is_train else torch.no_grad()):\n","        for data, target in data_bar:\n","            #print(target.size())\n","            #insert cropping here\n","            data, target = data.cuda(non_blocking=True), target.cuda(non_blocking=True)\n","            out = net(data)\n","            loss = criterion(out, target)\n","\n","            if is_train:\n","                train_optimizer.zero_grad()\n","                loss.backward()\n","                train_optimizer.step()\n","\n","\n","            total_num += data.size(0)\n","            total_loss += loss.item() * data.size(0)\n","            prediction = torch.argsort(out, dim=-1, descending=True)\n","            total_correct_1 += torch.sum((prediction[:, 0:1] == target.unsqueeze(dim=-1)).any(dim=-1).float()).item()\n","            total_correct_3 += torch.sum((prediction[:, 0:3] == target.unsqueeze(dim=-1)).any(dim=-1).float()).item()\n","            \n","            data_bar.set_description('{} Epoch: [{}/{}] lr: {:.4f} Loss: {:.4f} ACC@1: {:.2f}% ACC@5: {:.2f}%'\n","                                     .format('Train' if is_train else 'Test', epoch, epochs, lr, total_loss / total_num,\n","                                             total_correct_1 / total_num * 100, total_correct_3 / total_num * 100))\n","        if is_train:  \n","          if schedule is not None:\n","            for milestone in schedule:\n","                lr *= 0.1 if epoch == milestone else 1.\n","            for param_group in train_optimizer.param_groups:\n","                param_group['lr'] = lr\n","\n","        return total_correct_1/total_num * 100\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n8OcFjEnduh2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669019893997,"user_tz":-480,"elapsed":1603264,"user":{"displayName":"Souw Chuan Neo","userId":"08556196228836469917"}},"outputId":"df4019c0-18c8-4e0d-d17a-882c3e3ea280"},"source":["train_acc = []\n","test_acc = []\n","\n","print(f\"Random Seed: {1}\")\n","best_train = 0\n","best_test = 0\n","np.random.seed(1)\n","\n","indices = get_indices(train_data, 1000)\n","sampler = torch.utils.data.SubsetRandomSampler(indices)\n","\n","train_loader = DataLoader(train_data, batch_size=batch_size, sampler = sampler , num_workers=16, pin_memory=True)\n","\n","resnet18 = ModelBase()\n","resnet18.cuda()\n","lr = 1e-3\n","optimizer = torch.optim.Adam(resnet18.parameters(), lr=lr, weight_decay = 1e-5)\n","epoch_start = 1\n","epochs = 100\n","for epoch in range(epoch_start, epochs+1):\n","  acc1 = train_val(resnet18, train_loader, optimizer)\n","  acc2 = train_val(resnet18, test_loader, None)\n","  if acc1 > best_train:\n","    best_train = acc1\n","  if acc2 > best_test:\n","    best_test = acc2\n","train_acc.append(acc1)\n","test_acc.append(acc2)\n","\n","torch.save(resnet18.state_dict(), \"/content/drive/My Drive/Models/Supervised/supervisedresnet18v1.1seed1.pth\")\n","print(train_acc)\n","print(test_acc)\n","top10 = np.sort(train_acc)[::-1]\n","print(\"top10 trainacc\",np.mean(top10[:10]))\n","top10 = np.sort(test_acc)[::-1]\n","print(\"top10 testacc\",np.mean(top10[:10]))\n","print(\"best test\",np.sort(test_acc)[::-1][0])\n","train_acc = np.array(train_acc)\n","test_acc = np.array(test_acc)\n","print(np.mean(train_acc))\n","print(np.mean(test_acc))\n","print(np.std(train_acc))\n","print(np.std(test_acc))\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Random Seed: 1\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","Train Epoch: [1/100] lr: 0.0010 Loss: 1.9373 ACC@1: 28.75% ACC@5: 63.05%: 100%|██████████| 79/79 [00:10<00:00,  7.39it/s]\n","Test Epoch: [1/100] lr: 0.0010 Loss: 1.8579 ACC@1: 34.79% ACC@5: 67.37%: 100%|██████████| 79/79 [00:04<00:00, 18.16it/s]\n","Train Epoch: [2/100] lr: 0.0010 Loss: 1.7537 ACC@1: 35.57% ACC@5: 69.92%: 100%|██████████| 79/79 [00:10<00:00,  7.47it/s]\n","Test Epoch: [2/100] lr: 0.0010 Loss: 1.7108 ACC@1: 39.32% ACC@5: 73.18%: 100%|██████████| 79/79 [00:04<00:00, 17.67it/s]\n","Train Epoch: [3/100] lr: 0.0010 Loss: 1.6478 ACC@1: 40.16% ACC@5: 73.40%: 100%|██████████| 79/79 [00:10<00:00,  7.40it/s]\n","Test Epoch: [3/100] lr: 0.0010 Loss: 1.4719 ACC@1: 46.52% ACC@5: 79.87%: 100%|██████████| 79/79 [00:04<00:00, 17.41it/s]\n","Train Epoch: [4/100] lr: 0.0010 Loss: 1.5583 ACC@1: 43.23% ACC@5: 76.22%: 100%|██████████| 79/79 [00:11<00:00,  7.13it/s]\n","Test Epoch: [4/100] lr: 0.0010 Loss: 1.4088 ACC@1: 49.58% ACC@5: 80.95%: 100%|██████████| 79/79 [00:04<00:00, 16.26it/s]\n","Train Epoch: [5/100] lr: 0.0010 Loss: 1.4712 ACC@1: 46.13% ACC@5: 78.66%: 100%|██████████| 79/79 [00:11<00:00,  7.05it/s]\n","Test Epoch: [5/100] lr: 0.0010 Loss: 1.3442 ACC@1: 52.10% ACC@5: 83.20%: 100%|██████████| 79/79 [00:04<00:00, 17.63it/s]\n","Train Epoch: [6/100] lr: 0.0010 Loss: 1.3958 ACC@1: 49.42% ACC@5: 80.83%: 100%|██████████| 79/79 [00:10<00:00,  7.34it/s]\n","Test Epoch: [6/100] lr: 0.0010 Loss: 1.4951 ACC@1: 47.35% ACC@5: 81.20%: 100%|██████████| 79/79 [00:04<00:00, 17.56it/s]\n","Train Epoch: [7/100] lr: 0.0010 Loss: 1.3854 ACC@1: 49.42% ACC@5: 80.73%: 100%|██████████| 79/79 [00:10<00:00,  7.41it/s]\n","Test Epoch: [7/100] lr: 0.0010 Loss: 1.3679 ACC@1: 51.69% ACC@5: 83.63%: 100%|██████████| 79/79 [00:04<00:00, 17.14it/s]\n","Train Epoch: [8/100] lr: 0.0010 Loss: 1.3248 ACC@1: 52.48% ACC@5: 82.33%: 100%|██████████| 79/79 [00:10<00:00,  7.19it/s]\n","Test Epoch: [8/100] lr: 0.0010 Loss: 1.3654 ACC@1: 51.69% ACC@5: 85.39%: 100%|██████████| 79/79 [00:05<00:00, 14.66it/s]\n","Train Epoch: [9/100] lr: 0.0010 Loss: 1.2670 ACC@1: 54.02% ACC@5: 83.66%: 100%|██████████| 79/79 [00:11<00:00,  7.18it/s]\n","Test Epoch: [9/100] lr: 0.0010 Loss: 1.2374 ACC@1: 57.17% ACC@5: 84.38%: 100%|██████████| 79/79 [00:04<00:00, 17.40it/s]\n","Train Epoch: [10/100] lr: 0.0010 Loss: 1.2019 ACC@1: 56.75% ACC@5: 84.91%: 100%|██████████| 79/79 [00:10<00:00,  7.28it/s]\n","Test Epoch: [10/100] lr: 0.0010 Loss: 1.5475 ACC@1: 51.51% ACC@5: 80.80%: 100%|██████████| 79/79 [00:04<00:00, 17.27it/s]\n","Train Epoch: [11/100] lr: 0.0010 Loss: 1.1942 ACC@1: 56.77% ACC@5: 85.69%: 100%|██████████| 79/79 [00:11<00:00,  7.04it/s]\n","Test Epoch: [11/100] lr: 0.0010 Loss: 1.0152 ACC@1: 64.16% ACC@5: 90.11%: 100%|██████████| 79/79 [00:04<00:00, 17.40it/s]\n","Train Epoch: [12/100] lr: 0.0010 Loss: 1.1394 ACC@1: 59.28% ACC@5: 86.37%: 100%|██████████| 79/79 [00:11<00:00,  6.98it/s]\n","Test Epoch: [12/100] lr: 0.0010 Loss: 1.0042 ACC@1: 64.68% ACC@5: 90.02%: 100%|██████████| 79/79 [00:04<00:00, 15.87it/s]\n","Train Epoch: [13/100] lr: 0.0010 Loss: 1.1119 ACC@1: 60.31% ACC@5: 86.91%: 100%|██████████| 79/79 [00:11<00:00,  7.11it/s]\n","Test Epoch: [13/100] lr: 0.0010 Loss: 1.4070 ACC@1: 56.53% ACC@5: 87.71%: 100%|██████████| 79/79 [00:04<00:00, 17.19it/s]\n","Train Epoch: [14/100] lr: 0.0010 Loss: 1.0985 ACC@1: 60.72% ACC@5: 87.11%: 100%|██████████| 79/79 [00:11<00:00,  7.17it/s]\n","Test Epoch: [14/100] lr: 0.0010 Loss: 0.9515 ACC@1: 66.81% ACC@5: 90.88%: 100%|██████████| 79/79 [00:04<00:00, 17.43it/s]\n","Train Epoch: [15/100] lr: 0.0010 Loss: 1.0591 ACC@1: 62.33% ACC@5: 88.19%: 100%|██████████| 79/79 [00:11<00:00,  7.17it/s]\n","Test Epoch: [15/100] lr: 0.0010 Loss: 0.9310 ACC@1: 68.25% ACC@5: 91.66%: 100%|██████████| 79/79 [00:04<00:00, 17.51it/s]\n","Train Epoch: [16/100] lr: 0.0010 Loss: 1.0382 ACC@1: 62.81% ACC@5: 88.23%: 100%|██████████| 79/79 [00:11<00:00,  7.05it/s]\n","Test Epoch: [16/100] lr: 0.0010 Loss: 1.2982 ACC@1: 59.40% ACC@5: 86.92%: 100%|██████████| 79/79 [00:05<00:00, 14.15it/s]\n","Train Epoch: [17/100] lr: 0.0010 Loss: 1.0017 ACC@1: 64.35% ACC@5: 88.81%: 100%|██████████| 79/79 [00:11<00:00,  7.14it/s]\n","Test Epoch: [17/100] lr: 0.0010 Loss: 1.1166 ACC@1: 64.22% ACC@5: 90.33%: 100%|██████████| 79/79 [00:04<00:00, 17.24it/s]\n","Train Epoch: [18/100] lr: 0.0010 Loss: 0.9717 ACC@1: 65.21% ACC@5: 89.14%: 100%|██████████| 79/79 [00:11<00:00,  7.09it/s]\n","Test Epoch: [18/100] lr: 0.0010 Loss: 1.1100 ACC@1: 65.89% ACC@5: 88.74%: 100%|██████████| 79/79 [00:04<00:00, 17.10it/s]\n","Train Epoch: [19/100] lr: 0.0010 Loss: 0.9745 ACC@1: 64.97% ACC@5: 89.33%: 100%|██████████| 79/79 [00:11<00:00,  7.14it/s]\n","Test Epoch: [19/100] lr: 0.0010 Loss: 0.9225 ACC@1: 69.24% ACC@5: 90.93%: 100%|██████████| 79/79 [00:04<00:00, 17.40it/s]\n","Train Epoch: [20/100] lr: 0.0010 Loss: 0.9455 ACC@1: 66.34% ACC@5: 89.29%: 100%|██████████| 79/79 [00:11<00:00,  7.05it/s]\n","Test Epoch: [20/100] lr: 0.0010 Loss: 0.9096 ACC@1: 69.97% ACC@5: 91.39%: 100%|██████████| 79/79 [00:04<00:00, 16.64it/s]\n","Train Epoch: [21/100] lr: 0.0010 Loss: 0.9484 ACC@1: 66.66% ACC@5: 89.47%: 100%|██████████| 79/79 [00:11<00:00,  7.17it/s]\n","Test Epoch: [21/100] lr: 0.0010 Loss: 1.0558 ACC@1: 65.89% ACC@5: 91.15%: 100%|██████████| 79/79 [00:04<00:00, 17.28it/s]\n","Train Epoch: [22/100] lr: 0.0010 Loss: 0.9243 ACC@1: 67.49% ACC@5: 89.98%: 100%|██████████| 79/79 [00:11<00:00,  7.16it/s]\n","Test Epoch: [22/100] lr: 0.0010 Loss: 1.0206 ACC@1: 68.13% ACC@5: 90.24%: 100%|██████████| 79/79 [00:04<00:00, 17.14it/s]\n","Train Epoch: [23/100] lr: 0.0010 Loss: 0.8887 ACC@1: 68.38% ACC@5: 90.85%: 100%|██████████| 79/79 [00:11<00:00,  7.06it/s]\n","Test Epoch: [23/100] lr: 0.0010 Loss: 0.7966 ACC@1: 73.56% ACC@5: 93.38%: 100%|██████████| 79/79 [00:04<00:00, 17.41it/s]\n","Train Epoch: [24/100] lr: 0.0010 Loss: 0.8875 ACC@1: 68.78% ACC@5: 90.68%: 100%|██████████| 79/79 [00:11<00:00,  7.10it/s]\n","Test Epoch: [24/100] lr: 0.0010 Loss: 1.0242 ACC@1: 69.65% ACC@5: 90.96%: 100%|██████████| 79/79 [00:04<00:00, 17.05it/s]\n","Train Epoch: [25/100] lr: 0.0010 Loss: 0.8530 ACC@1: 70.40% ACC@5: 90.88%: 100%|██████████| 79/79 [00:11<00:00,  7.02it/s]\n","Test Epoch: [25/100] lr: 0.0010 Loss: 0.9011 ACC@1: 71.10% ACC@5: 93.10%: 100%|██████████| 79/79 [00:04<00:00, 16.94it/s]\n","Train Epoch: [26/100] lr: 0.0010 Loss: 0.8487 ACC@1: 70.03% ACC@5: 91.32%: 100%|██████████| 79/79 [00:11<00:00,  7.14it/s]\n","Test Epoch: [26/100] lr: 0.0010 Loss: 0.8495 ACC@1: 72.65% ACC@5: 92.87%: 100%|██████████| 79/79 [00:04<00:00, 16.78it/s]\n","Train Epoch: [27/100] lr: 0.0010 Loss: 0.8431 ACC@1: 70.27% ACC@5: 91.25%: 100%|██████████| 79/79 [00:11<00:00,  7.13it/s]\n","Test Epoch: [27/100] lr: 0.0010 Loss: 0.8070 ACC@1: 73.67% ACC@5: 92.73%: 100%|██████████| 79/79 [00:04<00:00, 16.97it/s]\n","Train Epoch: [28/100] lr: 0.0010 Loss: 0.8285 ACC@1: 71.10% ACC@5: 91.37%: 100%|██████████| 79/79 [00:11<00:00,  7.03it/s]\n","Test Epoch: [28/100] lr: 0.0010 Loss: 0.8300 ACC@1: 72.96% ACC@5: 93.50%: 100%|██████████| 79/79 [00:04<00:00, 16.74it/s]\n","Train Epoch: [29/100] lr: 0.0010 Loss: 0.8304 ACC@1: 70.35% ACC@5: 91.21%: 100%|██████████| 79/79 [00:11<00:00,  7.11it/s]\n","Test Epoch: [29/100] lr: 0.0010 Loss: 0.8187 ACC@1: 73.75% ACC@5: 93.77%: 100%|██████████| 79/79 [00:04<00:00, 17.08it/s]\n","Train Epoch: [30/100] lr: 0.0010 Loss: 0.7860 ACC@1: 72.19% ACC@5: 92.35%: 100%|██████████| 79/79 [00:11<00:00,  7.11it/s]\n","Test Epoch: [30/100] lr: 0.0010 Loss: 0.8747 ACC@1: 71.70% ACC@5: 93.20%: 100%|██████████| 79/79 [00:04<00:00, 17.08it/s]\n","Train Epoch: [31/100] lr: 0.0010 Loss: 0.7910 ACC@1: 72.29% ACC@5: 91.88%: 100%|██████████| 79/79 [00:11<00:00,  7.13it/s]\n","Test Epoch: [31/100] lr: 0.0010 Loss: 0.7700 ACC@1: 75.63% ACC@5: 93.59%: 100%|██████████| 79/79 [00:04<00:00, 17.14it/s]\n","Train Epoch: [32/100] lr: 0.0010 Loss: 0.7571 ACC@1: 73.26% ACC@5: 92.49%: 100%|██████████| 79/79 [00:11<00:00,  6.97it/s]\n","Test Epoch: [32/100] lr: 0.0010 Loss: 0.8407 ACC@1: 73.72% ACC@5: 93.29%: 100%|██████████| 79/79 [00:04<00:00, 16.81it/s]\n","Train Epoch: [33/100] lr: 0.0010 Loss: 0.7741 ACC@1: 72.65% ACC@5: 92.27%: 100%|██████████| 79/79 [00:11<00:00,  7.11it/s]\n","Test Epoch: [33/100] lr: 0.0010 Loss: 0.7433 ACC@1: 76.26% ACC@5: 94.65%: 100%|██████████| 79/79 [00:04<00:00, 17.27it/s]\n","Train Epoch: [34/100] lr: 0.0010 Loss: 0.7436 ACC@1: 73.27% ACC@5: 92.68%: 100%|██████████| 79/79 [00:11<00:00,  7.14it/s]\n","Test Epoch: [34/100] lr: 0.0010 Loss: 0.8076 ACC@1: 75.07% ACC@5: 94.30%: 100%|██████████| 79/79 [00:04<00:00, 17.17it/s]\n","Train Epoch: [35/100] lr: 0.0010 Loss: 0.7639 ACC@1: 72.96% ACC@5: 92.71%: 100%|██████████| 79/79 [00:10<00:00,  7.19it/s]\n","Test Epoch: [35/100] lr: 0.0010 Loss: 0.7657 ACC@1: 75.73% ACC@5: 94.93%: 100%|██████████| 79/79 [00:04<00:00, 17.09it/s]\n","Train Epoch: [36/100] lr: 0.0010 Loss: 0.7355 ACC@1: 74.09% ACC@5: 93.07%: 100%|██████████| 79/79 [00:12<00:00,  6.40it/s]\n","Test Epoch: [36/100] lr: 0.0010 Loss: 0.8465 ACC@1: 74.35% ACC@5: 93.47%: 100%|██████████| 79/79 [00:04<00:00, 17.18it/s]\n","Train Epoch: [37/100] lr: 0.0010 Loss: 0.7276 ACC@1: 74.25% ACC@5: 93.21%: 100%|██████████| 79/79 [00:11<00:00,  7.09it/s]\n","Test Epoch: [37/100] lr: 0.0010 Loss: 0.8317 ACC@1: 74.26% ACC@5: 93.92%: 100%|██████████| 79/79 [00:04<00:00, 17.08it/s]\n","Train Epoch: [38/100] lr: 0.0010 Loss: 0.7222 ACC@1: 74.93% ACC@5: 93.06%: 100%|██████████| 79/79 [00:11<00:00,  7.13it/s]\n","Test Epoch: [38/100] lr: 0.0010 Loss: 0.8715 ACC@1: 74.52% ACC@5: 93.17%: 100%|██████████| 79/79 [00:04<00:00, 17.05it/s]\n","Train Epoch: [39/100] lr: 0.0010 Loss: 0.7037 ACC@1: 75.41% ACC@5: 93.44%: 100%|██████████| 79/79 [00:11<00:00,  7.16it/s]\n","Test Epoch: [39/100] lr: 0.0010 Loss: 1.0965 ACC@1: 69.63% ACC@5: 93.18%: 100%|██████████| 79/79 [00:04<00:00, 16.82it/s]\n","Train Epoch: [40/100] lr: 0.0010 Loss: 0.7361 ACC@1: 74.54% ACC@5: 92.81%: 100%|██████████| 79/79 [00:11<00:00,  7.08it/s]\n","Test Epoch: [40/100] lr: 0.0010 Loss: 0.7197 ACC@1: 77.17% ACC@5: 94.94%: 100%|██████████| 79/79 [00:04<00:00, 16.79it/s]\n","Train Epoch: [41/100] lr: 0.0010 Loss: 0.6655 ACC@1: 76.14% ACC@5: 93.97%: 100%|██████████| 79/79 [00:11<00:00,  7.09it/s]\n","Test Epoch: [41/100] lr: 0.0010 Loss: 0.9006 ACC@1: 74.01% ACC@5: 93.81%: 100%|██████████| 79/79 [00:04<00:00, 16.75it/s]\n","Train Epoch: [42/100] lr: 0.0010 Loss: 0.6701 ACC@1: 76.78% ACC@5: 93.69%: 100%|██████████| 79/79 [00:11<00:00,  7.09it/s]\n","Test Epoch: [42/100] lr: 0.0010 Loss: 0.7488 ACC@1: 76.23% ACC@5: 95.05%: 100%|██████████| 79/79 [00:04<00:00, 17.17it/s]\n","Train Epoch: [43/100] lr: 0.0010 Loss: 0.6714 ACC@1: 76.62% ACC@5: 93.70%: 100%|██████████| 79/79 [00:11<00:00,  7.15it/s]\n","Test Epoch: [43/100] lr: 0.0010 Loss: 1.1581 ACC@1: 69.01% ACC@5: 91.22%: 100%|██████████| 79/79 [00:04<00:00, 17.19it/s]\n","Train Epoch: [44/100] lr: 0.0010 Loss: 0.6505 ACC@1: 77.09% ACC@5: 94.05%: 100%|██████████| 79/79 [00:11<00:00,  6.98it/s]\n","Test Epoch: [44/100] lr: 0.0010 Loss: 0.8145 ACC@1: 75.68% ACC@5: 94.06%: 100%|██████████| 79/79 [00:04<00:00, 16.79it/s]\n","Train Epoch: [45/100] lr: 0.0010 Loss: 0.6620 ACC@1: 76.63% ACC@5: 93.85%: 100%|██████████| 79/79 [00:11<00:00,  7.14it/s]\n","Test Epoch: [45/100] lr: 0.0010 Loss: 0.7997 ACC@1: 76.85% ACC@5: 94.45%: 100%|██████████| 79/79 [00:04<00:00, 17.20it/s]\n","Train Epoch: [46/100] lr: 0.0010 Loss: 0.6541 ACC@1: 76.83% ACC@5: 93.83%: 100%|██████████| 79/79 [00:11<00:00,  7.12it/s]\n","Test Epoch: [46/100] lr: 0.0010 Loss: 0.8780 ACC@1: 74.95% ACC@5: 94.16%: 100%|██████████| 79/79 [00:04<00:00, 17.16it/s]\n","Train Epoch: [47/100] lr: 0.0010 Loss: 0.6433 ACC@1: 77.64% ACC@5: 94.30%: 100%|██████████| 79/79 [00:11<00:00,  7.09it/s]\n","Test Epoch: [47/100] lr: 0.0010 Loss: 0.7334 ACC@1: 78.52% ACC@5: 95.22%: 100%|██████████| 79/79 [00:04<00:00, 16.85it/s]\n","Train Epoch: [48/100] lr: 0.0010 Loss: 0.6291 ACC@1: 78.56% ACC@5: 94.11%: 100%|██████████| 79/79 [00:11<00:00,  6.85it/s]\n","Test Epoch: [48/100] lr: 0.0010 Loss: 0.8144 ACC@1: 75.71% ACC@5: 95.09%: 100%|██████████| 79/79 [00:04<00:00, 16.84it/s]\n","Train Epoch: [49/100] lr: 0.0010 Loss: 0.6237 ACC@1: 78.27% ACC@5: 94.33%: 100%|██████████| 79/79 [00:11<00:00,  7.00it/s]\n","Test Epoch: [49/100] lr: 0.0010 Loss: 0.8441 ACC@1: 75.72% ACC@5: 93.90%: 100%|██████████| 79/79 [00:04<00:00, 16.84it/s]\n","Train Epoch: [50/100] lr: 0.0010 Loss: 0.6366 ACC@1: 77.02% ACC@5: 94.21%: 100%|██████████| 79/79 [00:11<00:00,  7.14it/s]\n","Test Epoch: [50/100] lr: 0.0010 Loss: 0.7479 ACC@1: 76.71% ACC@5: 95.11%: 100%|██████████| 79/79 [00:04<00:00, 16.90it/s]\n","Train Epoch: [51/100] lr: 0.0010 Loss: 0.5937 ACC@1: 79.33% ACC@5: 94.24%: 100%|██████████| 79/79 [00:11<00:00,  7.15it/s]\n","Test Epoch: [51/100] lr: 0.0010 Loss: 0.9012 ACC@1: 75.04% ACC@5: 93.71%: 100%|██████████| 79/79 [00:04<00:00, 16.97it/s]\n","Train Epoch: [52/100] lr: 0.0010 Loss: 0.5988 ACC@1: 78.93% ACC@5: 94.35%: 100%|██████████| 79/79 [00:11<00:00,  6.63it/s]\n","Test Epoch: [52/100] lr: 0.0010 Loss: 0.8568 ACC@1: 77.02% ACC@5: 94.85%: 100%|██████████| 79/79 [00:04<00:00, 16.74it/s]\n","Train Epoch: [53/100] lr: 0.0010 Loss: 0.6088 ACC@1: 78.60% ACC@5: 94.20%: 100%|██████████| 79/79 [00:11<00:00,  7.08it/s]\n","Test Epoch: [53/100] lr: 0.0010 Loss: 0.7329 ACC@1: 78.38% ACC@5: 95.10%: 100%|██████████| 79/79 [00:04<00:00, 16.79it/s]\n","Train Epoch: [54/100] lr: 0.0010 Loss: 0.5820 ACC@1: 79.35% ACC@5: 95.07%: 100%|██████████| 79/79 [00:11<00:00,  7.12it/s]\n","Test Epoch: [54/100] lr: 0.0010 Loss: 0.7719 ACC@1: 77.59% ACC@5: 95.35%: 100%|██████████| 79/79 [00:04<00:00, 16.88it/s]\n","Train Epoch: [55/100] lr: 0.0010 Loss: 0.5953 ACC@1: 79.27% ACC@5: 94.55%: 100%|██████████| 79/79 [00:11<00:00,  7.08it/s]\n","Test Epoch: [55/100] lr: 0.0010 Loss: 0.7665 ACC@1: 77.30% ACC@5: 95.66%: 100%|██████████| 79/79 [00:04<00:00, 16.14it/s]\n","Train Epoch: [56/100] lr: 0.0010 Loss: 0.5751 ACC@1: 80.03% ACC@5: 94.83%: 100%|██████████| 79/79 [00:12<00:00,  6.36it/s]\n","Test Epoch: [56/100] lr: 0.0010 Loss: 0.8717 ACC@1: 77.70% ACC@5: 93.70%: 100%|██████████| 79/79 [00:04<00:00, 15.96it/s]\n","Train Epoch: [57/100] lr: 0.0010 Loss: 0.5772 ACC@1: 80.04% ACC@5: 94.39%: 100%|██████████| 79/79 [00:11<00:00,  6.61it/s]\n","Test Epoch: [57/100] lr: 0.0010 Loss: 0.9238 ACC@1: 74.84% ACC@5: 94.09%: 100%|██████████| 79/79 [00:05<00:00, 15.01it/s]\n","Train Epoch: [58/100] lr: 0.0010 Loss: 0.5776 ACC@1: 79.82% ACC@5: 94.70%: 100%|██████████| 79/79 [00:12<00:00,  6.31it/s]\n","Test Epoch: [58/100] lr: 0.0010 Loss: 0.7414 ACC@1: 79.24% ACC@5: 95.57%: 100%|██████████| 79/79 [00:04<00:00, 17.07it/s]\n","Train Epoch: [59/100] lr: 0.0010 Loss: 0.5508 ACC@1: 80.92% ACC@5: 94.75%: 100%|██████████| 79/79 [00:12<00:00,  6.54it/s]\n","Test Epoch: [59/100] lr: 0.0010 Loss: 0.7404 ACC@1: 78.91% ACC@5: 95.88%: 100%|██████████| 79/79 [00:04<00:00, 16.20it/s]\n","Train Epoch: [60/100] lr: 0.0010 Loss: 0.5476 ACC@1: 81.36% ACC@5: 95.14%: 100%|██████████| 79/79 [00:11<00:00,  6.97it/s]\n","Test Epoch: [60/100] lr: 0.0001 Loss: 0.7237 ACC@1: 79.69% ACC@5: 95.38%: 100%|██████████| 79/79 [00:04<00:00, 16.94it/s]\n","Train Epoch: [61/100] lr: 0.0001 Loss: 0.4780 ACC@1: 83.77% ACC@5: 95.76%: 100%|██████████| 79/79 [00:11<00:00,  7.03it/s]\n","Test Epoch: [61/100] lr: 0.0001 Loss: 0.5917 ACC@1: 82.82% ACC@5: 96.71%: 100%|██████████| 79/79 [00:04<00:00, 16.89it/s]\n","Train Epoch: [62/100] lr: 0.0001 Loss: 0.4285 ACC@1: 85.29% ACC@5: 96.46%: 100%|██████████| 79/79 [00:11<00:00,  7.14it/s]\n","Test Epoch: [62/100] lr: 0.0001 Loss: 0.5857 ACC@1: 82.93% ACC@5: 96.80%: 100%|██████████| 79/79 [00:04<00:00, 16.89it/s]\n","Train Epoch: [63/100] lr: 0.0001 Loss: 0.4177 ACC@1: 85.94% ACC@5: 96.31%: 100%|██████████| 79/79 [00:11<00:00,  6.94it/s]\n","Test Epoch: [63/100] lr: 0.0001 Loss: 0.6201 ACC@1: 82.65% ACC@5: 96.55%: 100%|██████████| 79/79 [00:04<00:00, 16.79it/s]\n","Train Epoch: [64/100] lr: 0.0001 Loss: 0.4196 ACC@1: 85.78% ACC@5: 96.38%: 100%|██████████| 79/79 [00:11<00:00,  7.03it/s]\n","Test Epoch: [64/100] lr: 0.0001 Loss: 0.6027 ACC@1: 82.95% ACC@5: 96.83%: 100%|██████████| 79/79 [00:04<00:00, 16.63it/s]\n","Train Epoch: [65/100] lr: 0.0001 Loss: 0.4022 ACC@1: 86.38% ACC@5: 96.70%: 100%|██████████| 79/79 [00:11<00:00,  6.89it/s]\n","Test Epoch: [65/100] lr: 0.0001 Loss: 0.6367 ACC@1: 82.80% ACC@5: 96.70%: 100%|██████████| 79/79 [00:04<00:00, 16.91it/s]\n","Train Epoch: [66/100] lr: 0.0001 Loss: 0.4234 ACC@1: 85.39% ACC@5: 96.35%: 100%|██████████| 79/79 [00:11<00:00,  7.15it/s]\n","Test Epoch: [66/100] lr: 0.0001 Loss: 0.6115 ACC@1: 83.27% ACC@5: 96.86%: 100%|██████████| 79/79 [00:04<00:00, 16.72it/s]\n","Train Epoch: [67/100] lr: 0.0001 Loss: 0.4104 ACC@1: 85.76% ACC@5: 96.50%: 100%|██████████| 79/79 [00:12<00:00,  6.50it/s]\n","Test Epoch: [67/100] lr: 0.0001 Loss: 0.6238 ACC@1: 82.99% ACC@5: 96.90%: 100%|██████████| 79/79 [00:04<00:00, 16.75it/s]\n","Train Epoch: [68/100] lr: 0.0001 Loss: 0.3965 ACC@1: 86.36% ACC@5: 96.72%: 100%|██████████| 79/79 [00:11<00:00,  7.12it/s]\n","Test Epoch: [68/100] lr: 0.0001 Loss: 0.6461 ACC@1: 82.96% ACC@5: 96.68%: 100%|██████████| 79/79 [00:04<00:00, 16.73it/s]\n","Train Epoch: [69/100] lr: 0.0001 Loss: 0.3918 ACC@1: 86.40% ACC@5: 96.63%: 100%|██████████| 79/79 [00:11<00:00,  6.91it/s]\n","Test Epoch: [69/100] lr: 0.0001 Loss: 0.6551 ACC@1: 82.99% ACC@5: 96.81%: 100%|██████████| 79/79 [00:04<00:00, 16.58it/s]\n","Train Epoch: [70/100] lr: 0.0001 Loss: 0.3868 ACC@1: 86.46% ACC@5: 96.65%: 100%|██████████| 79/79 [00:11<00:00,  7.07it/s]\n","Test Epoch: [70/100] lr: 0.0001 Loss: 0.6400 ACC@1: 83.28% ACC@5: 96.75%: 100%|██████████| 79/79 [00:05<00:00, 14.76it/s]\n","Train Epoch: [71/100] lr: 0.0001 Loss: 0.3775 ACC@1: 86.99% ACC@5: 96.81%: 100%|██████████| 79/79 [00:11<00:00,  6.74it/s]\n","Test Epoch: [71/100] lr: 0.0001 Loss: 0.6480 ACC@1: 82.98% ACC@5: 96.60%: 100%|██████████| 79/79 [00:04<00:00, 16.59it/s]\n","Train Epoch: [72/100] lr: 0.0001 Loss: 0.3725 ACC@1: 87.15% ACC@5: 96.69%: 100%|██████████| 79/79 [00:11<00:00,  7.02it/s]\n","Test Epoch: [72/100] lr: 0.0001 Loss: 0.6355 ACC@1: 83.48% ACC@5: 96.82%: 100%|██████████| 79/79 [00:04<00:00, 16.72it/s]\n","Train Epoch: [73/100] lr: 0.0001 Loss: 0.3718 ACC@1: 87.40% ACC@5: 96.98%: 100%|██████████| 79/79 [00:11<00:00,  7.07it/s]\n","Test Epoch: [73/100] lr: 0.0001 Loss: 0.6716 ACC@1: 83.30% ACC@5: 96.65%: 100%|██████████| 79/79 [00:04<00:00, 16.64it/s]\n","Train Epoch: [74/100] lr: 0.0001 Loss: 0.3648 ACC@1: 87.45% ACC@5: 96.86%: 100%|██████████| 79/79 [00:11<00:00,  6.95it/s]\n","Test Epoch: [74/100] lr: 0.0001 Loss: 0.6776 ACC@1: 83.18% ACC@5: 96.73%: 100%|██████████| 79/79 [00:05<00:00, 13.19it/s]\n","Train Epoch: [75/100] lr: 0.0001 Loss: 0.3757 ACC@1: 86.74% ACC@5: 96.68%: 100%|██████████| 79/79 [00:11<00:00,  7.02it/s]\n","Test Epoch: [75/100] lr: 0.0001 Loss: 0.6788 ACC@1: 83.15% ACC@5: 96.76%: 100%|██████████| 79/79 [00:04<00:00, 16.77it/s]\n","Train Epoch: [76/100] lr: 0.0001 Loss: 0.3767 ACC@1: 87.10% ACC@5: 97.01%: 100%|██████████| 79/79 [00:11<00:00,  7.17it/s]\n","Test Epoch: [76/100] lr: 0.0001 Loss: 0.6858 ACC@1: 83.34% ACC@5: 96.78%: 100%|██████████| 79/79 [00:04<00:00, 16.73it/s]\n","Train Epoch: [77/100] lr: 0.0001 Loss: 0.3619 ACC@1: 87.34% ACC@5: 97.08%: 100%|██████████| 79/79 [00:11<00:00,  7.05it/s]\n","Test Epoch: [77/100] lr: 0.0001 Loss: 0.6879 ACC@1: 83.15% ACC@5: 96.75%: 100%|██████████| 79/79 [00:04<00:00, 16.74it/s]\n","Train Epoch: [78/100] lr: 0.0001 Loss: 0.3709 ACC@1: 87.05% ACC@5: 96.82%: 100%|██████████| 79/79 [00:11<00:00,  6.99it/s]\n","Test Epoch: [78/100] lr: 0.0001 Loss: 0.6793 ACC@1: 83.54% ACC@5: 96.70%: 100%|██████████| 79/79 [00:04<00:00, 16.52it/s]\n","Train Epoch: [79/100] lr: 0.0001 Loss: 0.3614 ACC@1: 87.13% ACC@5: 97.01%: 100%|██████████| 79/79 [00:11<00:00,  7.05it/s]\n","Test Epoch: [79/100] lr: 0.0001 Loss: 0.6679 ACC@1: 83.75% ACC@5: 96.96%: 100%|██████████| 79/79 [00:04<00:00, 16.67it/s]\n","Train Epoch: [80/100] lr: 0.0001 Loss: 0.3771 ACC@1: 87.11% ACC@5: 96.75%: 100%|██████████| 79/79 [00:11<00:00,  6.98it/s]\n","Test Epoch: [80/100] lr: 0.0000 Loss: 0.6755 ACC@1: 83.40% ACC@5: 96.91%: 100%|██████████| 79/79 [00:04<00:00, 16.38it/s]\n","Train Epoch: [81/100] lr: 0.0000 Loss: 0.3588 ACC@1: 87.95% ACC@5: 97.24%: 100%|██████████| 79/79 [00:11<00:00,  7.14it/s]\n","Test Epoch: [81/100] lr: 0.0000 Loss: 0.6659 ACC@1: 83.63% ACC@5: 96.95%: 100%|██████████| 79/79 [00:04<00:00, 16.51it/s]\n","Train Epoch: [82/100] lr: 0.0000 Loss: 0.3374 ACC@1: 88.02% ACC@5: 97.27%: 100%|██████████| 79/79 [00:11<00:00,  6.83it/s]\n","Test Epoch: [82/100] lr: 0.0000 Loss: 0.6650 ACC@1: 83.63% ACC@5: 96.98%: 100%|██████████| 79/79 [00:04<00:00, 16.60it/s]\n","Train Epoch: [83/100] lr: 0.0000 Loss: 0.3511 ACC@1: 88.00% ACC@5: 97.06%: 100%|██████████| 79/79 [00:11<00:00,  7.09it/s]\n","Test Epoch: [83/100] lr: 0.0000 Loss: 0.6626 ACC@1: 83.68% ACC@5: 96.96%: 100%|██████████| 79/79 [00:04<00:00, 16.93it/s]\n","Train Epoch: [84/100] lr: 0.0000 Loss: 0.3591 ACC@1: 87.58% ACC@5: 97.03%: 100%|██████████| 79/79 [00:11<00:00,  7.09it/s]\n","Test Epoch: [84/100] lr: 0.0000 Loss: 0.6744 ACC@1: 83.69% ACC@5: 97.04%: 100%|██████████| 79/79 [00:04<00:00, 16.69it/s]\n","Train Epoch: [85/100] lr: 0.0000 Loss: 0.3505 ACC@1: 87.56% ACC@5: 97.09%: 100%|██████████| 79/79 [00:11<00:00,  7.12it/s]\n","Test Epoch: [85/100] lr: 0.0000 Loss: 0.6812 ACC@1: 83.85% ACC@5: 96.98%: 100%|██████████| 79/79 [00:04<00:00, 16.60it/s]\n","Train Epoch: [86/100] lr: 0.0000 Loss: 0.3375 ACC@1: 88.38% ACC@5: 97.19%: 100%|██████████| 79/79 [00:12<00:00,  6.49it/s]\n","Test Epoch: [86/100] lr: 0.0000 Loss: 0.6632 ACC@1: 83.89% ACC@5: 97.00%: 100%|██████████| 79/79 [00:04<00:00, 16.62it/s]\n","Train Epoch: [87/100] lr: 0.0000 Loss: 0.3494 ACC@1: 87.94% ACC@5: 97.03%: 100%|██████████| 79/79 [00:11<00:00,  7.07it/s]\n","Test Epoch: [87/100] lr: 0.0000 Loss: 0.6857 ACC@1: 83.62% ACC@5: 96.92%: 100%|██████████| 79/79 [00:04<00:00, 16.43it/s]\n","Train Epoch: [88/100] lr: 0.0000 Loss: 0.3632 ACC@1: 87.31% ACC@5: 96.72%: 100%|██████████| 79/79 [00:11<00:00,  7.02it/s]\n","Test Epoch: [88/100] lr: 0.0000 Loss: 0.6558 ACC@1: 83.99% ACC@5: 97.08%: 100%|██████████| 79/79 [00:04<00:00, 16.95it/s]\n","Train Epoch: [89/100] lr: 0.0000 Loss: 0.3400 ACC@1: 88.34% ACC@5: 97.18%: 100%|██████████| 79/79 [00:11<00:00,  7.05it/s]\n","Test Epoch: [89/100] lr: 0.0000 Loss: 0.6597 ACC@1: 83.79% ACC@5: 97.08%: 100%|██████████| 79/79 [00:05<00:00, 15.21it/s]\n","Train Epoch: [90/100] lr: 0.0000 Loss: 0.3421 ACC@1: 88.12% ACC@5: 97.43%: 100%|██████████| 79/79 [00:12<00:00,  6.55it/s]\n","Test Epoch: [90/100] lr: 0.0000 Loss: 0.6550 ACC@1: 83.93% ACC@5: 97.00%: 100%|██████████| 79/79 [00:04<00:00, 16.65it/s]\n","Train Epoch: [91/100] lr: 0.0000 Loss: 0.3396 ACC@1: 88.32% ACC@5: 97.05%: 100%|██████████| 79/79 [00:11<00:00,  7.11it/s]\n","Test Epoch: [91/100] lr: 0.0000 Loss: 0.6647 ACC@1: 83.86% ACC@5: 96.97%: 100%|██████████| 79/79 [00:04<00:00, 16.75it/s]\n","Train Epoch: [92/100] lr: 0.0000 Loss: 0.3372 ACC@1: 88.39% ACC@5: 97.28%: 100%|██████████| 79/79 [00:11<00:00,  7.09it/s]\n","Test Epoch: [92/100] lr: 0.0000 Loss: 0.6761 ACC@1: 83.74% ACC@5: 97.01%: 100%|██████████| 79/79 [00:04<00:00, 16.75it/s]\n","Train Epoch: [93/100] lr: 0.0000 Loss: 0.3432 ACC@1: 88.17% ACC@5: 97.08%: 100%|██████████| 79/79 [00:11<00:00,  6.91it/s]\n","Test Epoch: [93/100] lr: 0.0000 Loss: 0.6696 ACC@1: 83.81% ACC@5: 96.92%: 100%|██████████| 79/79 [00:06<00:00, 12.82it/s]\n","Train Epoch: [94/100] lr: 0.0000 Loss: 0.3402 ACC@1: 88.61% ACC@5: 97.00%: 100%|██████████| 79/79 [00:11<00:00,  7.02it/s]\n","Test Epoch: [94/100] lr: 0.0000 Loss: 0.6745 ACC@1: 83.54% ACC@5: 96.98%: 100%|██████████| 79/79 [00:04<00:00, 16.35it/s]\n","Train Epoch: [95/100] lr: 0.0000 Loss: 0.3439 ACC@1: 88.43% ACC@5: 97.22%: 100%|██████████| 79/79 [00:11<00:00,  7.03it/s]\n","Test Epoch: [95/100] lr: 0.0000 Loss: 0.6869 ACC@1: 83.54% ACC@5: 97.03%: 100%|██████████| 79/79 [00:04<00:00, 16.63it/s]\n","Train Epoch: [96/100] lr: 0.0000 Loss: 0.3449 ACC@1: 87.84% ACC@5: 97.14%: 100%|██████████| 79/79 [00:11<00:00,  6.97it/s]\n","Test Epoch: [96/100] lr: 0.0000 Loss: 0.6823 ACC@1: 83.74% ACC@5: 96.94%: 100%|██████████| 79/79 [00:04<00:00, 16.59it/s]\n","Train Epoch: [97/100] lr: 0.0000 Loss: 0.3373 ACC@1: 88.03% ACC@5: 97.21%: 100%|██████████| 79/79 [00:11<00:00,  6.97it/s]\n","Test Epoch: [97/100] lr: 0.0000 Loss: 0.6670 ACC@1: 84.05% ACC@5: 97.01%: 100%|██████████| 79/79 [00:04<00:00, 16.48it/s]\n","Train Epoch: [98/100] lr: 0.0000 Loss: 0.3441 ACC@1: 87.74% ACC@5: 97.33%: 100%|██████████| 79/79 [00:11<00:00,  7.01it/s]\n","Test Epoch: [98/100] lr: 0.0000 Loss: 0.6639 ACC@1: 83.99% ACC@5: 97.02%: 100%|██████████| 79/79 [00:04<00:00, 16.36it/s]\n","Train Epoch: [99/100] lr: 0.0000 Loss: 0.3415 ACC@1: 88.23% ACC@5: 97.19%: 100%|██████████| 79/79 [00:11<00:00,  7.04it/s]\n","Test Epoch: [99/100] lr: 0.0000 Loss: 0.6717 ACC@1: 83.92% ACC@5: 96.94%: 100%|██████████| 79/79 [00:04<00:00, 16.07it/s]\n","Train Epoch: [100/100] lr: 0.0000 Loss: 0.3494 ACC@1: 88.11% ACC@5: 97.02%: 100%|██████████| 79/79 [00:11<00:00,  7.03it/s]\n","Test Epoch: [100/100] lr: 0.0000 Loss: 0.6693 ACC@1: 83.94% ACC@5: 96.94%: 100%|██████████| 79/79 [00:04<00:00, 16.39it/s]\n"]},{"output_type":"stream","name":"stdout","text":["[88.11]\n","[83.94]\n","top10 trainacc 88.11\n","top10 testacc 83.94\n","best test 83.94\n","88.11\n","83.94\n","0.0\n","0.0\n"]}]},{"cell_type":"code","source":["import random\n","def rand_bbox(size, lam):\n","    W = size\n","    H = size\n","    cut_rat = np.sqrt(1. - lam)\n","    cut_w = np.int(W * cut_rat)\n","    cut_h = np.int(H * cut_rat)\n","\n","    # uniform\n","    cx = np.random.randint(W)\n","    cy = np.random.randint(H)\n","\n","    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n","    bby1 = np.clip(cy - cut_h // 2, 0, H)\n","    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n","    bby2 = np.clip(cy + cut_h // 2, 0, H)\n","    #print(bbx1, bby1, bbx2, bby2)\n","    return bbx1, bby1, bbx2, bby2\n","\n","def train_val(net, data_loader, train_optimizer):\n","    global lr\n","    schedule = [60, 80]\n","    criterion = nn.CrossEntropyLoss().cuda()\n","    is_train = train_optimizer is not None\n","    net.train() if is_train else net.eval()\n","    sa = 0\n","    total_loss, total_correct_1, total_correct_3, total_num, data_bar = 0.0, 0.0, 0.0, 0, tqdm(data_loader, position=0, leave=True)\n","    with (torch.enable_grad() if is_train else torch.no_grad()):\n","        for data, target in data_bar:\n","            #print(target.size())\n","\n","            #insert cropping here\n","            #print(data.shape)\\\n","            if is_train:\n","              dupe= data.clone()\n","              #print(dupe.shape)\n","              for i in range(dupe.shape[0]):\n","                if random.random()<0.5:\n","                  continue\n","                lam=np.random.beta(1,1)\n","                x1,y1,x2,y2=rand_bbox(32,lam)\n","                #print(target[i].size())\n","                try:\n","                  dupe[i][:,x1:x2,y1:y2]=0\n","                except:\n","                  print(dupe[i].shape,i,x1,x2,y1,y2)\n","                  sa+=1\n","              data = dupe\n","            data, target = data.cuda(non_blocking=True), target.cuda(non_blocking=True)\n","            out = net(data)\n","            loss = criterion(out, target)\n","\n","            if is_train:\n","                train_optimizer.zero_grad()\n","                loss.backward()\n","                train_optimizer.step()\n","\n","\n","            total_num += data.size(0)\n","            total_loss += loss.item() * data.size(0)\n","            prediction = torch.argsort(out, dim=-1, descending=True)\n","            total_correct_1 += torch.sum((prediction[:, 0:1] == target.unsqueeze(dim=-1)).any(dim=-1).float()).item()\n","            total_correct_3 += torch.sum((prediction[:, 0:3] == target.unsqueeze(dim=-1)).any(dim=-1).float()).item()\n","            \n","            data_bar.set_description('{} Epoch: [{}/{}] lr: {:.4f} Loss: {:.4f} ACC@1: {:.2f}% ACC@5: {:.2f}%'\n","                                     .format('Train' if is_train else 'Test', epoch, epochs, lr, total_loss / total_num,\n","                                             total_correct_1 / total_num * 100, total_correct_3 / total_num * 100))\n","        if is_train:  \n","          if schedule is not None:\n","            for milestone in schedule:\n","                lr *= 0.1 if epoch == milestone else 1.\n","            for param_group in train_optimizer.param_groups:\n","                param_group['lr'] = lr\n","\n","        return total_correct_1/total_num * 100\n","\n","train_acc = []\n","test_acc = []\n","for i in range(1):\n","  print(f\"Random Seed: {i}\")\n","  best_train = 0\n","  best_test = 0\n","  np.random.seed(i)\n","\n","  indices = get_indices(train_data, 1000)\n","  sampler = torch.utils.data.SubsetRandomSampler(indices)\n","\n","  train_loader = DataLoader(train_data, batch_size=batch_size, sampler = sampler , num_workers=16, pin_memory=True)\n","\n","  resnet18 = ModelBase()\n","  resnet18.cuda()\n","  lr = 1e-3\n","  optimizer = torch.optim.Adam(resnet18.parameters(), lr=lr, weight_decay = 1e-5)\n","  epoch_start = 1\n","  epochs = 100\n","  for epoch in range(epoch_start, epochs+1):\n","    acc1 = train_val(resnet18, train_loader, optimizer)\n","    acc2 = train_val(resnet18, test_loader, None)\n","    if acc1 > best_train:\n","      best_train = acc1\n","    if acc2 > best_test:\n","      best_test = acc2\n","  train_acc.append(acc1)\n","  test_acc.append(acc2)\n","  \n","  torch.save(resnet18.state_dict(), \"/content/drive/My Drive/Models/Supervised/supervisedcutoutresnet18v1.1.pth\")\n","print(train_acc)\n","print(test_acc)\n","import numpy as np\n","train_acc = np.array(train_acc)\n","test_acc = np.array(test_acc)\n","print(np.mean(train_acc))\n","print(np.mean(test_acc))\n","print(np.std(train_acc))\n","print(np.std(test_acc))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yXBgycP60A4-","outputId":"8cd43c6f-81c3-4e3d-b253-c992bd2f33d7","executionInfo":{"status":"ok","timestamp":1668931086181,"user_tz":-480,"elapsed":1194731,"user":{"displayName":"Souw Chuan Neo","userId":"08556196228836469917"}}},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Random Seed: 0\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","  0%|          | 0/79 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  \n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  import sys\n","Train Epoch: [1/100] lr: 0.0010 Loss: 1.9736 ACC@1: 26.90% ACC@5: 60.18%: 100%|██████████| 79/79 [00:12<00:00,  6.35it/s]\n","Test Epoch: [1/100] lr: 0.0010 Loss: 1.7584 ACC@1: 37.16% ACC@5: 70.44%: 100%|██████████| 79/79 [00:04<00:00, 16.19it/s]\n","Train Epoch: [2/100] lr: 0.0010 Loss: 1.8042 ACC@1: 33.54% ACC@5: 68.02%: 100%|██████████| 79/79 [00:12<00:00,  6.09it/s]\n","Test Epoch: [2/100] lr: 0.0010 Loss: 1.5462 ACC@1: 43.64% ACC@5: 76.55%: 100%|██████████| 79/79 [00:04<00:00, 16.04it/s]\n","Train Epoch: [3/100] lr: 0.0010 Loss: 1.7363 ACC@1: 36.45% ACC@5: 70.87%: 100%|██████████| 79/79 [00:12<00:00,  6.29it/s]\n","Test Epoch: [3/100] lr: 0.0010 Loss: 1.6230 ACC@1: 39.81% ACC@5: 78.08%: 100%|██████████| 79/79 [00:04<00:00, 16.36it/s]\n","Train Epoch: [4/100] lr: 0.0010 Loss: 1.6479 ACC@1: 39.88% ACC@5: 74.24%: 100%|██████████| 79/79 [00:12<00:00,  6.41it/s]\n","Test Epoch: [4/100] lr: 0.0010 Loss: 1.9488 ACC@1: 37.24% ACC@5: 73.78%: 100%|██████████| 79/79 [00:04<00:00, 16.12it/s]\n","Train Epoch: [5/100] lr: 0.0010 Loss: 1.6037 ACC@1: 41.47% ACC@5: 75.21%: 100%|██████████| 79/79 [00:12<00:00,  6.45it/s]\n","Test Epoch: [5/100] lr: 0.0010 Loss: 1.4890 ACC@1: 45.58% ACC@5: 80.09%: 100%|██████████| 79/79 [00:04<00:00, 16.24it/s]\n","Train Epoch: [6/100] lr: 0.0010 Loss: 1.5432 ACC@1: 43.99% ACC@5: 76.62%: 100%|██████████| 79/79 [00:12<00:00,  6.41it/s]\n","Test Epoch: [6/100] lr: 0.0010 Loss: 1.7801 ACC@1: 42.15% ACC@5: 74.37%: 100%|██████████| 79/79 [00:04<00:00, 16.49it/s]\n","Train Epoch: [7/100] lr: 0.0010 Loss: 1.5099 ACC@1: 45.46% ACC@5: 77.41%: 100%|██████████| 79/79 [00:12<00:00,  6.47it/s]\n","Test Epoch: [7/100] lr: 0.0010 Loss: 1.4372 ACC@1: 48.97% ACC@5: 79.73%: 100%|██████████| 79/79 [00:04<00:00, 16.27it/s]\n","Train Epoch: [8/100] lr: 0.0010 Loss: 1.4541 ACC@1: 47.40% ACC@5: 78.97%: 100%|██████████| 79/79 [00:12<00:00,  6.36it/s]\n","Test Epoch: [8/100] lr: 0.0010 Loss: 1.5729 ACC@1: 50.70% ACC@5: 82.11%: 100%|██████████| 79/79 [00:04<00:00, 16.11it/s]\n","Train Epoch: [9/100] lr: 0.0010 Loss: 1.4309 ACC@1: 48.58% ACC@5: 79.20%: 100%|██████████| 79/79 [00:12<00:00,  6.45it/s]\n","Test Epoch: [9/100] lr: 0.0010 Loss: 1.5521 ACC@1: 50.13% ACC@5: 82.46%: 100%|██████████| 79/79 [00:04<00:00, 15.95it/s]\n","Train Epoch: [10/100] lr: 0.0010 Loss: 1.3983 ACC@1: 49.74% ACC@5: 80.58%: 100%|██████████| 79/79 [00:12<00:00,  6.39it/s]\n","Test Epoch: [10/100] lr: 0.0010 Loss: 1.2301 ACC@1: 55.48% ACC@5: 85.77%: 100%|██████████| 79/79 [00:04<00:00, 16.32it/s]\n","Train Epoch: [11/100] lr: 0.0010 Loss: 1.3546 ACC@1: 51.55% ACC@5: 81.37%: 100%|██████████| 79/79 [00:12<00:00,  6.35it/s]\n","Test Epoch: [11/100] lr: 0.0010 Loss: 1.3361 ACC@1: 54.21% ACC@5: 84.73%: 100%|██████████| 79/79 [00:04<00:00, 16.24it/s]\n","Train Epoch: [12/100] lr: 0.0010 Loss: 1.3202 ACC@1: 51.77% ACC@5: 82.47%: 100%|██████████| 79/79 [00:12<00:00,  6.33it/s]\n","Test Epoch: [12/100] lr: 0.0010 Loss: 1.1470 ACC@1: 59.55% ACC@5: 87.39%: 100%|██████████| 79/79 [00:04<00:00, 16.03it/s]\n","Train Epoch: [13/100] lr: 0.0010 Loss: 1.3137 ACC@1: 53.16% ACC@5: 82.73%: 100%|██████████| 79/79 [00:12<00:00,  6.41it/s]\n","Test Epoch: [13/100] lr: 0.0010 Loss: 1.4064 ACC@1: 55.27% ACC@5: 84.10%: 100%|██████████| 79/79 [00:04<00:00, 16.20it/s]\n","Train Epoch: [14/100] lr: 0.0010 Loss: 1.2848 ACC@1: 53.83% ACC@5: 83.64%: 100%|██████████| 79/79 [00:12<00:00,  6.38it/s]\n","Test Epoch: [14/100] lr: 0.0010 Loss: 1.2691 ACC@1: 56.95% ACC@5: 87.14%: 100%|██████████| 79/79 [00:04<00:00, 16.10it/s]\n","Train Epoch: [15/100] lr: 0.0010 Loss: 1.2556 ACC@1: 55.51% ACC@5: 83.53%: 100%|██████████| 79/79 [00:12<00:00,  6.45it/s]\n","Test Epoch: [15/100] lr: 0.0010 Loss: 1.5878 ACC@1: 50.54% ACC@5: 86.02%: 100%|██████████| 79/79 [00:04<00:00, 16.21it/s]\n","Train Epoch: [16/100] lr: 0.0010 Loss: 1.2352 ACC@1: 55.80% ACC@5: 84.12%: 100%|██████████| 79/79 [00:12<00:00,  6.37it/s]\n","Test Epoch: [16/100] lr: 0.0010 Loss: 1.2429 ACC@1: 57.98% ACC@5: 85.50%: 100%|██████████| 79/79 [00:04<00:00, 16.19it/s]\n","Train Epoch: [17/100] lr: 0.0010 Loss: 1.2184 ACC@1: 56.83% ACC@5: 84.24%: 100%|██████████| 79/79 [00:12<00:00,  6.38it/s]\n","Test Epoch: [17/100] lr: 0.0010 Loss: 1.4727 ACC@1: 52.24% ACC@5: 84.88%: 100%|██████████| 79/79 [00:04<00:00, 16.06it/s]\n","Train Epoch: [18/100] lr: 0.0010 Loss: 1.2091 ACC@1: 56.47% ACC@5: 85.01%: 100%|██████████| 79/79 [00:12<00:00,  6.44it/s]\n","Test Epoch: [18/100] lr: 0.0010 Loss: 1.0691 ACC@1: 64.65% ACC@5: 88.52%: 100%|██████████| 79/79 [00:04<00:00, 15.82it/s]\n","Train Epoch: [19/100] lr: 0.0010 Loss: 1.1695 ACC@1: 58.16% ACC@5: 85.69%: 100%|██████████| 79/79 [00:12<00:00,  6.37it/s]\n","Test Epoch: [19/100] lr: 0.0010 Loss: 1.0422 ACC@1: 65.59% ACC@5: 90.14%: 100%|██████████| 79/79 [00:04<00:00, 15.95it/s]\n","Train Epoch: [20/100] lr: 0.0010 Loss: 1.1435 ACC@1: 59.54% ACC@5: 86.02%: 100%|██████████| 79/79 [00:12<00:00,  6.21it/s]\n","Test Epoch: [20/100] lr: 0.0010 Loss: 1.1385 ACC@1: 61.36% ACC@5: 88.79%: 100%|██████████| 79/79 [00:05<00:00, 13.18it/s]\n","Train Epoch: [21/100] lr: 0.0010 Loss: 1.1259 ACC@1: 60.06% ACC@5: 86.21%: 100%|██████████| 79/79 [00:12<00:00,  6.31it/s]\n","Test Epoch: [21/100] lr: 0.0010 Loss: 1.0588 ACC@1: 64.69% ACC@5: 90.85%: 100%|██████████| 79/79 [00:04<00:00, 16.18it/s]\n","Train Epoch: [22/100] lr: 0.0010 Loss: 1.1268 ACC@1: 59.71% ACC@5: 86.45%: 100%|██████████| 79/79 [00:12<00:00,  6.40it/s]\n","Test Epoch: [22/100] lr: 0.0010 Loss: 0.9990 ACC@1: 65.90% ACC@5: 90.26%: 100%|██████████| 79/79 [00:04<00:00, 16.06it/s]\n","Train Epoch: [23/100] lr: 0.0010 Loss: 1.1099 ACC@1: 60.71% ACC@5: 86.64%: 100%|██████████| 79/79 [00:12<00:00,  6.38it/s]\n","Test Epoch: [23/100] lr: 0.0010 Loss: 1.0546 ACC@1: 66.55% ACC@5: 89.99%: 100%|██████████| 79/79 [00:04<00:00, 15.98it/s]\n","Train Epoch: [24/100] lr: 0.0010 Loss: 1.0880 ACC@1: 61.81% ACC@5: 87.02%: 100%|██████████| 79/79 [00:12<00:00,  6.41it/s]\n","Test Epoch: [24/100] lr: 0.0010 Loss: 0.8970 ACC@1: 69.73% ACC@5: 91.74%: 100%|██████████| 79/79 [00:04<00:00, 16.06it/s]\n","Train Epoch: [25/100] lr: 0.0010 Loss: 1.0917 ACC@1: 60.96% ACC@5: 87.28%: 100%|██████████| 79/79 [00:12<00:00,  6.38it/s]\n","Test Epoch: [25/100] lr: 0.0010 Loss: 0.9969 ACC@1: 66.17% ACC@5: 89.61%: 100%|██████████| 79/79 [00:04<00:00, 15.83it/s]\n","Train Epoch: [26/100] lr: 0.0010 Loss: 1.0570 ACC@1: 62.71% ACC@5: 87.75%: 100%|██████████| 79/79 [00:12<00:00,  6.36it/s]\n","Test Epoch: [26/100] lr: 0.0010 Loss: 0.9280 ACC@1: 69.71% ACC@5: 92.23%: 100%|██████████| 79/79 [00:04<00:00, 15.84it/s]\n","Train Epoch: [27/100] lr: 0.0010 Loss: 1.0602 ACC@1: 61.96% ACC@5: 87.59%: 100%|██████████| 79/79 [00:12<00:00,  6.37it/s]\n","Test Epoch: [27/100] lr: 0.0010 Loss: 0.9913 ACC@1: 68.25% ACC@5: 91.77%: 100%|██████████| 79/79 [00:05<00:00, 15.73it/s]\n","Train Epoch: [28/100] lr: 0.0010 Loss: 1.0301 ACC@1: 63.55% ACC@5: 87.96%: 100%|██████████| 79/79 [00:12<00:00,  6.35it/s]\n","Test Epoch: [28/100] lr: 0.0010 Loss: 1.0500 ACC@1: 67.24% ACC@5: 89.93%: 100%|██████████| 79/79 [00:04<00:00, 15.90it/s]\n","Train Epoch: [29/100] lr: 0.0010 Loss: 1.0453 ACC@1: 63.27% ACC@5: 87.87%: 100%|██████████| 79/79 [00:12<00:00,  6.35it/s]\n","Test Epoch: [29/100] lr: 0.0010 Loss: 1.0066 ACC@1: 66.83% ACC@5: 91.84%: 100%|██████████| 79/79 [00:05<00:00, 13.25it/s]\n","Train Epoch: [30/100] lr: 0.0010 Loss: 1.0102 ACC@1: 63.74% ACC@5: 88.45%: 100%|██████████| 79/79 [00:12<00:00,  6.29it/s]\n","Test Epoch: [30/100] lr: 0.0010 Loss: 1.0246 ACC@1: 66.95% ACC@5: 91.80%: 100%|██████████| 79/79 [00:04<00:00, 15.92it/s]\n","Train Epoch: [31/100] lr: 0.0010 Loss: 0.9944 ACC@1: 65.15% ACC@5: 88.70%: 100%|██████████| 79/79 [00:12<00:00,  6.42it/s]\n","Test Epoch: [31/100] lr: 0.0010 Loss: 0.9386 ACC@1: 70.31% ACC@5: 93.00%: 100%|██████████| 79/79 [00:04<00:00, 15.98it/s]\n","Train Epoch: [32/100] lr: 0.0010 Loss: 0.9869 ACC@1: 65.67% ACC@5: 88.73%: 100%|██████████| 79/79 [00:12<00:00,  6.40it/s]\n","Test Epoch: [32/100] lr: 0.0010 Loss: 0.8027 ACC@1: 74.12% ACC@5: 93.84%: 100%|██████████| 79/79 [00:04<00:00, 15.92it/s]\n","Train Epoch: [33/100] lr: 0.0010 Loss: 0.9776 ACC@1: 65.49% ACC@5: 89.09%: 100%|██████████| 79/79 [00:12<00:00,  6.38it/s]\n","Test Epoch: [33/100] lr: 0.0010 Loss: 1.0644 ACC@1: 66.64% ACC@5: 90.04%: 100%|██████████| 79/79 [00:04<00:00, 15.88it/s]\n","Train Epoch: [34/100] lr: 0.0010 Loss: 0.9587 ACC@1: 66.45% ACC@5: 89.76%: 100%|██████████| 79/79 [00:12<00:00,  6.32it/s]\n","Test Epoch: [34/100] lr: 0.0010 Loss: 0.8754 ACC@1: 72.03% ACC@5: 92.95%: 100%|██████████| 79/79 [00:05<00:00, 15.36it/s]\n","\n","Train Epoch: [35/100] lr: 0.0010 Loss: 0.9458 ACC@1: 65.93% ACC@5: 89.47%: 100%|██████████| 79/79 [00:12<00:00,  6.28it/s]\n","\n","Test Epoch: [35/100] lr: 0.0010 Loss: 0.8901 ACC@1: 71.62% ACC@5: 92.91%: 100%|██████████| 79/79 [00:05<00:00, 15.55it/s]\n","\n","Train Epoch: [36/100] lr: 0.0010 Loss: 0.9425 ACC@1: 67.01% ACC@5: 89.66%: 100%|██████████| 79/79 [00:12<00:00,  6.26it/s]\n","\n","Test Epoch: [36/100] lr: 0.0010 Loss: 0.8585 ACC@1: 72.95% ACC@5: 93.34%: 100%|██████████| 79/79 [00:05<00:00, 15.04it/s]\n","\n","Train Epoch: [37/100] lr: 0.0010 Loss: 0.9186 ACC@1: 67.51% ACC@5: 90.17%: 100%|██████████| 79/79 [00:12<00:00,  6.23it/s]\n","\n","Test Epoch: [37/100] lr: 0.0010 Loss: 0.9971 ACC@1: 70.18% ACC@5: 92.23%: 100%|██████████| 79/79 [00:05<00:00, 15.35it/s]\n","\n","Train Epoch: [38/100] lr: 0.0010 Loss: 0.9412 ACC@1: 67.49% ACC@5: 89.48%: 100%|██████████| 79/79 [00:12<00:00,  6.29it/s]\n","\n","Test Epoch: [38/100] lr: 0.0010 Loss: 0.9516 ACC@1: 69.95% ACC@5: 93.40%: 100%|██████████| 79/79 [00:05<00:00, 13.83it/s]\n","\n","Train Epoch: [39/100] lr: 0.0010 Loss: 0.9074 ACC@1: 67.43% ACC@5: 89.75%: 100%|██████████| 79/79 [00:12<00:00,  6.27it/s]\n","\n","Test Epoch: [39/100] lr: 0.0010 Loss: 0.9443 ACC@1: 70.75% ACC@5: 92.46%: 100%|██████████| 79/79 [00:05<00:00, 15.34it/s]\n","\n","Train Epoch: [40/100] lr: 0.0010 Loss: 0.8959 ACC@1: 67.75% ACC@5: 90.83%: 100%|██████████| 79/79 [00:12<00:00,  6.22it/s]\n","\n","Test Epoch: [40/100] lr: 0.0010 Loss: 0.8907 ACC@1: 71.66% ACC@5: 93.37%: 100%|██████████| 79/79 [00:05<00:00, 15.46it/s]\n","\n","Train Epoch: [41/100] lr: 0.0010 Loss: 0.9007 ACC@1: 68.80% ACC@5: 90.10%: 100%|██████████| 79/79 [00:12<00:00,  6.26it/s]\n","\n","Test Epoch: [41/100] lr: 0.0010 Loss: 0.7938 ACC@1: 74.72% ACC@5: 94.37%: 100%|██████████| 79/79 [00:05<00:00, 15.06it/s]\n","\n","Train Epoch: [42/100] lr: 0.0010 Loss: 0.8912 ACC@1: 68.49% ACC@5: 90.15%: 100%|██████████| 79/79 [00:12<00:00,  6.31it/s]\n","\n","Test Epoch: [42/100] lr: 0.0010 Loss: 1.0773 ACC@1: 69.73% ACC@5: 90.57%: 100%|██████████| 79/79 [00:05<00:00, 15.46it/s]\n","\n","Train Epoch: [43/100] lr: 0.0010 Loss: 0.8789 ACC@1: 68.59% ACC@5: 90.99%: 100%|██████████| 79/79 [00:12<00:00,  6.25it/s]\n","\n","Test Epoch: [43/100] lr: 0.0010 Loss: 1.0692 ACC@1: 70.06% ACC@5: 91.63%: 100%|██████████| 79/79 [00:05<00:00, 15.33it/s]\n","\n","Train Epoch: [44/100] lr: 0.0010 Loss: 0.8675 ACC@1: 69.77% ACC@5: 91.05%: 100%|██████████| 79/79 [00:12<00:00,  6.33it/s]\n","\n","Test Epoch: [44/100] lr: 0.0010 Loss: 0.8763 ACC@1: 73.27% ACC@5: 93.71%: 100%|██████████| 79/79 [00:05<00:00, 15.34it/s]\n","\n","Train Epoch: [45/100] lr: 0.0010 Loss: 0.8493 ACC@1: 70.24% ACC@5: 91.16%: 100%|██████████| 79/79 [00:12<00:00,  6.32it/s]\n","\n","Test Epoch: [45/100] lr: 0.0010 Loss: 0.8600 ACC@1: 74.24% ACC@5: 94.50%: 100%|██████████| 79/79 [00:05<00:00, 15.64it/s]\n","\n","Train Epoch: [46/100] lr: 0.0010 Loss: 0.8537 ACC@1: 69.87% ACC@5: 91.09%: 100%|██████████| 79/79 [00:12<00:00,  6.23it/s]\n","\n","Test Epoch: [46/100] lr: 0.0010 Loss: 0.7507 ACC@1: 75.76% ACC@5: 94.63%: 100%|██████████| 79/79 [00:05<00:00, 15.37it/s]\n","\n","Train Epoch: [47/100] lr: 0.0010 Loss: 0.8517 ACC@1: 70.27% ACC@5: 91.02%: 100%|██████████| 79/79 [00:12<00:00,  6.25it/s]\n","\n","Test Epoch: [47/100] lr: 0.0010 Loss: 0.8453 ACC@1: 74.58% ACC@5: 93.56%: 100%|██████████| 79/79 [00:05<00:00, 14.97it/s]\n","\n","Train Epoch: [48/100] lr: 0.0010 Loss: 0.8514 ACC@1: 69.85% ACC@5: 90.93%: 100%|██████████| 79/79 [00:13<00:00,  5.88it/s]\n","\n","Test Epoch: [48/100] lr: 0.0010 Loss: 0.7998 ACC@1: 75.55% ACC@5: 93.83%: 100%|██████████| 79/79 [00:05<00:00, 15.61it/s]\n","\n","Train Epoch: [49/100] lr: 0.0010 Loss: 0.8509 ACC@1: 69.62% ACC@5: 91.09%: 100%|██████████| 79/79 [00:12<00:00,  6.24it/s]\n","\n","Test Epoch: [49/100] lr: 0.0010 Loss: 0.7244 ACC@1: 76.71% ACC@5: 95.08%: 100%|██████████| 79/79 [00:05<00:00, 15.23it/s]\n","\n","Train Epoch: [50/100] lr: 0.0010 Loss: 0.8211 ACC@1: 71.25% ACC@5: 91.47%: 100%|██████████| 79/79 [00:12<00:00,  6.16it/s]\n","\n","Test Epoch: [50/100] lr: 0.0010 Loss: 0.7193 ACC@1: 77.89% ACC@5: 95.33%: 100%|██████████| 79/79 [00:05<00:00, 15.07it/s]\n","\n","Train Epoch: [51/100] lr: 0.0010 Loss: 0.8043 ACC@1: 71.64% ACC@5: 91.69%: 100%|██████████| 79/79 [00:12<00:00,  6.22it/s]\n","\n","Test Epoch: [51/100] lr: 0.0010 Loss: 0.7331 ACC@1: 77.22% ACC@5: 95.04%: 100%|██████████| 79/79 [00:05<00:00, 15.23it/s]\n","\n","Train Epoch: [52/100] lr: 0.0010 Loss: 0.8111 ACC@1: 71.26% ACC@5: 91.49%: 100%|██████████| 79/79 [00:12<00:00,  6.28it/s]\n","\n","Test Epoch: [52/100] lr: 0.0010 Loss: 0.9191 ACC@1: 73.75% ACC@5: 93.71%: 100%|██████████| 79/79 [00:05<00:00, 15.31it/s]\n","\n","Train Epoch: [53/100] lr: 0.0010 Loss: 0.8002 ACC@1: 71.88% ACC@5: 91.72%: 100%|██████████| 79/79 [00:12<00:00,  6.23it/s]\n","\n","Test Epoch: [53/100] lr: 0.0010 Loss: 0.8676 ACC@1: 74.31% ACC@5: 94.19%: 100%|██████████| 79/79 [00:05<00:00, 15.24it/s]\n","\n","Train Epoch: [54/100] lr: 0.0010 Loss: 0.8017 ACC@1: 71.69% ACC@5: 91.58%: 100%|██████████| 79/79 [00:12<00:00,  6.27it/s]\n","\n","Test Epoch: [54/100] lr: 0.0010 Loss: 0.7714 ACC@1: 76.62% ACC@5: 93.93%: 100%|██████████| 79/79 [00:05<00:00, 15.29it/s]\n","\n","Train Epoch: [55/100] lr: 0.0010 Loss: 0.7745 ACC@1: 72.90% ACC@5: 92.01%: 100%|██████████| 79/79 [00:12<00:00,  6.21it/s]\n","\n","Test Epoch: [55/100] lr: 0.0010 Loss: 0.7903 ACC@1: 76.20% ACC@5: 93.86%: 100%|██████████| 79/79 [00:05<00:00, 15.31it/s]\n","\n","Train Epoch: [56/100] lr: 0.0010 Loss: 0.7887 ACC@1: 72.10% ACC@5: 91.98%: 100%|██████████| 79/79 [00:12<00:00,  6.21it/s]\n","\n","Test Epoch: [56/100] lr: 0.0010 Loss: 0.8829 ACC@1: 73.53% ACC@5: 93.55%: 100%|██████████| 79/79 [00:05<00:00, 14.37it/s]\n","\n","Train Epoch: [57/100] lr: 0.0010 Loss: 0.7930 ACC@1: 72.02% ACC@5: 91.85%: 100%|██████████| 79/79 [00:13<00:00,  5.88it/s]\n","\n","Test Epoch: [57/100] lr: 0.0010 Loss: 0.8152 ACC@1: 76.11% ACC@5: 94.06%: 100%|██████████| 79/79 [00:05<00:00, 15.28it/s]\n","\n","Train Epoch: [58/100] lr: 0.0010 Loss: 0.7855 ACC@1: 72.61% ACC@5: 91.60%: 100%|██████████| 79/79 [00:12<00:00,  6.22it/s]\n","\n","Test Epoch: [58/100] lr: 0.0010 Loss: 0.7116 ACC@1: 78.01% ACC@5: 94.63%: 100%|██████████| 79/79 [00:05<00:00, 15.10it/s]\n","\n","Train Epoch: [59/100] lr: 0.0010 Loss: 0.7676 ACC@1: 73.28% ACC@5: 92.22%: 100%|██████████| 79/79 [00:12<00:00,  6.15it/s]\n","\n","Test Epoch: [59/100] lr: 0.0010 Loss: 0.7584 ACC@1: 76.71% ACC@5: 94.85%: 100%|██████████| 79/79 [00:05<00:00, 15.15it/s]\n","\n","Train Epoch: [60/100] lr: 0.0010 Loss: 0.7742 ACC@1: 72.92% ACC@5: 92.29%: 100%|██████████| 79/79 [00:12<00:00,  6.22it/s]\n","\n","Test Epoch: [60/100] lr: 0.0001 Loss: 0.7456 ACC@1: 77.36% ACC@5: 94.65%: 100%|██████████| 79/79 [00:05<00:00, 15.44it/s]\n","\n","Train Epoch: [61/100] lr: 0.0001 Loss: 0.6678 ACC@1: 76.58% ACC@5: 93.61%: 100%|██████████| 79/79 [00:12<00:00,  6.29it/s]\n","\n","Test Epoch: [61/100] lr: 0.0001 Loss: 0.5939 ACC@1: 81.50% ACC@5: 96.09%: 100%|██████████| 79/79 [00:05<00:00, 15.31it/s]\n","\n","Train Epoch: [62/100] lr: 0.0001 Loss: 0.6479 ACC@1: 77.48% ACC@5: 93.44%: 100%|██████████| 79/79 [00:12<00:00,  6.27it/s]\n","\n","Test Epoch: [62/100] lr: 0.0001 Loss: 0.5886 ACC@1: 81.79% ACC@5: 96.08%: 100%|██████████| 79/79 [00:05<00:00, 15.24it/s]\n","\n","Train Epoch: [63/100] lr: 0.0001 Loss: 0.6388 ACC@1: 77.93% ACC@5: 93.69%: 100%|██████████| 79/79 [00:12<00:00,  6.24it/s]\n","\n","Test Epoch: [63/100] lr: 0.0001 Loss: 0.6001 ACC@1: 81.44% ACC@5: 96.14%: 100%|██████████| 79/79 [00:05<00:00, 15.30it/s]\n","\n","Train Epoch: [64/100] lr: 0.0001 Loss: 0.6308 ACC@1: 78.05% ACC@5: 93.88%: 100%|██████████| 79/79 [00:12<00:00,  6.20it/s]\n","\n","Test Epoch: [64/100] lr: 0.0001 Loss: 0.5870 ACC@1: 81.98% ACC@5: 96.18%: 100%|██████████| 79/79 [00:05<00:00, 15.35it/s]\n","\n","Train Epoch: [65/100] lr: 0.0001 Loss: 0.6156 ACC@1: 78.29% ACC@5: 94.29%: 100%|██████████| 79/79 [00:12<00:00,  6.24it/s]\n","\n","Test Epoch: [65/100] lr: 0.0001 Loss: 0.5888 ACC@1: 82.31% ACC@5: 96.21%: 100%|██████████| 79/79 [00:05<00:00, 14.59it/s]\n","\n","Train Epoch: [66/100] lr: 0.0001 Loss: 0.6014 ACC@1: 78.88% ACC@5: 94.13%: 100%|██████████| 79/79 [00:13<00:00,  5.82it/s]\n","\n","Test Epoch: [66/100] lr: 0.0001 Loss: 0.6021 ACC@1: 81.87% ACC@5: 96.21%: 100%|██████████| 79/79 [00:05<00:00, 15.16it/s]\n","\n","Train Epoch: [67/100] lr: 0.0001 Loss: 0.6224 ACC@1: 78.15% ACC@5: 93.66%: 100%|██████████| 79/79 [00:12<00:00,  6.20it/s]\n","\n","Test Epoch: [67/100] lr: 0.0001 Loss: 0.6096 ACC@1: 82.03% ACC@5: 96.17%: 100%|██████████| 79/79 [00:05<00:00, 15.05it/s]\n","\n","Train Epoch: [68/100] lr: 0.0001 Loss: 0.5994 ACC@1: 79.32% ACC@5: 94.50%: 100%|██████████| 79/79 [00:12<00:00,  6.17it/s]\n","\n","Test Epoch: [68/100] lr: 0.0001 Loss: 0.5897 ACC@1: 82.32% ACC@5: 96.16%: 100%|██████████| 79/79 [00:05<00:00, 15.10it/s]\n","\n","Train Epoch: [69/100] lr: 0.0001 Loss: 0.6027 ACC@1: 79.30% ACC@5: 94.30%: 100%|██████████| 79/79 [00:12<00:00,  6.21it/s]\n","\n","Test Epoch: [69/100] lr: 0.0001 Loss: 0.5943 ACC@1: 82.22% ACC@5: 96.22%: 100%|██████████| 79/79 [00:05<00:00, 15.08it/s]\n","\n","Train Epoch: [70/100] lr: 0.0001 Loss: 0.5969 ACC@1: 79.06% ACC@5: 94.12%: 100%|██████████| 79/79 [00:12<00:00,  6.15it/s]\n","\n","Test Epoch: [70/100] lr: 0.0001 Loss: 0.6100 ACC@1: 82.05% ACC@5: 96.20%: 100%|██████████| 79/79 [00:05<00:00, 15.00it/s]\n","\n","Train Epoch: [71/100] lr: 0.0001 Loss: 0.5931 ACC@1: 79.62% ACC@5: 94.57%: 100%|██████████| 79/79 [00:12<00:00,  6.20it/s]\n","\n","Test Epoch: [71/100] lr: 0.0001 Loss: 0.6047 ACC@1: 82.38% ACC@5: 96.29%: 100%|██████████| 79/79 [00:05<00:00, 14.83it/s]\n","\n","Train Epoch: [72/100] lr: 0.0001 Loss: 0.5770 ACC@1: 79.66% ACC@5: 94.70%: 100%|██████████| 79/79 [00:12<00:00,  6.17it/s]\n","\n","Test Epoch: [72/100] lr: 0.0001 Loss: 0.6180 ACC@1: 82.25% ACC@5: 96.15%: 100%|██████████| 79/79 [00:05<00:00, 14.93it/s]\n","\n","Train Epoch: [73/100] lr: 0.0001 Loss: 0.5791 ACC@1: 79.29% ACC@5: 94.67%: 100%|██████████| 79/79 [00:12<00:00,  6.14it/s]\n","\n","Test Epoch: [73/100] lr: 0.0001 Loss: 0.6022 ACC@1: 82.56% ACC@5: 96.30%: 100%|██████████| 79/79 [00:05<00:00, 14.68it/s]\n","\n","Train Epoch: [74/100] lr: 0.0001 Loss: 0.5941 ACC@1: 79.24% ACC@5: 94.52%: 100%|██████████| 79/79 [00:12<00:00,  6.26it/s]\n","\n","Test Epoch: [74/100] lr: 0.0001 Loss: 0.6070 ACC@1: 82.27% ACC@5: 96.31%: 100%|██████████| 79/79 [00:05<00:00, 14.57it/s]\n","\n","Train Epoch: [75/100] lr: 0.0001 Loss: 0.5709 ACC@1: 80.27% ACC@5: 94.80%: 100%|██████████| 79/79 [00:13<00:00,  5.85it/s]\n","\n","Test Epoch: [75/100] lr: 0.0001 Loss: 0.6124 ACC@1: 82.18% ACC@5: 96.14%: 100%|██████████| 79/79 [00:05<00:00, 15.11it/s]\n","\n","Train Epoch: [76/100] lr: 0.0001 Loss: 0.5809 ACC@1: 79.62% ACC@5: 94.53%: 100%|██████████| 79/79 [00:12<00:00,  6.17it/s]\n","\n","Test Epoch: [76/100] lr: 0.0001 Loss: 0.6336 ACC@1: 81.90% ACC@5: 96.15%: 100%|██████████| 79/79 [00:05<00:00, 14.95it/s]\n","\n","Train Epoch: [77/100] lr: 0.0001 Loss: 0.5670 ACC@1: 80.39% ACC@5: 94.60%: 100%|██████████| 79/79 [00:12<00:00,  6.21it/s]\n","\n","Test Epoch: [77/100] lr: 0.0001 Loss: 0.6373 ACC@1: 82.22% ACC@5: 96.16%: 100%|██████████| 79/79 [00:06<00:00, 12.51it/s]\n","\n","Train Epoch: [78/100] lr: 0.0001 Loss: 0.5571 ACC@1: 80.43% ACC@5: 94.79%: 100%|██████████| 79/79 [00:12<00:00,  6.23it/s]\n","\n","Test Epoch: [78/100] lr: 0.0001 Loss: 0.6298 ACC@1: 82.22% ACC@5: 96.25%: 100%|██████████| 79/79 [00:05<00:00, 15.10it/s]\n","\n","Train Epoch: [79/100] lr: 0.0001 Loss: 0.5536 ACC@1: 80.59% ACC@5: 94.90%: 100%|██████████| 79/79 [00:12<00:00,  6.24it/s]\n","\n","Test Epoch: [79/100] lr: 0.0001 Loss: 0.6317 ACC@1: 82.16% ACC@5: 96.29%: 100%|██████████| 79/79 [00:05<00:00, 15.24it/s]\n","\n","Train Epoch: [80/100] lr: 0.0001 Loss: 0.5775 ACC@1: 79.66% ACC@5: 94.73%: 100%|██████████| 79/79 [00:12<00:00,  6.21it/s]\n","\n","Test Epoch: [80/100] lr: 0.0000 Loss: 0.6204 ACC@1: 82.49% ACC@5: 96.27%: 100%|██████████| 79/79 [00:05<00:00, 15.33it/s]\n","\n","Train Epoch: [81/100] lr: 0.0000 Loss: 0.5558 ACC@1: 80.24% ACC@5: 94.90%: 100%|██████████| 79/79 [00:12<00:00,  6.27it/s]\n","\n","Test Epoch: [81/100] lr: 0.0000 Loss: 0.6126 ACC@1: 82.90% ACC@5: 96.35%: 100%|██████████| 79/79 [00:05<00:00, 14.97it/s]\n","\n","Train Epoch: [82/100] lr: 0.0000 Loss: 0.5390 ACC@1: 81.18% ACC@5: 95.21%: 100%|██████████| 79/79 [00:12<00:00,  6.19it/s]\n","\n","Test Epoch: [82/100] lr: 0.0000 Loss: 0.6205 ACC@1: 82.77% ACC@5: 96.29%: 100%|██████████| 79/79 [00:05<00:00, 14.78it/s]\n","\n","Train Epoch: [83/100] lr: 0.0000 Loss: 0.5472 ACC@1: 81.00% ACC@5: 94.89%: 100%|██████████| 79/79 [00:12<00:00,  6.11it/s]\n","\n","Test Epoch: [83/100] lr: 0.0000 Loss: 0.6070 ACC@1: 82.88% ACC@5: 96.47%: 100%|██████████| 79/79 [00:05<00:00, 13.19it/s]\n","\n","Train Epoch: [84/100] lr: 0.0000 Loss: 0.5412 ACC@1: 81.25% ACC@5: 95.09%: 100%|██████████| 79/79 [00:12<00:00,  6.15it/s]\n","\n","Test Epoch: [84/100] lr: 0.0000 Loss: 0.6137 ACC@1: 82.76% ACC@5: 96.44%: 100%|██████████| 79/79 [00:05<00:00, 14.95it/s]\n","\n","Train Epoch: [85/100] lr: 0.0000 Loss: 0.5388 ACC@1: 81.31% ACC@5: 94.94%: 100%|██████████| 79/79 [00:12<00:00,  6.23it/s]\n","\n","Test Epoch: [85/100] lr: 0.0000 Loss: 0.6172 ACC@1: 82.73% ACC@5: 96.48%: 100%|██████████| 79/79 [00:05<00:00, 15.00it/s]\n","\n","Train Epoch: [86/100] lr: 0.0000 Loss: 0.5565 ACC@1: 80.48% ACC@5: 94.59%: 100%|██████████| 79/79 [00:12<00:00,  6.20it/s]\n","\n","Test Epoch: [86/100] lr: 0.0000 Loss: 0.6118 ACC@1: 82.63% ACC@5: 96.46%: 100%|██████████| 79/79 [00:05<00:00, 14.99it/s]\n","\n","Train Epoch: [87/100] lr: 0.0000 Loss: 0.5613 ACC@1: 80.41% ACC@5: 94.37%: 100%|██████████| 79/79 [00:13<00:00,  6.07it/s]\n","\n","Test Epoch: [87/100] lr: 0.0000 Loss: 0.6206 ACC@1: 82.69% ACC@5: 96.33%: 100%|██████████| 79/79 [00:05<00:00, 15.06it/s]\n","\n","Train Epoch: [88/100] lr: 0.0000 Loss: 0.5502 ACC@1: 80.84% ACC@5: 94.62%: 100%|██████████| 79/79 [00:12<00:00,  6.20it/s]\n","\n","Test Epoch: [88/100] lr: 0.0000 Loss: 0.6172 ACC@1: 82.76% ACC@5: 96.42%: 100%|██████████| 79/79 [00:05<00:00, 15.05it/s]\n","\n","Train Epoch: [89/100] lr: 0.0000 Loss: 0.5567 ACC@1: 80.33% ACC@5: 94.86%: 100%|██████████| 79/79 [00:12<00:00,  6.18it/s]\n","\n","Test Epoch: [89/100] lr: 0.0000 Loss: 0.6110 ACC@1: 82.84% ACC@5: 96.43%: 100%|██████████| 79/79 [00:05<00:00, 15.00it/s]\n","\n","Train Epoch: [90/100] lr: 0.0000 Loss: 0.5381 ACC@1: 81.13% ACC@5: 95.41%: 100%|██████████| 79/79 [00:12<00:00,  6.19it/s]\n","\n","Test Epoch: [90/100] lr: 0.0000 Loss: 0.6045 ACC@1: 82.66% ACC@5: 96.47%: 100%|██████████| 79/79 [00:05<00:00, 14.89it/s]\n","\n","Train Epoch: [91/100] lr: 0.0000 Loss: 0.5330 ACC@1: 81.35% ACC@5: 95.20%: 100%|██████████| 79/79 [00:12<00:00,  6.14it/s]\n","\n","Test Epoch: [91/100] lr: 0.0000 Loss: 0.6207 ACC@1: 82.70% ACC@5: 96.26%: 100%|██████████| 79/79 [00:05<00:00, 15.04it/s]\n","\n","Train Epoch: [92/100] lr: 0.0000 Loss: 0.5516 ACC@1: 80.80% ACC@5: 94.86%: 100%|██████████| 79/79 [00:12<00:00,  6.22it/s]\n","\n","Test Epoch: [92/100] lr: 0.0000 Loss: 0.6318 ACC@1: 82.77% ACC@5: 96.33%: 100%|██████████| 79/79 [00:05<00:00, 14.05it/s]\n","\n","Train Epoch: [93/100] lr: 0.0000 Loss: 0.5305 ACC@1: 81.45% ACC@5: 95.02%: 100%|██████████| 79/79 [00:13<00:00,  5.93it/s]\n","\n","Test Epoch: [93/100] lr: 0.0000 Loss: 0.6121 ACC@1: 82.74% ACC@5: 96.46%: 100%|██████████| 79/79 [00:05<00:00, 15.09it/s]\n","\n","Train Epoch: [94/100] lr: 0.0000 Loss: 0.5386 ACC@1: 81.39% ACC@5: 95.07%: 100%|██████████| 79/79 [00:12<00:00,  6.24it/s]\n","\n","Test Epoch: [94/100] lr: 0.0000 Loss: 0.6129 ACC@1: 83.02% ACC@5: 96.43%: 100%|██████████| 79/79 [00:05<00:00, 15.18it/s]\n","\n","Train Epoch: [95/100] lr: 0.0000 Loss: 0.5435 ACC@1: 80.84% ACC@5: 94.98%: 100%|██████████| 79/79 [00:12<00:00,  6.18it/s]\n","\n","Test Epoch: [95/100] lr: 0.0000 Loss: 0.6204 ACC@1: 82.81% ACC@5: 96.42%: 100%|██████████| 79/79 [00:05<00:00, 15.07it/s]\n","\n","Train Epoch: [96/100] lr: 0.0000 Loss: 0.5457 ACC@1: 80.51% ACC@5: 95.05%: 100%|██████████| 79/79 [00:12<00:00,  6.16it/s]\n","\n","Test Epoch: [96/100] lr: 0.0000 Loss: 0.6178 ACC@1: 82.92% ACC@5: 96.35%: 100%|██████████| 79/79 [00:05<00:00, 14.81it/s]\n","\n","Train Epoch: [97/100] lr: 0.0000 Loss: 0.5446 ACC@1: 80.95% ACC@5: 94.94%: 100%|██████████| 79/79 [00:12<00:00,  6.20it/s]\n","\n","Test Epoch: [97/100] lr: 0.0000 Loss: 0.6091 ACC@1: 82.85% ACC@5: 96.51%: 100%|██████████| 79/79 [00:05<00:00, 15.22it/s]\n","\n","Train Epoch: [98/100] lr: 0.0000 Loss: 0.5337 ACC@1: 81.34% ACC@5: 95.17%: 100%|██████████| 79/79 [00:12<00:00,  6.22it/s]\n","\n","Test Epoch: [98/100] lr: 0.0000 Loss: 0.6101 ACC@1: 82.76% ACC@5: 96.48%: 100%|██████████| 79/79 [00:05<00:00, 14.69it/s]\n","\n","Train Epoch: [99/100] lr: 0.0000 Loss: 0.5378 ACC@1: 81.29% ACC@5: 95.04%: 100%|██████████| 79/79 [00:12<00:00,  6.21it/s]\n","\n","Test Epoch: [99/100] lr: 0.0000 Loss: 0.6129 ACC@1: 82.73% ACC@5: 96.49%: 100%|██████████| 79/79 [00:05<00:00, 15.02it/s]\n","\n","Train Epoch: [100/100] lr: 0.0000 Loss: 0.5267 ACC@1: 82.22% ACC@5: 95.13%: 100%|██████████| 79/79 [00:12<00:00,  6.19it/s]\n","\n","Test Epoch: [100/100] lr: 0.0000 Loss: 0.6068 ACC@1: 82.91% ACC@5: 96.54%: 100%|██████████| 79/79 [00:05<00:00, 14.89it/s]"]},{"output_type":"stream","name":"stdout","text":["[82.22]\n","[82.91]\n","82.22\n","82.91\n","0.0\n","0.0\n","[82.22]\n","[82.91]\n","82.22\n","82.91\n","0.0\n","0.0\n"]},{"output_type":"stream","name":"stderr","text":["\n","\n"]}]},{"cell_type":"code","source":["  class ModelBase(nn.Module):\n","    \"\"\"\n","    Common CIFAR ResNet recipe.\n","    Comparing with ImageNet ResNet recipe, it:\n","    (i) replaces conv1 with kernel=3, str=1\n","    (ii) removes pool1\n","    \"\"\"\n","    def __init__(self, feature_dim=10, arch='resnet18', bn_splits=16):\n","        super(ModelBase, self).__init__()\n","\n","        # use split batchnorm\n","        norm_layer = nn.BatchNorm2d\n","        # get specified resnet model\n","        resnet_arch = getattr(resnet, arch)\n","        net = resnet_arch(num_classes=feature_dim, norm_layer=norm_layer)\n","\n","        # make changes to original resnet\n","        self.net = []\n","        for name, module in net.named_children():\n","            if name == 'conv1':\n","                module = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n","            if isinstance(module, nn.MaxPool2d):\n","                continue\n","            if isinstance(module, nn.Linear):\n","                self.net.append(nn.Flatten(1))\n","            self.net.append(module)\n","\n","        # build net\n","        self.net = nn.Sequential(*self.net)\n","\n","    def forward(self, x):\n","        x = self.net(x)\n","        x = torch.flatten(x,start_dim=1)\n","        # note: not normalized here\n","        return x\n","class classifier(nn.Module):\n","\n","    def __init__(self, feature_dim=10, arch='resnet18', bn_splits=16):\n","        super(classifier, self).__init__()\n","\n","        self.net = nn.Linear(1024,10)\n","\n","    def forward(self, x):\n","        x = self.net(x)\n","        # note: not normalized here\n","        return x\n","supervised = ModelBase()\n","supervised.load_state_dict(torch.load(\"/content/drive/My Drive/Models/Supervised/supervisedresnet18v1.1.pth\"))\n","cutout = ModelBase()\n","cutout.load_state_dict(torch.load(\"/content/drive/My Drive/Models/Supervised/supervisedresnet18v1.1seed1.pth\"))\n","supervised.net =supervised.net[:-1]\n","cutout.net =cutout.net[:-1]\n","for i in supervised.parameters():\n","  i.requires_grad = False\n","for i in cutout.parameters():\n","  i.requires_grad = False\n","supervised=supervised.cuda()\n","cutout = cutout.cuda()\n","finallayer = classifier()"],"metadata":{"id":"xM_M25-DdS-w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"u0qCZbOvdTrt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","\n","def train_val(base1,base2,final, data_loader, train_optimizer):\n","    global lr\n","    schedule = [60, 80]\n","    criterion = nn.CrossEntropyLoss().cuda()\n","    is_train = train_optimizer is not None\n","    final.train() if is_train else final.eval()\n","    sa = 0\n","    total_loss, total_correct_1, total_correct_3, total_num, data_bar = 0.0, 0.0, 0.0, 0, tqdm(data_loader, position=0, leave=True)\n","    with (torch.enable_grad() if is_train else torch.no_grad()):\n","        for data, target in data_bar:\n","            data = data.cuda()\n","            output1 = base1(data)\n","            output2 = base2(data)\n","            #print(output1.shape,output2.shape)\n","            data = torch.cat((output1,output2),1)\n","            #print(data.shape)\n","\n","            data, target = data.cuda(non_blocking=True), target.cuda(non_blocking=True)\n","            out = final(data)\n","            loss = criterion(out, target)\n","\n","            if is_train:\n","                train_optimizer.zero_grad()\n","                loss.backward()\n","                train_optimizer.step()\n","\n","\n","            total_num += data.size(0)\n","            total_loss += loss.item() * data.size(0)\n","            prediction = torch.argsort(out, dim=-1, descending=True)\n","            total_correct_1 += torch.sum((prediction[:, 0:1] == target.unsqueeze(dim=-1)).any(dim=-1).float()).item()\n","            total_correct_3 += torch.sum((prediction[:, 0:3] == target.unsqueeze(dim=-1)).any(dim=-1).float()).item()\n","            \n","            data_bar.set_description('{} Epoch: [{}/{}] lr: {:.4f} Loss: {:.4f} ACC@1: {:.2f}% ACC@5: {:.2f}%'\n","                                     .format('Train' if is_train else 'Test', epoch, epochs, lr, total_loss / total_num,\n","                                             total_correct_1 / total_num * 100, total_correct_3 / total_num * 100))\n","        if is_train:  \n","          if schedule is not None:\n","            for milestone in schedule:\n","                lr *= 0.1 if epoch == milestone else 1.\n","            for param_group in train_optimizer.param_groups:\n","                param_group['lr'] = lr\n","\n","        return total_correct_1/total_num * 100\n","\n","train_acc = []\n","test_acc = []\n","for i in range(1):\n","  print(f\"Random Seed: {i}\")\n","  best_train = 0\n","  best_test = 0\n","  np.random.seed(i)\n","\n","  indices = get_indices(train_data, 1000)\n","  sampler = torch.utils.data.SubsetRandomSampler(indices)\n","\n","  train_loader = DataLoader(train_data, batch_size=batch_size, sampler = sampler , num_workers=16, pin_memory=True)\n","  finallayer = classifier()\n","  finallayer=finallayer.cuda()\n","  lr = 1e-4\n","  optimizer = torch.optim.Adam(finallayer.parameters(), lr=lr, weight_decay = 1e-5)\n","  epoch_start = 1\n","  epochs = 100\n","  for epoch in range(epoch_start, epochs+1):\n","    acc1 = train_val(supervised,cutout,finallayer, train_loader, optimizer)\n","    acc2 = train_val(supervised,cutout,finallayer, test_loader, None)\n","    if acc1 > best_train:\n","      best_train = acc1\n","    if acc2 > best_test:\n","      best_test = acc2\n","  train_acc.append(acc1)\n","  test_acc.append(acc2)\n","  \n","  torch.save(finallayer.state_dict(), \"/content/drive/My Drive/Models/Supervised/finallayerconcat1024.pth\")\n","print(train_acc)\n","print(test_acc)\n","top10 = np.sort(train_acc)[::-1]\n","print(\"top10 trainacc\",np.mean(top10[:10]))\n","top10 = np.sort(test_acc)[::-1]\n","print(\"top10 testacc\",np.mean(top10[:10]))\n","print(\"best test\",np.sort(test_acc)[::-1][0])\n","train_acc = np.array(train_acc)\n","test_acc = np.array(test_acc)\n","print(np.mean(train_acc))\n","print(np.mean(test_acc))\n","print(np.std(train_acc))\n","print(np.std(test_acc))\n"],"metadata":{"id":"vbkq9JtH757B","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669021546927,"user_tz":-480,"elapsed":1565968,"user":{"displayName":"Souw Chuan Neo","userId":"08556196228836469917"}},"outputId":"08a8d49c-1581-4cf9-83d6-45871fd0355f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Random Seed: 0\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch: [1/100] lr: 0.0001 Loss: 1.5442 ACC@1: 58.76% ACC@5: 82.39%: 100%|██████████| 79/79 [00:07<00:00, 10.06it/s]\n","Test Epoch: [1/100] lr: 0.0001 Loss: 0.9724 ACC@1: 82.16% ACC@5: 96.20%: 100%|██████████| 79/79 [00:06<00:00, 11.42it/s]\n","Train Epoch: [2/100] lr: 0.0001 Loss: 0.8338 ACC@1: 84.46% ACC@5: 96.56%: 100%|██████████| 79/79 [00:07<00:00, 10.16it/s]\n","Test Epoch: [2/100] lr: 0.0001 Loss: 0.6760 ACC@1: 83.83% ACC@5: 96.69%: 100%|██████████| 79/79 [00:06<00:00, 11.30it/s]\n","Train Epoch: [3/100] lr: 0.0001 Loss: 0.6555 ACC@1: 85.13% ACC@5: 96.65%: 100%|██████████| 79/79 [00:08<00:00,  9.49it/s]\n","Test Epoch: [3/100] lr: 0.0001 Loss: 0.5749 ACC@1: 84.15% ACC@5: 96.94%: 100%|██████████| 79/79 [00:07<00:00, 10.15it/s]\n","Train Epoch: [4/100] lr: 0.0001 Loss: 0.5747 ACC@1: 85.34% ACC@5: 96.60%: 100%|██████████| 79/79 [00:07<00:00,  9.88it/s]\n","Test Epoch: [4/100] lr: 0.0001 Loss: 0.5265 ACC@1: 84.29% ACC@5: 97.02%: 100%|██████████| 79/79 [00:07<00:00, 11.24it/s]\n","Train Epoch: [5/100] lr: 0.0001 Loss: 0.5208 ACC@1: 86.41% ACC@5: 96.95%: 100%|██████████| 79/79 [00:07<00:00, 10.04it/s]\n","Test Epoch: [5/100] lr: 0.0001 Loss: 0.5006 ACC@1: 84.39% ACC@5: 96.98%: 100%|██████████| 79/79 [00:07<00:00, 11.17it/s]\n","Train Epoch: [6/100] lr: 0.0001 Loss: 0.4959 ACC@1: 86.21% ACC@5: 96.68%: 100%|██████████| 79/79 [00:07<00:00, 10.15it/s]\n","Test Epoch: [6/100] lr: 0.0001 Loss: 0.4861 ACC@1: 84.37% ACC@5: 97.09%: 100%|██████████| 79/79 [00:07<00:00, 11.09it/s]\n","Train Epoch: [7/100] lr: 0.0001 Loss: 0.4539 ACC@1: 86.80% ACC@5: 96.87%: 100%|██████████| 79/79 [00:08<00:00,  9.72it/s]\n","Test Epoch: [7/100] lr: 0.0001 Loss: 0.4793 ACC@1: 84.38% ACC@5: 97.02%: 100%|██████████| 79/79 [00:08<00:00,  9.66it/s]\n","Train Epoch: [8/100] lr: 0.0001 Loss: 0.4430 ACC@1: 87.02% ACC@5: 96.86%: 100%|██████████| 79/79 [00:07<00:00, 10.01it/s]\n","Test Epoch: [8/100] lr: 0.0001 Loss: 0.4714 ACC@1: 84.35% ACC@5: 97.02%: 100%|██████████| 79/79 [00:07<00:00, 11.10it/s]\n","Train Epoch: [9/100] lr: 0.0001 Loss: 0.4265 ACC@1: 87.00% ACC@5: 96.94%: 100%|██████████| 79/79 [00:07<00:00,  9.91it/s]\n","Test Epoch: [9/100] lr: 0.0001 Loss: 0.4715 ACC@1: 84.42% ACC@5: 97.01%: 100%|██████████| 79/79 [00:07<00:00, 11.08it/s]\n","Train Epoch: [10/100] lr: 0.0001 Loss: 0.4255 ACC@1: 86.55% ACC@5: 96.78%: 100%|██████████| 79/79 [00:07<00:00,  9.96it/s]\n","Test Epoch: [10/100] lr: 0.0001 Loss: 0.4689 ACC@1: 84.40% ACC@5: 97.01%: 100%|██████████| 79/79 [00:07<00:00, 10.99it/s]\n","Train Epoch: [11/100] lr: 0.0001 Loss: 0.4179 ACC@1: 86.47% ACC@5: 97.04%: 100%|██████████| 79/79 [00:08<00:00,  9.41it/s]\n","Test Epoch: [11/100] lr: 0.0001 Loss: 0.4706 ACC@1: 84.34% ACC@5: 96.94%: 100%|██████████| 79/79 [00:07<00:00,  9.89it/s]\n","Train Epoch: [12/100] lr: 0.0001 Loss: 0.4066 ACC@1: 87.10% ACC@5: 97.00%: 100%|██████████| 79/79 [00:07<00:00,  9.99it/s]\n","Test Epoch: [12/100] lr: 0.0001 Loss: 0.4702 ACC@1: 84.40% ACC@5: 96.94%: 100%|██████████| 79/79 [00:07<00:00, 11.03it/s]\n","Train Epoch: [13/100] lr: 0.0001 Loss: 0.3943 ACC@1: 87.20% ACC@5: 97.06%: 100%|██████████| 79/79 [00:08<00:00,  9.86it/s]\n","Test Epoch: [13/100] lr: 0.0001 Loss: 0.4712 ACC@1: 84.41% ACC@5: 96.99%: 100%|██████████| 79/79 [00:07<00:00, 11.06it/s]\n","Train Epoch: [14/100] lr: 0.0001 Loss: 0.3961 ACC@1: 87.11% ACC@5: 96.96%: 100%|██████████| 79/79 [00:08<00:00,  9.86it/s]\n","Test Epoch: [14/100] lr: 0.0001 Loss: 0.4739 ACC@1: 84.38% ACC@5: 96.99%: 100%|██████████| 79/79 [00:07<00:00, 10.97it/s]\n","Train Epoch: [15/100] lr: 0.0001 Loss: 0.3881 ACC@1: 87.17% ACC@5: 96.98%: 100%|██████████| 79/79 [00:08<00:00,  9.53it/s]\n","Test Epoch: [15/100] lr: 0.0001 Loss: 0.4744 ACC@1: 84.35% ACC@5: 97.00%: 100%|██████████| 79/79 [00:07<00:00, 10.21it/s]\n","Train Epoch: [16/100] lr: 0.0001 Loss: 0.3992 ACC@1: 86.51% ACC@5: 96.86%: 100%|██████████| 79/79 [00:07<00:00,  9.89it/s]\n","Test Epoch: [16/100] lr: 0.0001 Loss: 0.4757 ACC@1: 84.46% ACC@5: 97.03%: 100%|██████████| 79/79 [00:07<00:00, 11.02it/s]\n","Train Epoch: [17/100] lr: 0.0001 Loss: 0.3819 ACC@1: 87.05% ACC@5: 97.52%: 100%|██████████| 79/79 [00:07<00:00,  9.90it/s]\n","Test Epoch: [17/100] lr: 0.0001 Loss: 0.4779 ACC@1: 84.37% ACC@5: 97.04%: 100%|██████████| 79/79 [00:07<00:00, 11.06it/s]\n","Train Epoch: [18/100] lr: 0.0001 Loss: 0.3810 ACC@1: 87.60% ACC@5: 96.98%: 100%|██████████| 79/79 [00:07<00:00,  9.90it/s]\n","Test Epoch: [18/100] lr: 0.0001 Loss: 0.4810 ACC@1: 84.35% ACC@5: 97.07%: 100%|██████████| 79/79 [00:07<00:00, 10.96it/s]\n","Train Epoch: [19/100] lr: 0.0001 Loss: 0.3749 ACC@1: 87.34% ACC@5: 97.11%: 100%|██████████| 79/79 [00:08<00:00,  9.35it/s]\n","Test Epoch: [19/100] lr: 0.0001 Loss: 0.4812 ACC@1: 84.45% ACC@5: 97.05%: 100%|██████████| 79/79 [00:07<00:00, 10.43it/s]\n","Train Epoch: [20/100] lr: 0.0001 Loss: 0.3780 ACC@1: 87.01% ACC@5: 96.96%: 100%|██████████| 79/79 [00:08<00:00,  9.80it/s]\n","Test Epoch: [20/100] lr: 0.0001 Loss: 0.4838 ACC@1: 84.41% ACC@5: 97.02%: 100%|██████████| 79/79 [00:07<00:00, 10.94it/s]\n","Train Epoch: [21/100] lr: 0.0001 Loss: 0.3737 ACC@1: 87.45% ACC@5: 96.95%: 100%|██████████| 79/79 [00:08<00:00,  9.87it/s]\n","Test Epoch: [21/100] lr: 0.0001 Loss: 0.4870 ACC@1: 84.41% ACC@5: 96.99%: 100%|██████████| 79/79 [00:07<00:00, 11.10it/s]\n","Train Epoch: [22/100] lr: 0.0001 Loss: 0.3616 ACC@1: 88.12% ACC@5: 97.11%: 100%|██████████| 79/79 [00:08<00:00,  9.87it/s]\n","Test Epoch: [22/100] lr: 0.0001 Loss: 0.4901 ACC@1: 84.26% ACC@5: 97.07%: 100%|██████████| 79/79 [00:07<00:00, 10.87it/s]\n","Train Epoch: [23/100] lr: 0.0001 Loss: 0.3690 ACC@1: 87.32% ACC@5: 96.94%: 100%|██████████| 79/79 [00:08<00:00,  9.51it/s]\n","Test Epoch: [23/100] lr: 0.0001 Loss: 0.4937 ACC@1: 84.26% ACC@5: 97.02%: 100%|██████████| 79/79 [00:07<00:00, 10.07it/s]\n","Train Epoch: [24/100] lr: 0.0001 Loss: 0.3565 ACC@1: 87.57% ACC@5: 97.38%: 100%|██████████| 79/79 [00:07<00:00,  9.95it/s]\n","Test Epoch: [24/100] lr: 0.0001 Loss: 0.4967 ACC@1: 84.19% ACC@5: 96.95%: 100%|██████████| 79/79 [00:07<00:00, 10.96it/s]\n","Train Epoch: [25/100] lr: 0.0001 Loss: 0.3622 ACC@1: 87.89% ACC@5: 97.07%: 100%|██████████| 79/79 [00:08<00:00,  9.81it/s]\n","Test Epoch: [25/100] lr: 0.0001 Loss: 0.4967 ACC@1: 84.27% ACC@5: 96.92%: 100%|██████████| 79/79 [00:07<00:00, 10.84it/s]\n","Train Epoch: [26/100] lr: 0.0001 Loss: 0.3578 ACC@1: 87.97% ACC@5: 97.15%: 100%|██████████| 79/79 [00:08<00:00,  9.83it/s]\n","Test Epoch: [26/100] lr: 0.0001 Loss: 0.5010 ACC@1: 84.27% ACC@5: 96.97%: 100%|██████████| 79/79 [00:07<00:00, 10.81it/s]\n","Train Epoch: [27/100] lr: 0.0001 Loss: 0.3534 ACC@1: 87.94% ACC@5: 97.10%: 100%|██████████| 79/79 [00:08<00:00,  9.35it/s]\n","Test Epoch: [27/100] lr: 0.0001 Loss: 0.5024 ACC@1: 84.09% ACC@5: 96.89%: 100%|██████████| 79/79 [00:07<00:00, 10.56it/s]\n","Train Epoch: [28/100] lr: 0.0001 Loss: 0.3534 ACC@1: 88.01% ACC@5: 96.86%: 100%|██████████| 79/79 [00:07<00:00,  9.89it/s]\n","Test Epoch: [28/100] lr: 0.0001 Loss: 0.5038 ACC@1: 84.09% ACC@5: 96.88%: 100%|██████████| 79/79 [00:07<00:00, 11.02it/s]\n","Train Epoch: [29/100] lr: 0.0001 Loss: 0.3494 ACC@1: 88.14% ACC@5: 96.90%: 100%|██████████| 79/79 [00:08<00:00,  9.87it/s]\n","Test Epoch: [29/100] lr: 0.0001 Loss: 0.5036 ACC@1: 84.22% ACC@5: 96.95%: 100%|██████████| 79/79 [00:07<00:00, 10.86it/s]\n","Train Epoch: [30/100] lr: 0.0001 Loss: 0.3662 ACC@1: 87.39% ACC@5: 96.96%: 100%|██████████| 79/79 [00:08<00:00,  9.63it/s]\n","Test Epoch: [30/100] lr: 0.0001 Loss: 0.5055 ACC@1: 84.04% ACC@5: 96.89%: 100%|██████████| 79/79 [00:07<00:00, 10.85it/s]\n","Train Epoch: [31/100] lr: 0.0001 Loss: 0.3597 ACC@1: 87.39% ACC@5: 97.04%: 100%|██████████| 79/79 [00:08<00:00,  9.34it/s]\n","Test Epoch: [31/100] lr: 0.0001 Loss: 0.5059 ACC@1: 84.09% ACC@5: 96.93%: 100%|██████████| 79/79 [00:07<00:00, 10.81it/s]\n","Train Epoch: [32/100] lr: 0.0001 Loss: 0.3446 ACC@1: 88.21% ACC@5: 97.31%: 100%|██████████| 79/79 [00:07<00:00,  9.89it/s]\n","Test Epoch: [32/100] lr: 0.0001 Loss: 0.5112 ACC@1: 84.05% ACC@5: 96.83%: 100%|██████████| 79/79 [00:07<00:00, 10.94it/s]\n","Train Epoch: [33/100] lr: 0.0001 Loss: 0.3560 ACC@1: 87.80% ACC@5: 97.25%: 100%|██████████| 79/79 [00:08<00:00,  9.82it/s]\n","Test Epoch: [33/100] lr: 0.0001 Loss: 0.5123 ACC@1: 84.04% ACC@5: 96.92%: 100%|██████████| 79/79 [00:07<00:00, 10.74it/s]\n","Train Epoch: [34/100] lr: 0.0001 Loss: 0.3506 ACC@1: 87.74% ACC@5: 97.13%: 100%|██████████| 79/79 [00:08<00:00,  9.81it/s]\n","Test Epoch: [34/100] lr: 0.0001 Loss: 0.5131 ACC@1: 84.02% ACC@5: 96.89%: 100%|██████████| 79/79 [00:07<00:00, 10.85it/s]\n","Train Epoch: [35/100] lr: 0.0001 Loss: 0.3671 ACC@1: 87.34% ACC@5: 96.68%: 100%|██████████| 79/79 [00:08<00:00,  9.20it/s]\n","Test Epoch: [35/100] lr: 0.0001 Loss: 0.5141 ACC@1: 83.91% ACC@5: 96.92%: 100%|██████████| 79/79 [00:07<00:00, 10.71it/s]\n","Train Epoch: [36/100] lr: 0.0001 Loss: 0.3544 ACC@1: 88.04% ACC@5: 97.07%: 100%|██████████| 79/79 [00:08<00:00,  9.80it/s]\n","Test Epoch: [36/100] lr: 0.0001 Loss: 0.5175 ACC@1: 84.01% ACC@5: 96.93%: 100%|██████████| 79/79 [00:07<00:00, 10.82it/s]\n","Train Epoch: [37/100] lr: 0.0001 Loss: 0.3462 ACC@1: 88.17% ACC@5: 97.19%: 100%|██████████| 79/79 [00:08<00:00,  9.81it/s]\n","Test Epoch: [37/100] lr: 0.0001 Loss: 0.5166 ACC@1: 84.02% ACC@5: 96.93%: 100%|██████████| 79/79 [00:07<00:00, 10.96it/s]\n","Train Epoch: [38/100] lr: 0.0001 Loss: 0.3487 ACC@1: 88.00% ACC@5: 97.14%: 100%|██████████| 79/79 [00:08<00:00,  9.80it/s]\n","Test Epoch: [38/100] lr: 0.0001 Loss: 0.5185 ACC@1: 83.95% ACC@5: 96.91%: 100%|██████████| 79/79 [00:07<00:00, 10.85it/s]\n","Train Epoch: [39/100] lr: 0.0001 Loss: 0.3461 ACC@1: 88.23% ACC@5: 97.13%: 100%|██████████| 79/79 [00:08<00:00,  9.38it/s]\n","Test Epoch: [39/100] lr: 0.0001 Loss: 0.5225 ACC@1: 83.86% ACC@5: 96.86%: 100%|██████████| 79/79 [00:07<00:00, 10.81it/s]\n","Train Epoch: [40/100] lr: 0.0001 Loss: 0.3527 ACC@1: 87.94% ACC@5: 96.90%: 100%|██████████| 79/79 [00:08<00:00,  9.74it/s]\n","Test Epoch: [40/100] lr: 0.0001 Loss: 0.5232 ACC@1: 83.90% ACC@5: 96.85%: 100%|██████████| 79/79 [00:07<00:00, 10.73it/s]\n","Train Epoch: [41/100] lr: 0.0001 Loss: 0.3453 ACC@1: 88.21% ACC@5: 97.12%: 100%|██████████| 79/79 [00:08<00:00,  9.83it/s]\n","Test Epoch: [41/100] lr: 0.0001 Loss: 0.5249 ACC@1: 83.91% ACC@5: 96.80%: 100%|██████████| 79/79 [00:07<00:00, 10.87it/s]\n","Train Epoch: [42/100] lr: 0.0001 Loss: 0.3519 ACC@1: 87.69% ACC@5: 96.93%: 100%|██████████| 79/79 [00:08<00:00,  9.52it/s]\n","Test Epoch: [42/100] lr: 0.0001 Loss: 0.5267 ACC@1: 83.85% ACC@5: 96.79%: 100%|██████████| 79/79 [00:07<00:00, 10.95it/s]\n","Train Epoch: [43/100] lr: 0.0001 Loss: 0.3429 ACC@1: 87.76% ACC@5: 97.28%: 100%|██████████| 79/79 [00:08<00:00,  9.33it/s]\n","Test Epoch: [43/100] lr: 0.0001 Loss: 0.5289 ACC@1: 83.74% ACC@5: 96.82%: 100%|██████████| 79/79 [00:07<00:00, 10.75it/s]\n","Train Epoch: [44/100] lr: 0.0001 Loss: 0.3478 ACC@1: 88.03% ACC@5: 96.97%: 100%|██████████| 79/79 [00:08<00:00,  9.78it/s]\n","Test Epoch: [44/100] lr: 0.0001 Loss: 0.5281 ACC@1: 83.78% ACC@5: 96.79%: 100%|██████████| 79/79 [00:07<00:00, 10.79it/s]\n","Train Epoch: [45/100] lr: 0.0001 Loss: 0.3426 ACC@1: 88.63% ACC@5: 97.10%: 100%|██████████| 79/79 [00:08<00:00,  9.72it/s]\n","Test Epoch: [45/100] lr: 0.0001 Loss: 0.5318 ACC@1: 83.75% ACC@5: 96.82%: 100%|██████████| 79/79 [00:07<00:00, 10.75it/s]\n","Train Epoch: [46/100] lr: 0.0001 Loss: 0.3360 ACC@1: 88.54% ACC@5: 97.03%: 100%|██████████| 79/79 [00:08<00:00,  9.72it/s]\n","Test Epoch: [46/100] lr: 0.0001 Loss: 0.5318 ACC@1: 83.74% ACC@5: 96.83%: 100%|██████████| 79/79 [00:07<00:00, 10.84it/s]\n","Train Epoch: [47/100] lr: 0.0001 Loss: 0.3432 ACC@1: 88.26% ACC@5: 97.19%: 100%|██████████| 79/79 [00:09<00:00,  8.45it/s]\n","Test Epoch: [47/100] lr: 0.0001 Loss: 0.5331 ACC@1: 83.85% ACC@5: 96.79%: 100%|██████████| 79/79 [00:07<00:00, 10.89it/s]\n","Train Epoch: [48/100] lr: 0.0001 Loss: 0.3399 ACC@1: 88.11% ACC@5: 97.26%: 100%|██████████| 79/79 [00:08<00:00,  9.75it/s]\n","Test Epoch: [48/100] lr: 0.0001 Loss: 0.5370 ACC@1: 83.65% ACC@5: 96.81%: 100%|██████████| 79/79 [00:07<00:00, 10.88it/s]\n","Train Epoch: [49/100] lr: 0.0001 Loss: 0.3370 ACC@1: 88.51% ACC@5: 97.26%: 100%|██████████| 79/79 [00:08<00:00,  9.73it/s]\n","Test Epoch: [49/100] lr: 0.0001 Loss: 0.5349 ACC@1: 83.67% ACC@5: 96.84%: 100%|██████████| 79/79 [00:07<00:00, 10.84it/s]\n","Train Epoch: [50/100] lr: 0.0001 Loss: 0.3444 ACC@1: 88.26% ACC@5: 97.20%: 100%|██████████| 79/79 [00:08<00:00,  9.59it/s]\n","Test Epoch: [50/100] lr: 0.0001 Loss: 0.5360 ACC@1: 83.75% ACC@5: 96.72%: 100%|██████████| 79/79 [00:07<00:00, 10.78it/s]\n","Train Epoch: [51/100] lr: 0.0001 Loss: 0.3306 ACC@1: 88.48% ACC@5: 97.48%: 100%|██████████| 79/79 [00:09<00:00,  8.23it/s]\n","Test Epoch: [51/100] lr: 0.0001 Loss: 0.5382 ACC@1: 83.65% ACC@5: 96.82%: 100%|██████████| 79/79 [00:07<00:00, 10.76it/s]\n","Train Epoch: [52/100] lr: 0.0001 Loss: 0.3413 ACC@1: 88.01% ACC@5: 96.95%: 100%|██████████| 79/79 [00:08<00:00,  9.73it/s]\n","Test Epoch: [52/100] lr: 0.0001 Loss: 0.5379 ACC@1: 83.69% ACC@5: 96.82%: 100%|██████████| 79/79 [00:07<00:00, 10.77it/s]\n","Train Epoch: [53/100] lr: 0.0001 Loss: 0.3467 ACC@1: 88.21% ACC@5: 96.89%: 100%|██████████| 79/79 [00:08<00:00,  9.59it/s]\n","Test Epoch: [53/100] lr: 0.0001 Loss: 0.5394 ACC@1: 83.64% ACC@5: 96.88%: 100%|██████████| 79/79 [00:07<00:00, 10.76it/s]\n","Train Epoch: [54/100] lr: 0.0001 Loss: 0.3468 ACC@1: 88.19% ACC@5: 97.04%: 100%|██████████| 79/79 [00:08<00:00,  9.85it/s]\n","Test Epoch: [54/100] lr: 0.0001 Loss: 0.5413 ACC@1: 83.64% ACC@5: 96.78%: 100%|██████████| 79/79 [00:07<00:00, 10.37it/s]\n","Train Epoch: [55/100] lr: 0.0001 Loss: 0.3382 ACC@1: 88.42% ACC@5: 97.35%: 100%|██████████| 79/79 [00:09<00:00,  8.71it/s]\n","Test Epoch: [55/100] lr: 0.0001 Loss: 0.5414 ACC@1: 83.67% ACC@5: 96.78%: 100%|██████████| 79/79 [00:07<00:00, 10.84it/s]\n","Train Epoch: [56/100] lr: 0.0001 Loss: 0.3411 ACC@1: 88.22% ACC@5: 97.14%: 100%|██████████| 79/79 [00:08<00:00,  9.67it/s]\n","Test Epoch: [56/100] lr: 0.0001 Loss: 0.5416 ACC@1: 83.55% ACC@5: 96.93%: 100%|██████████| 79/79 [00:07<00:00, 10.73it/s]\n","Train Epoch: [57/100] lr: 0.0001 Loss: 0.3302 ACC@1: 88.74% ACC@5: 97.36%: 100%|██████████| 79/79 [00:08<00:00,  9.78it/s]\n","Test Epoch: [57/100] lr: 0.0001 Loss: 0.5447 ACC@1: 83.67% ACC@5: 96.81%: 100%|██████████| 79/79 [00:07<00:00, 10.78it/s]\n","Train Epoch: [58/100] lr: 0.0001 Loss: 0.3396 ACC@1: 88.28% ACC@5: 97.37%: 100%|██████████| 79/79 [00:08<00:00,  9.72it/s]\n","Test Epoch: [58/100] lr: 0.0001 Loss: 0.5458 ACC@1: 83.63% ACC@5: 96.80%: 100%|██████████| 79/79 [00:07<00:00, 10.29it/s]\n","Train Epoch: [59/100] lr: 0.0001 Loss: 0.3425 ACC@1: 88.20% ACC@5: 97.36%: 100%|██████████| 79/79 [00:08<00:00,  9.74it/s]\n","Test Epoch: [59/100] lr: 0.0001 Loss: 0.5524 ACC@1: 83.52% ACC@5: 96.79%: 100%|██████████| 79/79 [00:07<00:00, 10.78it/s]\n","Train Epoch: [60/100] lr: 0.0001 Loss: 0.3318 ACC@1: 88.11% ACC@5: 97.48%: 100%|██████████| 79/79 [00:08<00:00,  9.80it/s]\n","Test Epoch: [60/100] lr: 0.0000 Loss: 0.5481 ACC@1: 83.47% ACC@5: 96.83%: 100%|██████████| 79/79 [00:07<00:00, 10.62it/s]\n","Train Epoch: [61/100] lr: 0.0000 Loss: 0.3398 ACC@1: 87.73% ACC@5: 97.44%: 100%|██████████| 79/79 [00:08<00:00,  9.72it/s]\n","Test Epoch: [61/100] lr: 0.0000 Loss: 0.5478 ACC@1: 83.54% ACC@5: 96.81%: 100%|██████████| 79/79 [00:07<00:00, 10.77it/s]\n","Train Epoch: [62/100] lr: 0.0000 Loss: 0.3427 ACC@1: 88.26% ACC@5: 97.18%: 100%|██████████| 79/79 [00:08<00:00,  9.80it/s]\n","Test Epoch: [62/100] lr: 0.0000 Loss: 0.5479 ACC@1: 83.50% ACC@5: 96.78%: 100%|██████████| 79/79 [00:07<00:00, 10.48it/s]\n","Train Epoch: [63/100] lr: 0.0000 Loss: 0.3217 ACC@1: 88.79% ACC@5: 97.35%: 100%|██████████| 79/79 [00:08<00:00,  9.77it/s]\n","Test Epoch: [63/100] lr: 0.0000 Loss: 0.5480 ACC@1: 83.51% ACC@5: 96.77%: 100%|██████████| 79/79 [00:07<00:00, 10.74it/s]\n","Train Epoch: [64/100] lr: 0.0000 Loss: 0.3437 ACC@1: 87.95% ACC@5: 96.97%: 100%|██████████| 79/79 [00:08<00:00,  9.68it/s]\n","Test Epoch: [64/100] lr: 0.0000 Loss: 0.5479 ACC@1: 83.51% ACC@5: 96.78%: 100%|██████████| 79/79 [00:07<00:00, 10.70it/s]\n","Train Epoch: [65/100] lr: 0.0000 Loss: 0.3391 ACC@1: 88.56% ACC@5: 97.24%: 100%|██████████| 79/79 [00:08<00:00,  9.62it/s]\n","Test Epoch: [65/100] lr: 0.0000 Loss: 0.5478 ACC@1: 83.52% ACC@5: 96.76%: 100%|██████████| 79/79 [00:07<00:00, 10.78it/s]\n","Train Epoch: [66/100] lr: 0.0000 Loss: 0.3375 ACC@1: 88.23% ACC@5: 97.14%: 100%|██████████| 79/79 [00:08<00:00,  9.54it/s]\n","Test Epoch: [66/100] lr: 0.0000 Loss: 0.5483 ACC@1: 83.53% ACC@5: 96.77%: 100%|██████████| 79/79 [00:07<00:00, 10.18it/s]\n","Train Epoch: [67/100] lr: 0.0000 Loss: 0.3462 ACC@1: 88.15% ACC@5: 97.21%: 100%|██████████| 79/79 [00:08<00:00,  9.62it/s]\n","Test Epoch: [67/100] lr: 0.0000 Loss: 0.5481 ACC@1: 83.53% ACC@5: 96.78%: 100%|██████████| 79/79 [00:07<00:00, 10.62it/s]\n","Train Epoch: [68/100] lr: 0.0000 Loss: 0.3520 ACC@1: 87.85% ACC@5: 97.07%: 100%|██████████| 79/79 [00:08<00:00,  9.67it/s]\n","Test Epoch: [68/100] lr: 0.0000 Loss: 0.5483 ACC@1: 83.45% ACC@5: 96.75%: 100%|██████████| 79/79 [00:07<00:00, 10.69it/s]\n","Train Epoch: [69/100] lr: 0.0000 Loss: 0.3464 ACC@1: 87.89% ACC@5: 97.05%: 100%|██████████| 79/79 [00:08<00:00,  9.63it/s]\n","Test Epoch: [69/100] lr: 0.0000 Loss: 0.5483 ACC@1: 83.56% ACC@5: 96.78%: 100%|██████████| 79/79 [00:07<00:00, 10.72it/s]\n","Train Epoch: [70/100] lr: 0.0000 Loss: 0.3276 ACC@1: 88.66% ACC@5: 97.25%: 100%|██████████| 79/79 [00:08<00:00,  9.59it/s]\n","Test Epoch: [70/100] lr: 0.0000 Loss: 0.5484 ACC@1: 83.55% ACC@5: 96.77%: 100%|██████████| 79/79 [00:07<00:00,  9.89it/s]\n","Train Epoch: [71/100] lr: 0.0000 Loss: 0.3339 ACC@1: 88.42% ACC@5: 97.22%: 100%|██████████| 79/79 [00:08<00:00,  9.65it/s]\n","Test Epoch: [71/100] lr: 0.0000 Loss: 0.5485 ACC@1: 83.55% ACC@5: 96.78%: 100%|██████████| 79/79 [00:07<00:00, 10.74it/s]\n","Train Epoch: [72/100] lr: 0.0000 Loss: 0.3362 ACC@1: 88.53% ACC@5: 97.06%: 100%|██████████| 79/79 [00:08<00:00,  9.65it/s]\n","Test Epoch: [72/100] lr: 0.0000 Loss: 0.5484 ACC@1: 83.52% ACC@5: 96.78%: 100%|██████████| 79/79 [00:07<00:00, 10.73it/s]\n","Train Epoch: [73/100] lr: 0.0000 Loss: 0.3365 ACC@1: 88.56% ACC@5: 97.16%: 100%|██████████| 79/79 [00:08<00:00,  9.64it/s]\n","Test Epoch: [73/100] lr: 0.0000 Loss: 0.5487 ACC@1: 83.53% ACC@5: 96.78%: 100%|██████████| 79/79 [00:07<00:00, 10.72it/s]\n","Train Epoch: [74/100] lr: 0.0000 Loss: 0.3478 ACC@1: 87.71% ACC@5: 97.08%: 100%|██████████| 79/79 [00:08<00:00,  9.32it/s]\n","Test Epoch: [74/100] lr: 0.0000 Loss: 0.5487 ACC@1: 83.45% ACC@5: 96.78%: 100%|██████████| 79/79 [00:08<00:00,  9.20it/s]\n","Train Epoch: [75/100] lr: 0.0000 Loss: 0.3448 ACC@1: 87.79% ACC@5: 97.21%: 100%|██████████| 79/79 [00:08<00:00,  9.61it/s]\n","Test Epoch: [75/100] lr: 0.0000 Loss: 0.5491 ACC@1: 83.52% ACC@5: 96.74%: 100%|██████████| 79/79 [00:07<00:00, 10.60it/s]\n","Train Epoch: [76/100] lr: 0.0000 Loss: 0.3206 ACC@1: 89.11% ACC@5: 97.58%: 100%|██████████| 79/79 [00:08<00:00,  9.54it/s]\n","Test Epoch: [76/100] lr: 0.0000 Loss: 0.5492 ACC@1: 83.47% ACC@5: 96.74%: 100%|██████████| 79/79 [00:07<00:00, 10.64it/s]\n","Train Epoch: [77/100] lr: 0.0000 Loss: 0.3491 ACC@1: 88.00% ACC@5: 97.05%: 100%|██████████| 79/79 [00:08<00:00,  9.61it/s]\n","Test Epoch: [77/100] lr: 0.0000 Loss: 0.5494 ACC@1: 83.44% ACC@5: 96.76%: 100%|██████████| 79/79 [00:07<00:00, 10.70it/s]\n","Train Epoch: [78/100] lr: 0.0000 Loss: 0.3333 ACC@1: 88.46% ACC@5: 97.29%: 100%|██████████| 79/79 [00:08<00:00,  9.48it/s]\n","Test Epoch: [78/100] lr: 0.0000 Loss: 0.5489 ACC@1: 83.49% ACC@5: 96.78%: 100%|██████████| 79/79 [00:08<00:00,  9.75it/s]\n","Train Epoch: [79/100] lr: 0.0000 Loss: 0.3350 ACC@1: 88.01% ACC@5: 97.01%: 100%|██████████| 79/79 [00:08<00:00,  9.63it/s]\n","Test Epoch: [79/100] lr: 0.0000 Loss: 0.5489 ACC@1: 83.45% ACC@5: 96.77%: 100%|██████████| 79/79 [00:07<00:00, 10.60it/s]\n","Train Epoch: [80/100] lr: 0.0000 Loss: 0.3386 ACC@1: 88.33% ACC@5: 97.26%: 100%|██████████| 79/79 [00:08<00:00,  9.42it/s]\n","Test Epoch: [80/100] lr: 0.0000 Loss: 0.5490 ACC@1: 83.47% ACC@5: 96.76%: 100%|██████████| 79/79 [00:07<00:00, 10.68it/s]\n","Train Epoch: [81/100] lr: 0.0000 Loss: 0.3368 ACC@1: 88.41% ACC@5: 97.40%: 100%|██████████| 79/79 [00:08<00:00,  9.53it/s]\n","Test Epoch: [81/100] lr: 0.0000 Loss: 0.5489 ACC@1: 83.47% ACC@5: 96.77%: 100%|██████████| 79/79 [00:07<00:00, 10.66it/s]\n","Train Epoch: [82/100] lr: 0.0000 Loss: 0.3395 ACC@1: 88.33% ACC@5: 97.22%: 100%|██████████| 79/79 [00:08<00:00,  9.05it/s]\n","Test Epoch: [82/100] lr: 0.0000 Loss: 0.5490 ACC@1: 83.46% ACC@5: 96.76%: 100%|██████████| 79/79 [00:07<00:00, 10.63it/s]\n","Train Epoch: [83/100] lr: 0.0000 Loss: 0.3300 ACC@1: 88.65% ACC@5: 97.12%: 100%|██████████| 79/79 [00:08<00:00,  9.56it/s]\n","Test Epoch: [83/100] lr: 0.0000 Loss: 0.5489 ACC@1: 83.46% ACC@5: 96.75%: 100%|██████████| 79/79 [00:07<00:00, 10.69it/s]\n","Train Epoch: [84/100] lr: 0.0000 Loss: 0.3314 ACC@1: 88.45% ACC@5: 97.49%: 100%|██████████| 79/79 [00:08<00:00,  9.64it/s]\n","Test Epoch: [84/100] lr: 0.0000 Loss: 0.5490 ACC@1: 83.46% ACC@5: 96.75%: 100%|██████████| 79/79 [00:07<00:00, 10.64it/s]\n","Train Epoch: [85/100] lr: 0.0000 Loss: 0.3386 ACC@1: 88.22% ACC@5: 97.25%: 100%|██████████| 79/79 [00:08<00:00,  9.59it/s]\n","Test Epoch: [85/100] lr: 0.0000 Loss: 0.5490 ACC@1: 83.47% ACC@5: 96.74%: 100%|██████████| 79/79 [00:07<00:00, 10.68it/s]\n","Train Epoch: [86/100] lr: 0.0000 Loss: 0.3498 ACC@1: 88.05% ACC@5: 96.81%: 100%|██████████| 79/79 [00:08<00:00,  9.10it/s]\n","Test Epoch: [86/100] lr: 0.0000 Loss: 0.5490 ACC@1: 83.46% ACC@5: 96.75%: 100%|██████████| 79/79 [00:07<00:00, 10.61it/s]\n","Train Epoch: [87/100] lr: 0.0000 Loss: 0.3378 ACC@1: 88.16% ACC@5: 97.46%: 100%|██████████| 79/79 [00:08<00:00,  9.53it/s]\n","Test Epoch: [87/100] lr: 0.0000 Loss: 0.5491 ACC@1: 83.47% ACC@5: 96.75%: 100%|██████████| 79/79 [00:07<00:00, 10.64it/s]\n","Train Epoch: [88/100] lr: 0.0000 Loss: 0.3412 ACC@1: 88.12% ACC@5: 97.18%: 100%|██████████| 79/79 [00:08<00:00,  9.48it/s]\n","Test Epoch: [88/100] lr: 0.0000 Loss: 0.5491 ACC@1: 83.47% ACC@5: 96.75%: 100%|██████████| 79/79 [00:07<00:00, 10.54it/s]\n","Train Epoch: [89/100] lr: 0.0000 Loss: 0.3369 ACC@1: 88.21% ACC@5: 97.26%: 100%|██████████| 79/79 [00:08<00:00,  9.58it/s]\n","Test Epoch: [89/100] lr: 0.0000 Loss: 0.5491 ACC@1: 83.47% ACC@5: 96.76%: 100%|██████████| 79/79 [00:07<00:00, 10.59it/s]\n","Train Epoch: [90/100] lr: 0.0000 Loss: 0.3459 ACC@1: 88.15% ACC@5: 97.23%: 100%|██████████| 79/79 [00:08<00:00,  9.10it/s]\n","Test Epoch: [90/100] lr: 0.0000 Loss: 0.5492 ACC@1: 83.48% ACC@5: 96.77%: 100%|██████████| 79/79 [00:07<00:00, 10.70it/s]\n","Train Epoch: [91/100] lr: 0.0000 Loss: 0.3491 ACC@1: 87.82% ACC@5: 96.98%: 100%|██████████| 79/79 [00:08<00:00,  9.54it/s]\n","Test Epoch: [91/100] lr: 0.0000 Loss: 0.5492 ACC@1: 83.48% ACC@5: 96.77%: 100%|██████████| 79/79 [00:07<00:00, 10.49it/s]\n","Train Epoch: [92/100] lr: 0.0000 Loss: 0.3427 ACC@1: 88.01% ACC@5: 97.27%: 100%|██████████| 79/79 [00:08<00:00,  9.55it/s]\n","Test Epoch: [92/100] lr: 0.0000 Loss: 0.5492 ACC@1: 83.46% ACC@5: 96.77%: 100%|██████████| 79/79 [00:07<00:00, 10.57it/s]\n","Train Epoch: [93/100] lr: 0.0000 Loss: 0.3450 ACC@1: 87.98% ACC@5: 97.17%: 100%|██████████| 79/79 [00:08<00:00,  9.56it/s]\n","Test Epoch: [93/100] lr: 0.0000 Loss: 0.5492 ACC@1: 83.48% ACC@5: 96.74%: 100%|██████████| 79/79 [00:07<00:00, 10.61it/s]\n","Train Epoch: [94/100] lr: 0.0000 Loss: 0.3426 ACC@1: 88.27% ACC@5: 97.21%: 100%|██████████| 79/79 [00:08<00:00,  9.14it/s]\n","Test Epoch: [94/100] lr: 0.0000 Loss: 0.5492 ACC@1: 83.49% ACC@5: 96.74%: 100%|██████████| 79/79 [00:07<00:00, 10.68it/s]\n","Train Epoch: [95/100] lr: 0.0000 Loss: 0.3339 ACC@1: 88.48% ACC@5: 97.27%: 100%|██████████| 79/79 [00:08<00:00,  9.60it/s]\n","Test Epoch: [95/100] lr: 0.0000 Loss: 0.5493 ACC@1: 83.51% ACC@5: 96.74%: 100%|██████████| 79/79 [00:07<00:00, 10.51it/s]\n","Train Epoch: [96/100] lr: 0.0000 Loss: 0.3443 ACC@1: 87.93% ACC@5: 97.33%: 100%|██████████| 79/79 [00:08<00:00,  9.65it/s]\n","Test Epoch: [96/100] lr: 0.0000 Loss: 0.5493 ACC@1: 83.51% ACC@5: 96.75%: 100%|██████████| 79/79 [00:07<00:00, 10.65it/s]\n","Train Epoch: [97/100] lr: 0.0000 Loss: 0.3407 ACC@1: 88.26% ACC@5: 97.24%: 100%|██████████| 79/79 [00:08<00:00,  9.54it/s]\n","Test Epoch: [97/100] lr: 0.0000 Loss: 0.5493 ACC@1: 83.52% ACC@5: 96.75%: 100%|██████████| 79/79 [00:07<00:00, 10.53it/s]\n","Train Epoch: [98/100] lr: 0.0000 Loss: 0.3394 ACC@1: 88.12% ACC@5: 97.63%: 100%|██████████| 79/79 [00:08<00:00,  9.13it/s]\n","Test Epoch: [98/100] lr: 0.0000 Loss: 0.5493 ACC@1: 83.53% ACC@5: 96.75%: 100%|██████████| 79/79 [00:07<00:00, 10.51it/s]\n","Train Epoch: [99/100] lr: 0.0000 Loss: 0.3394 ACC@1: 88.48% ACC@5: 97.33%: 100%|██████████| 79/79 [00:08<00:00,  9.41it/s]\n","Test Epoch: [99/100] lr: 0.0000 Loss: 0.5493 ACC@1: 83.52% ACC@5: 96.74%: 100%|██████████| 79/79 [00:07<00:00, 10.41it/s]\n","Train Epoch: [100/100] lr: 0.0000 Loss: 0.3384 ACC@1: 88.23% ACC@5: 97.18%: 100%|██████████| 79/79 [00:08<00:00,  9.42it/s]\n","Test Epoch: [100/100] lr: 0.0000 Loss: 0.5493 ACC@1: 83.49% ACC@5: 96.74%: 100%|██████████| 79/79 [00:07<00:00, 10.51it/s]\n"]},{"output_type":"stream","name":"stdout","text":["[88.23]\n","[83.49]\n","top10 trainacc 88.23\n","top10 testacc 83.49\n","best test 83.49\n","88.23\n","83.49\n","0.0\n","0.0\n"]}]},{"cell_type":"code","source":["!pip install torchcam"],"metadata":{"id":"gsIGXx3hTCzi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","import matplotlib.pyplot as plt\n","from torchcam.utils import overlay_mask\n","supervised = ModelBase()\n","supervised.load_state_dict(torch.load(\"/content/drive/My Drive/Models/Supervised/supervisedresnet18v1.1.pth\"))\n","\n","result = overlay_mask(to_pil_image(img), to_pil_image(activation_map[0].squeeze(0), mode='F'), alpha=0.5)\n","# Display it\n","plt.imshow(result); plt.axis('off'); plt.tight_layout(); plt.show()"],"metadata":{"id":"lVvfdoc5dDyZ"},"execution_count":null,"outputs":[]}]}